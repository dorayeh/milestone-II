{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text classification of disputed? with narrative\n",
    "\n",
    "### (experimental. f23 m2 team 10. mads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "2023-09-23 13:37:40.497301: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Built-in libraries\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "# Third-party libraries for data handling and processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Pre-processing\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Model selection and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Deep Learning Libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "# Miscellaneous\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4028530, 18)\n"
     ]
    }
   ],
   "source": [
    "# load complaints.csv into a dataframe DATA\n",
    "DATA = pd.read_csv('complaints.csv')\n",
    "\n",
    "# print shape of DATA\n",
    "print(DATA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset to disputed not null and narrative not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164034, 18)\n"
     ]
    }
   ],
   "source": [
    "# subset DATA narrative and disputed columns that are not null\n",
    "data = DATA[DATA['Consumer complaint narrative'].notnull() & DATA['Consumer disputed?'].notnull()]\n",
    "\n",
    "# print shape of data\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to narrative and disputed columns, and rename disputed to label\n",
    "data = data[['Consumer complaint narrative', 'Consumer disputed?']]\n",
    "\n",
    "# rename columns to narrative and disputed\n",
    "data.columns = ['narrative', 'disputed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper function: preprocess narrative text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the narrative column\n",
    "\n",
    "def preprocess_narrative(text):\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove XXXX like pattern\n",
    "    text = re.sub(r'x{2,}', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply preprocess on narrative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31338    XXXX XXXX, 2016 To Whom It May Concern : I am ...\n",
      "31664    History of account : I am a co-signer on XXXX ...\n",
      "35599    I have excellent credit and the only times I m...\n",
      "49963    There are XXXX hard pulls in my credit reports...\n",
      "70552    Experian has fail to delete wrong personal inf...\n",
      "Name: narrative, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164034/164034 [00:16<00:00, 10021.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164034, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{31338: 'may concern writing regard barrett financial llc dealing company feel important pertinent agencies notified shady business practices every hard money lender az said illegal owner occupied home hoping letter writing get someone look company ability follow understand law really hurt someone information terrified capable hoping put fraud alert credit agencies everyone involved deal unhappy company worked particular man show respect told blatant lies another party process purchasing home took personal financial info including bank account info social security numbers sent us prequalification letter found home put offer following guidelines thought good wrong man disappeared buyers sellers agent agent title company inspection company hold talked receptionist know going would circumstances allow us speak another lender phone message machines voice mails full front main line went days lost house asked simply pay us back wire transfer costs title company refused still refuses day knew paperwork took advantage lied person lack communication clients subpar best honestly believe man thinks law better people pure sociopath humble opinion sincerely ca az',\n",
       " 31664: 'history account cosigner auto loan husband immigrant trying establish credit loan years never reported late appx actually corporate wide credit reporting issue months pull reporting bureau fixing resulted us losing established credit account files significantly dropping credit scores time forcing us sub prime lending histories added back bureau increased score renumeration given higher rates interest pay new accounts established time using auto pay option though current issue sent alert company called bridgecrest stage never heard placed late credit file called find company told bought earlier year escalated team supervisor named pulled file determined likely fault transferring auto draft gave number call escalated supervisor informed issue lay solely bridgecrest refused issue email stating acted error directed back bridgecrest continue day disavow responsibility party provider provide auto pay internally immediately caught late pay plus current bill set new auto pay bridgecrest great deal pushed back forth gave awhile credit report came saw dropped scores points closing new home put us much higher interest rate new home currently buying cost us added month called bridgecrest back plea bargain courtesy removal late reporting justification shared responsibility scripted response politely told rep would forced involve cfpb ignored claimed calls made told never happened asked recording calls supposedly tried make letters showing bridgecrest ever sent us letter introduction bridgecrest stage became politely frustrated transferred supervisor named listened sympathetically looked phone log informed one numbers calling bad number number mine said ability pull record see call log asked would identified calling said would likely say bridgecrest would like speak recollection ever receiving call call numbers saved phone however still working numbers go directly bridgecrest point unrecognizable company phone number nt know bridgecrest would assumed sales people blocked also nt seem know cfpb gave extension suggested call try get send email stating fault brings degrees back started',\n",
       " 35599: 'excellent credit times miss payment revolving credit account vendor fails send bill aka car care synchrony bank formerly ge capital one several similar cards company also large money market account one specialty cards purchase interest free qualifying purchase period time presuming account kept current get slapped interest rate either paid regularly full deadline know always pay full time never received bill emergency car repair purchase unsure would show bill since busy way keep track companies refuse notify owe money way knowing missed payment bill showed immediately called company tell missed payment nt received bill nice customer service agent took late charge told would problem made double payment assured notated account three days later started getting regular calls number synchrony bank telling call putting hold minutes since busy calls came meetings dinner hung first tried call back time called back got someone nt even identify company department took minutes precious time figure collections determined nt security tell accounts breached told already called tell mistake told pay payment due later month asking check records leave alone intervening two weeks received daily calls letter synchrony often dinner time times day supposed call yesterday finally called number letter spent minutes transferred finally get escalation unit us asked supervisor told could refer supervisor voice mail none people answered phone identified company answered phone supervisor never called back despite clearly stating would report cfpb nt hear back evening scheduled payment months suspect going slapped huge fees interest rates synchrony ca nt seem send bills regular basis keep records straight btw first time received bill specialty cards',\n",
       " 49963: 'hard pulls credit reports authorize turned also credit reports address notwas never mine',\n",
       " 70552: 'experian fail delete wrong personal information report send request twice send results deleted wrong personal information still showing report'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print before preprocessing\n",
    "print(data.narrative.head())\n",
    "\n",
    "# apply preprocess_narrative to narrative column\n",
    "data['narrative_processed'] = data['narrative'].progress_apply(preprocess_narrative)\n",
    "\n",
    "# print shape of data\n",
    "print(data.shape)\n",
    "\n",
    "# print after preprocessing\n",
    "data.narrative_processed.head(5).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load google word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import word2vec model\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment. Subset No and Yes 3:1 ratio. SMOTE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     128227\n",
       "Yes     35807\n",
       "Name: disputed, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset to narrative and disputed both not null columns, and rename the columns for simplicity\n",
    "\n",
    "df1 = DATA[DATA['Consumer complaint narrative'].notnull() & DATA['Consumer disputed?'].notnull()][['Consumer complaint narrative', 'Consumer disputed?']]\n",
    "\n",
    "df1.columns = ['narrative', 'disputed']\n",
    "\n",
    "df1.disputed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143228, 2)\n",
      "No     107421\n",
      "Yes     35807\n",
      "Name: disputed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# subset df to have 35807 disputed and 35807*3 not disputed complaints (randomly sampled)\n",
    "\n",
    "disputed = df1[df1['disputed'] == 'Yes']\n",
    "not_disputed = df1[df1['disputed'] == 'No']\n",
    "not_disputed = not_disputed.sample(n=disputed.shape[0]*3)\n",
    "\n",
    "df1 = pd.concat([disputed, not_disputed])\n",
    "\n",
    "# print shape of df \n",
    "print(df1.shape)\n",
    "\n",
    "# print value counts of disputed column\n",
    "print(df1['disputed'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode disputed column to 0 and 1\n",
    "\n",
    "le = LabelEncoder()\n",
    "df1.disputed = le.fit_transform(df1.disputed)\n",
    "y = df1.disputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply preprocess to narrative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143228/143228 [00:13<00:00, 10350.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1636316: 'aimloancom quoting ultra low rates quote arm required verbaige aimloancom sent wednesday subject aimloancom rate watch notification mortgage rate update aimloancom interest rates accurate year fixed rate guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit year fixed rate guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit year fixed rate guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit guaranteed points lender credit',\n",
       " 1411671: 'contacted workplace attorney spoke employee stated collecting debt cashcall would like intention taking care matter explain circumstances preventing continuing payments employee begun make threats legal action garnishing wages would moneyshe stated owed conversation requested validation debt stated sent explain recieve asked located county ca told right sue continue get hostile hung immediately called cashcall spoke told account balance time payment posted another payment orginial loan amount returned explained account principial amount interest sold account question identity request cease contact workplace research found numerous compliants',\n",
       " 3786611: 'debt collection debt home lived parents high school first last name father must mix tagging collection intended never put service name ever especially year old kid company would logically look dates collection makes sense could debt spoke collections agent explained debt month birthday even open cable service year old logical way much debt could taken place short time shame simple mix living address could causing much credit pain never cable service collection falsely charged name',\n",
       " 2416924: 'looked credit report noticed company state listed credit report representative contact company last week behalf busy days week careful research representative saw company authorized collect licensed collect state specific rule relates able collect appears credit report license expired according office financial regulations business licensing division confirmed state website order collect must licensed agencies goodactive standing company representative called asked speak office manager compliance officer asked call reference explained transferred guy said name represented company legal counsel went denounce claim verbally representative attempted scold attorney never represented review via reviewing bar association website active attorney name one listed deceased attorney fidelity information corp willfully maliciously violating state law fdcpa fair debt collection practices act using deceptive practices false representation sure office nt support businesses state operating manner company contacted give opportunity rectify problem however failed continuing report account ca nt legally report damaging well family mind never contract company time current court action pending nt another legal actions prior company ca nt get resolved expeditiously legal action definitely go forward letter faxed well proof expired status agencies well copy bar website deceased attorney name tried assume resurrect identity deceased attorney',\n",
       " 3969925: 'credit card turn went business bought orchard bank filed bankruptcy accounts listed bankruptcy sold account legally discharged turn sold portfolio recovery services harassing nonstop collect account owe account ones purchased account even valid'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process narrative column\n",
    "df1['narrative_processed'] = df1['narrative'].progress_apply(preprocess_narrative)\n",
    "\n",
    "# print some to check\n",
    "df1.narrative_processed.sample(5).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tfidf_matrix: (143228, 87750)\n",
      "['aa', 'aaa', 'aaaa', 'aaaaan', 'aaaaargh', 'aaaallllll', 'aaabank', 'aaaked', 'aaaratings', 'aaarm']\n",
      "['zoned', 'zonehad', 'zones', 'zonethanking', 'zoning', 'zoo', 'zoom', 'zooms', 'ztuff', 'zwicker']\n"
     ]
    }
   ],
   "source": [
    "# tfidf\n",
    "\n",
    "# initialize tfidf vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# fit and transform tfidf vectorizer on narrative column\n",
    "tfidf.fit(df1.narrative_processed)\n",
    "\n",
    "tfidf_matrix = tfidf.transform(df1.narrative_processed)\n",
    "\n",
    "# print shape of tfidf_matrix\n",
    "print(f\"shape of tfidf_matrix: {tfidf_matrix.shape}\")\n",
    "\n",
    "# get tfidf feature names\n",
    "tfidf_features = tfidf.get_feature_names()\n",
    "\n",
    "# print first 10 and last 10 feature names\n",
    "print(tfidf_features[:10]), print(tfidf_features[-10:]), \n",
    "\n",
    "# get tfidf weights\n",
    "tfidf_weights = tfidf.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214842, 87750) (214842,)\n",
      "1    107421\n",
      "0    107421\n",
      "Name: disputed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# smote to oversample minority class for tfidf data\n",
    "\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_smote, y_smote = smote.fit_resample(tfidf_matrix, y)\n",
    "\n",
    "# print shape of X_smote, y_smote\n",
    "print(X_smote.shape, y_smote.shape)\n",
    "\n",
    "# print value counts of y_smote\n",
    "print(pd.Series(y_smote).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150389, 87750) (64453, 87750) (150389,) (64453,)\n"
     ]
    }
   ],
   "source": [
    "# train test split: X_smote_train, X_smote_test, y_smote_train, y_smote_test, 70% train, 30% test, random_state=0, stratify=y_smote\n",
    "X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, stratify=y_smote, random_state=0, test_size=0.3)\n",
    "\n",
    "# print shape of X_smote_train, X_smote_test, y_smote_train, y_smote_test\n",
    "print(X_smote_train.shape, X_smote_test.shape, y_smote_train.shape, y_smote_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "naive bayes tfidf: f1 0.698"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes with tfidf data, wrap in a function\n",
    "\n",
    "def train_naive_bayes(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # instantiate naive bayes model\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    # fit model\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    # print f1 score\n",
    "    print(f\"f1 score: {f1_score(y_test, y_pred)}\")\n",
    "\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.6998790376775234\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train with naive bayes\n",
    "model_nb = train_naive_bayes(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression tfidf, f1: 0.663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with logistic regression, wrap in a function\n",
    "\n",
    "def train_lr(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # instantiate logistic regression model\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # print f1 score\n",
    "    print(f\"f1 score: {f1_score(y_test, y_pred)}\")\n",
    "\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.6638555150763508\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train with logistic regression\n",
    "model_lr = train_lr(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train lightgbm \n",
    "\n",
    "def train_lgbm(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # instantiate lightgbm model\n",
    "    lgbm = LGBMClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = lgbm.predict(X_test)\n",
    "\n",
    "    # print f1 score\n",
    "    print(f\"f1 score: {f1_score(y_test, y_pred)}\")\n",
    "\n",
    "    return lgbm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier TFIDF: 0.749"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with xgboost, in a function\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", message=\"Starting in XGBoost 1.3.0\")\n",
    "\n",
    "def train_xgb(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # instantiate xgboost model\n",
    "    xgb = XGBClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "    # fit model\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = xgb.predict(X_test)\n",
    "\n",
    "    # print f1 score\n",
    "    print(f\"f1 score: {f1_score(y_test, y_pred)}\")\n",
    "\n",
    "    return xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:12:29] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1 score: 0.7480209908387442\n"
     ]
    }
   ],
   "source": [
    "# train with xgboost\n",
    "\n",
    "model_xgb = train_xgb(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoostClassifier TFIDF 0.764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost in a function\n",
    "\n",
    "def train_catboost(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # instantiate catboost model\n",
    "    catboost = CatBoostClassifier(random_state=42)\n",
    "\n",
    "    # fit model\n",
    "    catboost.fit(X_train, y_train)\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = catboost.predict(X_test)\n",
    "\n",
    "    # print accuracy score\n",
    "    print(f\"accuracy score: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "    # print f1 score\n",
    "    print(f\"f1 score: {f1_score(y_test, y_pred)}\")\n",
    "\n",
    "    return catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.100046\n",
      "0:\tlearn: 0.6777597\ttotal: 249ms\tremaining: 4m 8s\n",
      "1:\tlearn: 0.6702825\ttotal: 334ms\tremaining: 2m 46s\n",
      "2:\tlearn: 0.6606521\ttotal: 432ms\tremaining: 2m 23s\n",
      "3:\tlearn: 0.6549548\ttotal: 524ms\tremaining: 2m 10s\n",
      "4:\tlearn: 0.6419058\ttotal: 607ms\tremaining: 2m\n",
      "5:\tlearn: 0.6376055\ttotal: 692ms\tremaining: 1m 54s\n",
      "6:\tlearn: 0.6337617\ttotal: 793ms\tremaining: 1m 52s\n",
      "7:\tlearn: 0.6301917\ttotal: 889ms\tremaining: 1m 50s\n",
      "8:\tlearn: 0.6149117\ttotal: 982ms\tremaining: 1m 48s\n",
      "9:\tlearn: 0.6077590\ttotal: 1.08s\tremaining: 1m 46s\n",
      "10:\tlearn: 0.5969467\ttotal: 1.19s\tremaining: 1m 47s\n",
      "11:\tlearn: 0.5903476\ttotal: 1.29s\tremaining: 1m 46s\n",
      "12:\tlearn: 0.5879821\ttotal: 1.38s\tremaining: 1m 44s\n",
      "13:\tlearn: 0.5859557\ttotal: 1.48s\tremaining: 1m 44s\n",
      "14:\tlearn: 0.5835187\ttotal: 1.57s\tremaining: 1m 43s\n",
      "15:\tlearn: 0.5816573\ttotal: 1.66s\tremaining: 1m 42s\n",
      "16:\tlearn: 0.5798496\ttotal: 1.74s\tremaining: 1m 40s\n",
      "17:\tlearn: 0.5781260\ttotal: 1.83s\tremaining: 1m 39s\n",
      "18:\tlearn: 0.5740408\ttotal: 2.01s\tremaining: 1m 43s\n",
      "19:\tlearn: 0.5725108\ttotal: 2.09s\tremaining: 1m 42s\n",
      "20:\tlearn: 0.5661963\ttotal: 2.17s\tremaining: 1m 41s\n",
      "21:\tlearn: 0.5596856\ttotal: 2.26s\tremaining: 1m 40s\n",
      "22:\tlearn: 0.5583585\ttotal: 2.35s\tremaining: 1m 39s\n",
      "23:\tlearn: 0.5502355\ttotal: 2.42s\tremaining: 1m 38s\n",
      "24:\tlearn: 0.5466448\ttotal: 2.51s\tremaining: 1m 37s\n",
      "25:\tlearn: 0.5453165\ttotal: 2.59s\tremaining: 1m 36s\n",
      "26:\tlearn: 0.5442021\ttotal: 2.67s\tremaining: 1m 36s\n",
      "27:\tlearn: 0.5428707\ttotal: 2.75s\tremaining: 1m 35s\n",
      "28:\tlearn: 0.5371544\ttotal: 2.83s\tremaining: 1m 34s\n",
      "29:\tlearn: 0.5309701\ttotal: 2.91s\tremaining: 1m 33s\n",
      "30:\tlearn: 0.5300275\ttotal: 2.99s\tremaining: 1m 33s\n",
      "31:\tlearn: 0.5266828\ttotal: 3.06s\tremaining: 1m 32s\n",
      "32:\tlearn: 0.5257624\ttotal: 3.14s\tremaining: 1m 32s\n",
      "33:\tlearn: 0.5210193\ttotal: 3.26s\tremaining: 1m 32s\n",
      "34:\tlearn: 0.5201460\ttotal: 3.38s\tremaining: 1m 33s\n",
      "35:\tlearn: 0.5186418\ttotal: 3.47s\tremaining: 1m 33s\n",
      "36:\tlearn: 0.5178519\ttotal: 3.56s\tremaining: 1m 32s\n",
      "37:\tlearn: 0.5147148\ttotal: 3.67s\tremaining: 1m 33s\n",
      "38:\tlearn: 0.5140292\ttotal: 3.77s\tremaining: 1m 32s\n",
      "39:\tlearn: 0.5105045\ttotal: 3.87s\tremaining: 1m 32s\n",
      "40:\tlearn: 0.5057103\ttotal: 3.95s\tremaining: 1m 32s\n",
      "41:\tlearn: 0.5002846\ttotal: 4.07s\tremaining: 1m 32s\n",
      "42:\tlearn: 0.4981990\ttotal: 4.16s\tremaining: 1m 32s\n",
      "43:\tlearn: 0.4954596\ttotal: 4.25s\tremaining: 1m 32s\n",
      "44:\tlearn: 0.4943570\ttotal: 4.34s\tremaining: 1m 32s\n",
      "45:\tlearn: 0.4909310\ttotal: 4.43s\tremaining: 1m 31s\n",
      "46:\tlearn: 0.4892658\ttotal: 4.51s\tremaining: 1m 31s\n",
      "47:\tlearn: 0.4877282\ttotal: 4.6s\tremaining: 1m 31s\n",
      "48:\tlearn: 0.4850824\ttotal: 4.68s\tremaining: 1m 30s\n",
      "49:\tlearn: 0.4845660\ttotal: 4.76s\tremaining: 1m 30s\n",
      "50:\tlearn: 0.4828563\ttotal: 4.85s\tremaining: 1m 30s\n",
      "51:\tlearn: 0.4817000\ttotal: 4.94s\tremaining: 1m 30s\n",
      "52:\tlearn: 0.4780527\ttotal: 5.02s\tremaining: 1m 29s\n",
      "53:\tlearn: 0.4770565\ttotal: 5.11s\tremaining: 1m 29s\n",
      "54:\tlearn: 0.4749680\ttotal: 5.19s\tremaining: 1m 29s\n",
      "55:\tlearn: 0.4732282\ttotal: 5.28s\tremaining: 1m 28s\n",
      "56:\tlearn: 0.4727722\ttotal: 5.36s\tremaining: 1m 28s\n",
      "57:\tlearn: 0.4721623\ttotal: 5.45s\tremaining: 1m 28s\n",
      "58:\tlearn: 0.4697560\ttotal: 5.54s\tremaining: 1m 28s\n",
      "59:\tlearn: 0.4685978\ttotal: 5.63s\tremaining: 1m 28s\n",
      "60:\tlearn: 0.4680272\ttotal: 5.72s\tremaining: 1m 28s\n",
      "61:\tlearn: 0.4671926\ttotal: 5.81s\tremaining: 1m 27s\n",
      "62:\tlearn: 0.4666688\ttotal: 5.9s\tremaining: 1m 27s\n",
      "63:\tlearn: 0.4642449\ttotal: 5.99s\tremaining: 1m 27s\n",
      "64:\tlearn: 0.4638854\ttotal: 6.08s\tremaining: 1m 27s\n",
      "65:\tlearn: 0.4630359\ttotal: 6.17s\tremaining: 1m 27s\n",
      "66:\tlearn: 0.4620262\ttotal: 6.27s\tremaining: 1m 27s\n",
      "67:\tlearn: 0.4615860\ttotal: 6.36s\tremaining: 1m 27s\n",
      "68:\tlearn: 0.4601925\ttotal: 6.45s\tremaining: 1m 26s\n",
      "69:\tlearn: 0.4595635\ttotal: 6.54s\tremaining: 1m 26s\n",
      "70:\tlearn: 0.4570603\ttotal: 6.63s\tremaining: 1m 26s\n",
      "71:\tlearn: 0.4567942\ttotal: 6.73s\tremaining: 1m 26s\n",
      "72:\tlearn: 0.4551153\ttotal: 6.82s\tremaining: 1m 26s\n",
      "73:\tlearn: 0.4548168\ttotal: 6.91s\tremaining: 1m 26s\n",
      "74:\tlearn: 0.4537240\ttotal: 6.99s\tremaining: 1m 26s\n",
      "75:\tlearn: 0.4528452\ttotal: 7.09s\tremaining: 1m 26s\n",
      "76:\tlearn: 0.4515937\ttotal: 7.18s\tremaining: 1m 26s\n",
      "77:\tlearn: 0.4504648\ttotal: 7.27s\tremaining: 1m 25s\n",
      "78:\tlearn: 0.4497669\ttotal: 7.36s\tremaining: 1m 25s\n",
      "79:\tlearn: 0.4481742\ttotal: 7.45s\tremaining: 1m 25s\n",
      "80:\tlearn: 0.4479422\ttotal: 7.61s\tremaining: 1m 26s\n",
      "81:\tlearn: 0.4474826\ttotal: 7.7s\tremaining: 1m 26s\n",
      "82:\tlearn: 0.4469683\ttotal: 7.78s\tremaining: 1m 25s\n",
      "83:\tlearn: 0.4457422\ttotal: 7.85s\tremaining: 1m 25s\n",
      "84:\tlearn: 0.4454059\ttotal: 7.93s\tremaining: 1m 25s\n",
      "85:\tlearn: 0.4451421\ttotal: 8.01s\tremaining: 1m 25s\n",
      "86:\tlearn: 0.4449528\ttotal: 8.1s\tremaining: 1m 24s\n",
      "87:\tlearn: 0.4444215\ttotal: 8.18s\tremaining: 1m 24s\n",
      "88:\tlearn: 0.4442003\ttotal: 8.26s\tremaining: 1m 24s\n",
      "89:\tlearn: 0.4437064\ttotal: 8.34s\tremaining: 1m 24s\n",
      "90:\tlearn: 0.4427961\ttotal: 8.44s\tremaining: 1m 24s\n",
      "91:\tlearn: 0.4425995\ttotal: 8.52s\tremaining: 1m 24s\n",
      "92:\tlearn: 0.4411210\ttotal: 8.62s\tremaining: 1m 24s\n",
      "93:\tlearn: 0.4403645\ttotal: 8.69s\tremaining: 1m 23s\n",
      "94:\tlearn: 0.4398799\ttotal: 8.78s\tremaining: 1m 23s\n",
      "95:\tlearn: 0.4390504\ttotal: 8.87s\tremaining: 1m 23s\n",
      "96:\tlearn: 0.4380043\ttotal: 8.96s\tremaining: 1m 23s\n",
      "97:\tlearn: 0.4377954\ttotal: 9.03s\tremaining: 1m 23s\n",
      "98:\tlearn: 0.4372012\ttotal: 9.11s\tremaining: 1m 22s\n",
      "99:\tlearn: 0.4364771\ttotal: 9.19s\tremaining: 1m 22s\n",
      "100:\tlearn: 0.4356329\ttotal: 9.28s\tremaining: 1m 22s\n",
      "101:\tlearn: 0.4347697\ttotal: 9.36s\tremaining: 1m 22s\n",
      "102:\tlearn: 0.4330624\ttotal: 9.52s\tremaining: 1m 22s\n",
      "103:\tlearn: 0.4321096\ttotal: 9.61s\tremaining: 1m 22s\n",
      "104:\tlearn: 0.4317216\ttotal: 9.7s\tremaining: 1m 22s\n",
      "105:\tlearn: 0.4311325\ttotal: 9.79s\tremaining: 1m 22s\n",
      "106:\tlearn: 0.4304514\ttotal: 9.88s\tremaining: 1m 22s\n",
      "107:\tlearn: 0.4302778\ttotal: 9.96s\tremaining: 1m 22s\n",
      "108:\tlearn: 0.4296642\ttotal: 10s\tremaining: 1m 22s\n",
      "109:\tlearn: 0.4286598\ttotal: 10.1s\tremaining: 1m 21s\n",
      "110:\tlearn: 0.4280342\ttotal: 10.2s\tremaining: 1m 21s\n",
      "111:\tlearn: 0.4274803\ttotal: 10.3s\tremaining: 1m 21s\n",
      "112:\tlearn: 0.4272325\ttotal: 10.4s\tremaining: 1m 21s\n",
      "113:\tlearn: 0.4262073\ttotal: 10.4s\tremaining: 1m 21s\n",
      "114:\tlearn: 0.4257231\ttotal: 10.5s\tremaining: 1m 20s\n",
      "115:\tlearn: 0.4254810\ttotal: 10.6s\tremaining: 1m 20s\n",
      "116:\tlearn: 0.4247973\ttotal: 10.7s\tremaining: 1m 20s\n",
      "117:\tlearn: 0.4245875\ttotal: 10.8s\tremaining: 1m 20s\n",
      "118:\tlearn: 0.4242424\ttotal: 10.8s\tremaining: 1m 20s\n",
      "119:\tlearn: 0.4236931\ttotal: 10.9s\tremaining: 1m 19s\n",
      "120:\tlearn: 0.4233534\ttotal: 11s\tremaining: 1m 19s\n",
      "121:\tlearn: 0.4227843\ttotal: 11.1s\tremaining: 1m 19s\n",
      "122:\tlearn: 0.4223656\ttotal: 11.2s\tremaining: 1m 19s\n",
      "123:\tlearn: 0.4217370\ttotal: 11.2s\tremaining: 1m 19s\n",
      "124:\tlearn: 0.4214926\ttotal: 11.3s\tremaining: 1m 19s\n",
      "125:\tlearn: 0.4213070\ttotal: 11.4s\tremaining: 1m 18s\n",
      "126:\tlearn: 0.4207340\ttotal: 11.5s\tremaining: 1m 18s\n",
      "127:\tlearn: 0.4205691\ttotal: 11.5s\tremaining: 1m 18s\n",
      "128:\tlearn: 0.4204020\ttotal: 11.6s\tremaining: 1m 18s\n",
      "129:\tlearn: 0.4200257\ttotal: 11.7s\tremaining: 1m 18s\n",
      "130:\tlearn: 0.4198069\ttotal: 11.8s\tremaining: 1m 18s\n",
      "131:\tlearn: 0.4189931\ttotal: 11.9s\tremaining: 1m 18s\n",
      "132:\tlearn: 0.4187247\ttotal: 12s\tremaining: 1m 17s\n",
      "133:\tlearn: 0.4178574\ttotal: 12s\tremaining: 1m 17s\n",
      "134:\tlearn: 0.4176659\ttotal: 12.1s\tremaining: 1m 17s\n",
      "135:\tlearn: 0.4167821\ttotal: 12.2s\tremaining: 1m 17s\n",
      "136:\tlearn: 0.4159909\ttotal: 12.3s\tremaining: 1m 17s\n",
      "137:\tlearn: 0.4152003\ttotal: 12.4s\tremaining: 1m 17s\n",
      "138:\tlearn: 0.4150339\ttotal: 12.5s\tremaining: 1m 17s\n",
      "139:\tlearn: 0.4147288\ttotal: 12.6s\tremaining: 1m 17s\n",
      "140:\tlearn: 0.4144361\ttotal: 12.7s\tremaining: 1m 17s\n",
      "141:\tlearn: 0.4142696\ttotal: 12.8s\tremaining: 1m 17s\n",
      "142:\tlearn: 0.4141131\ttotal: 12.8s\tremaining: 1m 17s\n",
      "143:\tlearn: 0.4138474\ttotal: 12.9s\tremaining: 1m 16s\n",
      "144:\tlearn: 0.4136302\ttotal: 13s\tremaining: 1m 16s\n",
      "145:\tlearn: 0.4134460\ttotal: 13.1s\tremaining: 1m 16s\n",
      "146:\tlearn: 0.4133077\ttotal: 13.2s\tremaining: 1m 16s\n",
      "147:\tlearn: 0.4128593\ttotal: 13.2s\tremaining: 1m 16s\n",
      "148:\tlearn: 0.4126748\ttotal: 13.3s\tremaining: 1m 16s\n",
      "149:\tlearn: 0.4125329\ttotal: 13.4s\tremaining: 1m 16s\n",
      "150:\tlearn: 0.4122221\ttotal: 13.5s\tremaining: 1m 16s\n",
      "151:\tlearn: 0.4117013\ttotal: 13.6s\tremaining: 1m 15s\n",
      "152:\tlearn: 0.4113432\ttotal: 13.7s\tremaining: 1m 15s\n",
      "153:\tlearn: 0.4107461\ttotal: 13.8s\tremaining: 1m 15s\n",
      "154:\tlearn: 0.4105978\ttotal: 13.9s\tremaining: 1m 15s\n",
      "155:\tlearn: 0.4099916\ttotal: 14s\tremaining: 1m 15s\n",
      "156:\tlearn: 0.4096268\ttotal: 14s\tremaining: 1m 15s\n",
      "157:\tlearn: 0.4094434\ttotal: 14.1s\tremaining: 1m 15s\n",
      "158:\tlearn: 0.4093046\ttotal: 14.2s\tremaining: 1m 15s\n",
      "159:\tlearn: 0.4086840\ttotal: 14.3s\tremaining: 1m 14s\n",
      "160:\tlearn: 0.4085308\ttotal: 14.4s\tremaining: 1m 14s\n",
      "161:\tlearn: 0.4083822\ttotal: 14.4s\tremaining: 1m 14s\n",
      "162:\tlearn: 0.4079104\ttotal: 14.5s\tremaining: 1m 14s\n",
      "163:\tlearn: 0.4070762\ttotal: 14.6s\tremaining: 1m 14s\n",
      "164:\tlearn: 0.4068771\ttotal: 14.7s\tremaining: 1m 14s\n",
      "165:\tlearn: 0.4062787\ttotal: 14.8s\tremaining: 1m 14s\n",
      "166:\tlearn: 0.4060019\ttotal: 14.9s\tremaining: 1m 14s\n",
      "167:\tlearn: 0.4057872\ttotal: 15s\tremaining: 1m 14s\n",
      "168:\tlearn: 0.4056238\ttotal: 15.1s\tremaining: 1m 14s\n",
      "169:\tlearn: 0.4053541\ttotal: 15.2s\tremaining: 1m 14s\n",
      "170:\tlearn: 0.4052019\ttotal: 15.3s\tremaining: 1m 14s\n",
      "171:\tlearn: 0.4047793\ttotal: 15.3s\tremaining: 1m 13s\n",
      "172:\tlearn: 0.4046399\ttotal: 15.4s\tremaining: 1m 13s\n",
      "173:\tlearn: 0.4044966\ttotal: 15.5s\tremaining: 1m 13s\n",
      "174:\tlearn: 0.4043414\ttotal: 15.6s\tremaining: 1m 13s\n",
      "175:\tlearn: 0.4042043\ttotal: 15.7s\tremaining: 1m 13s\n",
      "176:\tlearn: 0.4035592\ttotal: 15.7s\tremaining: 1m 13s\n",
      "177:\tlearn: 0.4032252\ttotal: 15.8s\tremaining: 1m 13s\n",
      "178:\tlearn: 0.4026656\ttotal: 15.9s\tremaining: 1m 12s\n",
      "179:\tlearn: 0.4023788\ttotal: 16s\tremaining: 1m 12s\n",
      "180:\tlearn: 0.4020331\ttotal: 16.1s\tremaining: 1m 12s\n",
      "181:\tlearn: 0.4016033\ttotal: 16.2s\tremaining: 1m 12s\n",
      "182:\tlearn: 0.4010719\ttotal: 16.2s\tremaining: 1m 12s\n",
      "183:\tlearn: 0.4007298\ttotal: 16.3s\tremaining: 1m 12s\n",
      "184:\tlearn: 0.4003358\ttotal: 16.4s\tremaining: 1m 12s\n",
      "185:\tlearn: 0.4000932\ttotal: 16.5s\tremaining: 1m 12s\n",
      "186:\tlearn: 0.3995296\ttotal: 16.6s\tremaining: 1m 12s\n",
      "187:\tlearn: 0.3992786\ttotal: 16.6s\tremaining: 1m 11s\n",
      "188:\tlearn: 0.3991341\ttotal: 16.7s\tremaining: 1m 11s\n",
      "189:\tlearn: 0.3989919\ttotal: 16.8s\tremaining: 1m 11s\n",
      "190:\tlearn: 0.3986942\ttotal: 16.9s\tremaining: 1m 11s\n",
      "191:\tlearn: 0.3985746\ttotal: 17s\tremaining: 1m 11s\n",
      "192:\tlearn: 0.3981093\ttotal: 17.1s\tremaining: 1m 11s\n",
      "193:\tlearn: 0.3978504\ttotal: 17.1s\tremaining: 1m 11s\n",
      "194:\tlearn: 0.3976678\ttotal: 17.2s\tremaining: 1m 11s\n",
      "195:\tlearn: 0.3975213\ttotal: 17.3s\tremaining: 1m 11s\n",
      "196:\tlearn: 0.3973728\ttotal: 17.4s\tremaining: 1m 10s\n",
      "197:\tlearn: 0.3972525\ttotal: 17.6s\tremaining: 1m 11s\n",
      "198:\tlearn: 0.3970943\ttotal: 17.7s\tremaining: 1m 11s\n",
      "199:\tlearn: 0.3969566\ttotal: 17.8s\tremaining: 1m 11s\n",
      "200:\tlearn: 0.3965522\ttotal: 17.8s\tremaining: 1m 10s\n",
      "201:\tlearn: 0.3959537\ttotal: 17.9s\tremaining: 1m 10s\n",
      "202:\tlearn: 0.3957967\ttotal: 18s\tremaining: 1m 10s\n",
      "203:\tlearn: 0.3956485\ttotal: 18.1s\tremaining: 1m 10s\n",
      "204:\tlearn: 0.3952882\ttotal: 18.2s\tremaining: 1m 10s\n",
      "205:\tlearn: 0.3948608\ttotal: 18.3s\tremaining: 1m 10s\n",
      "206:\tlearn: 0.3947187\ttotal: 18.4s\tremaining: 1m 10s\n",
      "207:\tlearn: 0.3941789\ttotal: 18.5s\tremaining: 1m 10s\n",
      "208:\tlearn: 0.3937231\ttotal: 18.6s\tremaining: 1m 10s\n",
      "209:\tlearn: 0.3934117\ttotal: 18.7s\tremaining: 1m 10s\n",
      "210:\tlearn: 0.3928910\ttotal: 18.8s\tremaining: 1m 10s\n",
      "211:\tlearn: 0.3927535\ttotal: 18.9s\tremaining: 1m 10s\n",
      "212:\tlearn: 0.3920757\ttotal: 19s\tremaining: 1m 10s\n",
      "213:\tlearn: 0.3916081\ttotal: 19.1s\tremaining: 1m 10s\n",
      "214:\tlearn: 0.3913474\ttotal: 19.2s\tremaining: 1m 10s\n",
      "215:\tlearn: 0.3911369\ttotal: 19.3s\tremaining: 1m 10s\n",
      "216:\tlearn: 0.3910094\ttotal: 19.4s\tremaining: 1m 9s\n",
      "217:\tlearn: 0.3907221\ttotal: 19.5s\tremaining: 1m 9s\n",
      "218:\tlearn: 0.3905831\ttotal: 19.6s\tremaining: 1m 9s\n",
      "219:\tlearn: 0.3903486\ttotal: 19.6s\tremaining: 1m 9s\n",
      "220:\tlearn: 0.3900008\ttotal: 19.7s\tremaining: 1m 9s\n",
      "221:\tlearn: 0.3896994\ttotal: 19.8s\tremaining: 1m 9s\n",
      "222:\tlearn: 0.3895999\ttotal: 19.9s\tremaining: 1m 9s\n",
      "223:\tlearn: 0.3892769\ttotal: 20s\tremaining: 1m 9s\n",
      "224:\tlearn: 0.3891664\ttotal: 20.1s\tremaining: 1m 9s\n",
      "225:\tlearn: 0.3890356\ttotal: 20.2s\tremaining: 1m 9s\n",
      "226:\tlearn: 0.3889236\ttotal: 20.3s\tremaining: 1m 9s\n",
      "227:\tlearn: 0.3887931\ttotal: 20.4s\tremaining: 1m 9s\n",
      "228:\tlearn: 0.3881747\ttotal: 20.5s\tremaining: 1m 9s\n",
      "229:\tlearn: 0.3880683\ttotal: 20.6s\tremaining: 1m 9s\n",
      "230:\tlearn: 0.3879493\ttotal: 20.7s\tremaining: 1m 9s\n",
      "231:\tlearn: 0.3878426\ttotal: 20.8s\tremaining: 1m 8s\n",
      "232:\tlearn: 0.3877129\ttotal: 20.9s\tremaining: 1m 8s\n",
      "233:\tlearn: 0.3875046\ttotal: 21s\tremaining: 1m 8s\n",
      "234:\tlearn: 0.3873778\ttotal: 21.1s\tremaining: 1m 8s\n",
      "235:\tlearn: 0.3869680\ttotal: 21.2s\tremaining: 1m 8s\n",
      "236:\tlearn: 0.3868509\ttotal: 21.3s\tremaining: 1m 8s\n",
      "237:\tlearn: 0.3866966\ttotal: 21.4s\tremaining: 1m 8s\n",
      "238:\tlearn: 0.3865687\ttotal: 21.5s\tremaining: 1m 8s\n",
      "239:\tlearn: 0.3864436\ttotal: 21.6s\tremaining: 1m 8s\n",
      "240:\tlearn: 0.3861892\ttotal: 21.7s\tremaining: 1m 8s\n",
      "241:\tlearn: 0.3860695\ttotal: 21.8s\tremaining: 1m 8s\n",
      "242:\tlearn: 0.3859604\ttotal: 21.8s\tremaining: 1m 8s\n",
      "243:\tlearn: 0.3857408\ttotal: 21.9s\tremaining: 1m 7s\n",
      "244:\tlearn: 0.3856119\ttotal: 22s\tremaining: 1m 7s\n",
      "245:\tlearn: 0.3855197\ttotal: 22.1s\tremaining: 1m 7s\n",
      "246:\tlearn: 0.3853997\ttotal: 22.2s\tremaining: 1m 7s\n",
      "247:\tlearn: 0.3852546\ttotal: 22.2s\tremaining: 1m 7s\n",
      "248:\tlearn: 0.3849318\ttotal: 22.4s\tremaining: 1m 7s\n",
      "249:\tlearn: 0.3848321\ttotal: 22.5s\tremaining: 1m 7s\n",
      "250:\tlearn: 0.3847188\ttotal: 22.6s\tremaining: 1m 7s\n",
      "251:\tlearn: 0.3846210\ttotal: 22.7s\tremaining: 1m 7s\n",
      "252:\tlearn: 0.3845036\ttotal: 22.7s\tremaining: 1m 7s\n",
      "253:\tlearn: 0.3843920\ttotal: 22.8s\tremaining: 1m 7s\n",
      "254:\tlearn: 0.3841092\ttotal: 22.9s\tremaining: 1m 6s\n",
      "255:\tlearn: 0.3840145\ttotal: 23s\tremaining: 1m 6s\n",
      "256:\tlearn: 0.3839292\ttotal: 23.1s\tremaining: 1m 6s\n",
      "257:\tlearn: 0.3838079\ttotal: 23.2s\tremaining: 1m 6s\n",
      "258:\tlearn: 0.3830212\ttotal: 23.3s\tremaining: 1m 6s\n",
      "259:\tlearn: 0.3828896\ttotal: 23.3s\tremaining: 1m 6s\n",
      "260:\tlearn: 0.3827941\ttotal: 23.4s\tremaining: 1m 6s\n",
      "261:\tlearn: 0.3827113\ttotal: 23.5s\tremaining: 1m 6s\n",
      "262:\tlearn: 0.3826249\ttotal: 23.6s\tremaining: 1m 6s\n",
      "263:\tlearn: 0.3825069\ttotal: 23.6s\tremaining: 1m 5s\n",
      "264:\tlearn: 0.3823733\ttotal: 23.8s\tremaining: 1m 5s\n",
      "265:\tlearn: 0.3822659\ttotal: 23.8s\tremaining: 1m 5s\n",
      "266:\tlearn: 0.3821453\ttotal: 23.9s\tremaining: 1m 5s\n",
      "267:\tlearn: 0.3819084\ttotal: 24s\tremaining: 1m 5s\n",
      "268:\tlearn: 0.3815885\ttotal: 24.1s\tremaining: 1m 5s\n",
      "269:\tlearn: 0.3814867\ttotal: 24.2s\tremaining: 1m 5s\n",
      "270:\tlearn: 0.3814057\ttotal: 24.2s\tremaining: 1m 5s\n",
      "271:\tlearn: 0.3812709\ttotal: 24.3s\tremaining: 1m 5s\n",
      "272:\tlearn: 0.3811787\ttotal: 24.4s\tremaining: 1m 5s\n",
      "273:\tlearn: 0.3810476\ttotal: 24.5s\tremaining: 1m 4s\n",
      "274:\tlearn: 0.3809507\ttotal: 24.6s\tremaining: 1m 4s\n",
      "275:\tlearn: 0.3807813\ttotal: 24.7s\tremaining: 1m 4s\n",
      "276:\tlearn: 0.3806794\ttotal: 24.8s\tremaining: 1m 4s\n",
      "277:\tlearn: 0.3805685\ttotal: 24.9s\tremaining: 1m 4s\n",
      "278:\tlearn: 0.3804765\ttotal: 24.9s\tremaining: 1m 4s\n",
      "279:\tlearn: 0.3803889\ttotal: 25s\tremaining: 1m 4s\n",
      "280:\tlearn: 0.3801918\ttotal: 25.1s\tremaining: 1m 4s\n",
      "281:\tlearn: 0.3800683\ttotal: 25.2s\tremaining: 1m 4s\n",
      "282:\tlearn: 0.3799724\ttotal: 25.3s\tremaining: 1m 4s\n",
      "283:\tlearn: 0.3798650\ttotal: 25.3s\tremaining: 1m 3s\n",
      "284:\tlearn: 0.3797554\ttotal: 25.4s\tremaining: 1m 3s\n",
      "285:\tlearn: 0.3796331\ttotal: 25.5s\tremaining: 1m 3s\n",
      "286:\tlearn: 0.3793974\ttotal: 25.6s\tremaining: 1m 3s\n",
      "287:\tlearn: 0.3792887\ttotal: 25.7s\tremaining: 1m 3s\n",
      "288:\tlearn: 0.3791399\ttotal: 25.8s\tremaining: 1m 3s\n",
      "289:\tlearn: 0.3790515\ttotal: 25.8s\tremaining: 1m 3s\n",
      "290:\tlearn: 0.3789610\ttotal: 25.9s\tremaining: 1m 3s\n",
      "291:\tlearn: 0.3786717\ttotal: 26s\tremaining: 1m 3s\n",
      "292:\tlearn: 0.3785620\ttotal: 26.1s\tremaining: 1m 3s\n",
      "293:\tlearn: 0.3784607\ttotal: 26.2s\tremaining: 1m 2s\n",
      "294:\tlearn: 0.3782814\ttotal: 26.3s\tremaining: 1m 2s\n",
      "295:\tlearn: 0.3781632\ttotal: 26.4s\tremaining: 1m 2s\n",
      "296:\tlearn: 0.3780399\ttotal: 26.5s\tremaining: 1m 2s\n",
      "297:\tlearn: 0.3779579\ttotal: 26.6s\tremaining: 1m 2s\n",
      "298:\tlearn: 0.3778317\ttotal: 26.7s\tremaining: 1m 2s\n",
      "299:\tlearn: 0.3776967\ttotal: 26.8s\tremaining: 1m 2s\n",
      "300:\tlearn: 0.3775846\ttotal: 26.8s\tremaining: 1m 2s\n",
      "301:\tlearn: 0.3775009\ttotal: 26.9s\tremaining: 1m 2s\n",
      "302:\tlearn: 0.3773811\ttotal: 27s\tremaining: 1m 2s\n",
      "303:\tlearn: 0.3771911\ttotal: 27.1s\tremaining: 1m 2s\n",
      "304:\tlearn: 0.3770825\ttotal: 27.2s\tremaining: 1m 1s\n",
      "305:\tlearn: 0.3767081\ttotal: 27.3s\tremaining: 1m 1s\n",
      "306:\tlearn: 0.3766077\ttotal: 27.4s\tremaining: 1m 1s\n",
      "307:\tlearn: 0.3765079\ttotal: 27.4s\tremaining: 1m 1s\n",
      "308:\tlearn: 0.3763905\ttotal: 27.5s\tremaining: 1m 1s\n",
      "309:\tlearn: 0.3763065\ttotal: 27.6s\tremaining: 1m 1s\n",
      "310:\tlearn: 0.3762160\ttotal: 27.7s\tremaining: 1m 1s\n",
      "311:\tlearn: 0.3761197\ttotal: 27.8s\tremaining: 1m 1s\n",
      "312:\tlearn: 0.3760181\ttotal: 27.9s\tremaining: 1m 1s\n",
      "313:\tlearn: 0.3758887\ttotal: 28s\tremaining: 1m 1s\n",
      "314:\tlearn: 0.3757849\ttotal: 28.1s\tremaining: 1m 1s\n",
      "315:\tlearn: 0.3756875\ttotal: 28.1s\tremaining: 1m\n",
      "316:\tlearn: 0.3755978\ttotal: 28.2s\tremaining: 1m\n",
      "317:\tlearn: 0.3755023\ttotal: 28.3s\tremaining: 1m\n",
      "318:\tlearn: 0.3753989\ttotal: 28.4s\tremaining: 1m\n",
      "319:\tlearn: 0.3751923\ttotal: 28.5s\tremaining: 1m\n",
      "320:\tlearn: 0.3750959\ttotal: 28.6s\tremaining: 1m\n",
      "321:\tlearn: 0.3749498\ttotal: 28.7s\tremaining: 1m\n",
      "322:\tlearn: 0.3748710\ttotal: 28.7s\tremaining: 1m\n",
      "323:\tlearn: 0.3746755\ttotal: 28.8s\tremaining: 1m\n",
      "324:\tlearn: 0.3745828\ttotal: 28.9s\tremaining: 1m\n",
      "325:\tlearn: 0.3744787\ttotal: 29s\tremaining: 59.9s\n",
      "326:\tlearn: 0.3743803\ttotal: 29.1s\tremaining: 59.9s\n",
      "327:\tlearn: 0.3743364\ttotal: 29.2s\tremaining: 59.7s\n",
      "328:\tlearn: 0.3742482\ttotal: 29.2s\tremaining: 59.6s\n",
      "329:\tlearn: 0.3741660\ttotal: 29.3s\tremaining: 59.5s\n",
      "330:\tlearn: 0.3739749\ttotal: 29.4s\tremaining: 59.4s\n",
      "331:\tlearn: 0.3738902\ttotal: 29.5s\tremaining: 59.3s\n",
      "332:\tlearn: 0.3738295\ttotal: 29.6s\tremaining: 59.2s\n",
      "333:\tlearn: 0.3737362\ttotal: 29.7s\tremaining: 59.2s\n",
      "334:\tlearn: 0.3736618\ttotal: 29.8s\tremaining: 59.1s\n",
      "335:\tlearn: 0.3735710\ttotal: 29.9s\tremaining: 59s\n",
      "336:\tlearn: 0.3734732\ttotal: 30s\tremaining: 59s\n",
      "337:\tlearn: 0.3732826\ttotal: 30.1s\tremaining: 59s\n",
      "338:\tlearn: 0.3731959\ttotal: 30.2s\tremaining: 58.9s\n",
      "339:\tlearn: 0.3731062\ttotal: 30.3s\tremaining: 58.9s\n",
      "340:\tlearn: 0.3728974\ttotal: 30.4s\tremaining: 58.8s\n",
      "341:\tlearn: 0.3727725\ttotal: 30.5s\tremaining: 58.7s\n",
      "342:\tlearn: 0.3724623\ttotal: 30.6s\tremaining: 58.6s\n",
      "343:\tlearn: 0.3723566\ttotal: 30.7s\tremaining: 58.5s\n",
      "344:\tlearn: 0.3722667\ttotal: 30.7s\tremaining: 58.4s\n",
      "345:\tlearn: 0.3721135\ttotal: 30.8s\tremaining: 58.3s\n",
      "346:\tlearn: 0.3719558\ttotal: 30.9s\tremaining: 58.2s\n",
      "347:\tlearn: 0.3718562\ttotal: 31s\tremaining: 58.1s\n",
      "348:\tlearn: 0.3717157\ttotal: 31.1s\tremaining: 58s\n",
      "349:\tlearn: 0.3715900\ttotal: 31.2s\tremaining: 57.9s\n",
      "350:\tlearn: 0.3715246\ttotal: 31.3s\tremaining: 57.8s\n",
      "351:\tlearn: 0.3714704\ttotal: 31.3s\tremaining: 57.7s\n",
      "352:\tlearn: 0.3712711\ttotal: 31.4s\tremaining: 57.6s\n",
      "353:\tlearn: 0.3711630\ttotal: 31.5s\tremaining: 57.5s\n",
      "354:\tlearn: 0.3710635\ttotal: 31.6s\tremaining: 57.4s\n",
      "355:\tlearn: 0.3709786\ttotal: 31.7s\tremaining: 57.3s\n",
      "356:\tlearn: 0.3705102\ttotal: 31.7s\tremaining: 57.2s\n",
      "357:\tlearn: 0.3704268\ttotal: 31.8s\tremaining: 57.1s\n",
      "358:\tlearn: 0.3703471\ttotal: 31.9s\tremaining: 56.9s\n",
      "359:\tlearn: 0.3702544\ttotal: 32s\tremaining: 56.8s\n",
      "360:\tlearn: 0.3701623\ttotal: 32s\tremaining: 56.7s\n",
      "361:\tlearn: 0.3700334\ttotal: 32.1s\tremaining: 56.6s\n",
      "362:\tlearn: 0.3699430\ttotal: 32.2s\tremaining: 56.5s\n",
      "363:\tlearn: 0.3698400\ttotal: 32.3s\tremaining: 56.5s\n",
      "364:\tlearn: 0.3697218\ttotal: 32.4s\tremaining: 56.4s\n",
      "365:\tlearn: 0.3696381\ttotal: 32.5s\tremaining: 56.3s\n",
      "366:\tlearn: 0.3695637\ttotal: 32.6s\tremaining: 56.2s\n",
      "367:\tlearn: 0.3694708\ttotal: 32.6s\tremaining: 56.1s\n",
      "368:\tlearn: 0.3693769\ttotal: 32.7s\tremaining: 56s\n",
      "369:\tlearn: 0.3692849\ttotal: 32.8s\tremaining: 55.9s\n",
      "370:\tlearn: 0.3692079\ttotal: 32.9s\tremaining: 55.8s\n",
      "371:\tlearn: 0.3690915\ttotal: 33s\tremaining: 55.7s\n",
      "372:\tlearn: 0.3690153\ttotal: 33.1s\tremaining: 55.6s\n",
      "373:\tlearn: 0.3688962\ttotal: 33.2s\tremaining: 55.5s\n",
      "374:\tlearn: 0.3687966\ttotal: 33.2s\tremaining: 55.4s\n",
      "375:\tlearn: 0.3687113\ttotal: 33.3s\tremaining: 55.3s\n",
      "376:\tlearn: 0.3685859\ttotal: 33.4s\tremaining: 55.2s\n",
      "377:\tlearn: 0.3685161\ttotal: 33.5s\tremaining: 55.1s\n",
      "378:\tlearn: 0.3684244\ttotal: 33.6s\tremaining: 55s\n",
      "379:\tlearn: 0.3682294\ttotal: 33.7s\tremaining: 54.9s\n",
      "380:\tlearn: 0.3681490\ttotal: 33.7s\tremaining: 54.8s\n",
      "381:\tlearn: 0.3680426\ttotal: 33.8s\tremaining: 54.7s\n",
      "382:\tlearn: 0.3679338\ttotal: 33.9s\tremaining: 54.6s\n",
      "383:\tlearn: 0.3678334\ttotal: 34s\tremaining: 54.6s\n",
      "384:\tlearn: 0.3677511\ttotal: 34.1s\tremaining: 54.5s\n",
      "385:\tlearn: 0.3676589\ttotal: 34.2s\tremaining: 54.4s\n",
      "386:\tlearn: 0.3675844\ttotal: 34.3s\tremaining: 54.3s\n",
      "387:\tlearn: 0.3675053\ttotal: 34.4s\tremaining: 54.2s\n",
      "388:\tlearn: 0.3674170\ttotal: 34.5s\tremaining: 54.2s\n",
      "389:\tlearn: 0.3673279\ttotal: 34.6s\tremaining: 54.1s\n",
      "390:\tlearn: 0.3672470\ttotal: 34.7s\tremaining: 54s\n",
      "391:\tlearn: 0.3671598\ttotal: 34.8s\tremaining: 53.9s\n",
      "392:\tlearn: 0.3670589\ttotal: 34.9s\tremaining: 53.8s\n",
      "393:\tlearn: 0.3669831\ttotal: 35s\tremaining: 53.8s\n",
      "394:\tlearn: 0.3668961\ttotal: 35s\tremaining: 53.7s\n",
      "395:\tlearn: 0.3668173\ttotal: 35.1s\tremaining: 53.6s\n",
      "396:\tlearn: 0.3667322\ttotal: 35.2s\tremaining: 53.5s\n",
      "397:\tlearn: 0.3666451\ttotal: 35.3s\tremaining: 53.4s\n",
      "398:\tlearn: 0.3665564\ttotal: 35.4s\tremaining: 53.3s\n",
      "399:\tlearn: 0.3664823\ttotal: 35.5s\tremaining: 53.3s\n",
      "400:\tlearn: 0.3663649\ttotal: 35.6s\tremaining: 53.2s\n",
      "401:\tlearn: 0.3662287\ttotal: 35.7s\tremaining: 53.1s\n",
      "402:\tlearn: 0.3661899\ttotal: 35.7s\tremaining: 53s\n",
      "403:\tlearn: 0.3660594\ttotal: 35.8s\tremaining: 52.9s\n",
      "404:\tlearn: 0.3659936\ttotal: 35.9s\tremaining: 52.8s\n",
      "405:\tlearn: 0.3659111\ttotal: 36s\tremaining: 52.7s\n",
      "406:\tlearn: 0.3658158\ttotal: 36.1s\tremaining: 52.6s\n",
      "407:\tlearn: 0.3657245\ttotal: 36.2s\tremaining: 52.5s\n",
      "408:\tlearn: 0.3655538\ttotal: 36.3s\tremaining: 52.4s\n",
      "409:\tlearn: 0.3654663\ttotal: 36.4s\tremaining: 52.3s\n",
      "410:\tlearn: 0.3653918\ttotal: 36.5s\tremaining: 52.3s\n",
      "411:\tlearn: 0.3653088\ttotal: 36.5s\tremaining: 52.2s\n",
      "412:\tlearn: 0.3652094\ttotal: 36.6s\tremaining: 52.1s\n",
      "413:\tlearn: 0.3648151\ttotal: 36.7s\tremaining: 52s\n",
      "414:\tlearn: 0.3647204\ttotal: 36.8s\tremaining: 51.9s\n",
      "415:\tlearn: 0.3646390\ttotal: 36.9s\tremaining: 51.8s\n",
      "416:\tlearn: 0.3645251\ttotal: 37s\tremaining: 51.7s\n",
      "417:\tlearn: 0.3644392\ttotal: 37s\tremaining: 51.6s\n",
      "418:\tlearn: 0.3643792\ttotal: 37.1s\tremaining: 51.5s\n",
      "419:\tlearn: 0.3642919\ttotal: 37.2s\tremaining: 51.4s\n",
      "420:\tlearn: 0.3642096\ttotal: 37.3s\tremaining: 51.3s\n",
      "421:\tlearn: 0.3640488\ttotal: 37.4s\tremaining: 51.2s\n",
      "422:\tlearn: 0.3639749\ttotal: 37.4s\tremaining: 51.1s\n",
      "423:\tlearn: 0.3638015\ttotal: 37.5s\tremaining: 51s\n",
      "424:\tlearn: 0.3637321\ttotal: 37.6s\tremaining: 50.9s\n",
      "425:\tlearn: 0.3636580\ttotal: 37.7s\tremaining: 50.8s\n",
      "426:\tlearn: 0.3635916\ttotal: 37.8s\tremaining: 50.7s\n",
      "427:\tlearn: 0.3635228\ttotal: 37.8s\tremaining: 50.6s\n",
      "428:\tlearn: 0.3634342\ttotal: 37.9s\tremaining: 50.5s\n",
      "429:\tlearn: 0.3633438\ttotal: 38s\tremaining: 50.4s\n",
      "430:\tlearn: 0.3632563\ttotal: 38.1s\tremaining: 50.3s\n",
      "431:\tlearn: 0.3631769\ttotal: 38.2s\tremaining: 50.2s\n",
      "432:\tlearn: 0.3630670\ttotal: 38.3s\tremaining: 50.1s\n",
      "433:\tlearn: 0.3629364\ttotal: 38.3s\tremaining: 50s\n",
      "434:\tlearn: 0.3628502\ttotal: 38.4s\tremaining: 49.9s\n",
      "435:\tlearn: 0.3627618\ttotal: 38.5s\tremaining: 49.8s\n",
      "436:\tlearn: 0.3627455\ttotal: 38.6s\tremaining: 49.7s\n",
      "437:\tlearn: 0.3626864\ttotal: 38.7s\tremaining: 49.6s\n",
      "438:\tlearn: 0.3626124\ttotal: 38.7s\tremaining: 49.5s\n",
      "439:\tlearn: 0.3622732\ttotal: 38.8s\tremaining: 49.4s\n",
      "440:\tlearn: 0.3622027\ttotal: 38.9s\tremaining: 49.3s\n",
      "441:\tlearn: 0.3621140\ttotal: 39s\tremaining: 49.2s\n",
      "442:\tlearn: 0.3620483\ttotal: 39s\tremaining: 49.1s\n",
      "443:\tlearn: 0.3619374\ttotal: 39.1s\tremaining: 49s\n",
      "444:\tlearn: 0.3617689\ttotal: 39.2s\tremaining: 48.9s\n",
      "445:\tlearn: 0.3616793\ttotal: 39.3s\tremaining: 48.8s\n",
      "446:\tlearn: 0.3615926\ttotal: 39.4s\tremaining: 48.8s\n",
      "447:\tlearn: 0.3615311\ttotal: 39.6s\tremaining: 48.8s\n",
      "448:\tlearn: 0.3614398\ttotal: 39.7s\tremaining: 48.8s\n",
      "449:\tlearn: 0.3610845\ttotal: 39.8s\tremaining: 48.7s\n",
      "450:\tlearn: 0.3609734\ttotal: 39.9s\tremaining: 48.6s\n",
      "451:\tlearn: 0.3609102\ttotal: 40s\tremaining: 48.5s\n",
      "452:\tlearn: 0.3608409\ttotal: 40.2s\tremaining: 48.5s\n",
      "453:\tlearn: 0.3607544\ttotal: 40.3s\tremaining: 48.4s\n",
      "454:\tlearn: 0.3605079\ttotal: 40.4s\tremaining: 48.4s\n",
      "455:\tlearn: 0.3603988\ttotal: 40.5s\tremaining: 48.4s\n",
      "456:\tlearn: 0.3603159\ttotal: 40.6s\tremaining: 48.3s\n",
      "457:\tlearn: 0.3602171\ttotal: 40.8s\tremaining: 48.3s\n",
      "458:\tlearn: 0.3601373\ttotal: 40.9s\tremaining: 48.2s\n",
      "459:\tlearn: 0.3600604\ttotal: 41s\tremaining: 48.1s\n",
      "460:\tlearn: 0.3599846\ttotal: 41.1s\tremaining: 48s\n",
      "461:\tlearn: 0.3599004\ttotal: 41.2s\tremaining: 48s\n",
      "462:\tlearn: 0.3598220\ttotal: 41.3s\tremaining: 47.9s\n",
      "463:\tlearn: 0.3597499\ttotal: 41.4s\tremaining: 47.9s\n",
      "464:\tlearn: 0.3596744\ttotal: 41.5s\tremaining: 47.8s\n",
      "465:\tlearn: 0.3594888\ttotal: 41.6s\tremaining: 47.7s\n",
      "466:\tlearn: 0.3594118\ttotal: 41.7s\tremaining: 47.6s\n",
      "467:\tlearn: 0.3593112\ttotal: 41.9s\tremaining: 47.6s\n",
      "468:\tlearn: 0.3591440\ttotal: 42s\tremaining: 47.5s\n",
      "469:\tlearn: 0.3590727\ttotal: 42.1s\tremaining: 47.5s\n",
      "470:\tlearn: 0.3589864\ttotal: 42.2s\tremaining: 47.4s\n",
      "471:\tlearn: 0.3588972\ttotal: 42.3s\tremaining: 47.3s\n",
      "472:\tlearn: 0.3588212\ttotal: 42.4s\tremaining: 47.3s\n",
      "473:\tlearn: 0.3587434\ttotal: 42.5s\tremaining: 47.2s\n",
      "474:\tlearn: 0.3586693\ttotal: 42.6s\tremaining: 47.1s\n",
      "475:\tlearn: 0.3585884\ttotal: 42.7s\tremaining: 47s\n",
      "476:\tlearn: 0.3585212\ttotal: 42.8s\tremaining: 46.9s\n",
      "477:\tlearn: 0.3584479\ttotal: 42.9s\tremaining: 46.8s\n",
      "478:\tlearn: 0.3582019\ttotal: 43s\tremaining: 46.7s\n",
      "479:\tlearn: 0.3581426\ttotal: 43s\tremaining: 46.6s\n",
      "480:\tlearn: 0.3581008\ttotal: 43.1s\tremaining: 46.5s\n",
      "481:\tlearn: 0.3580304\ttotal: 43.2s\tremaining: 46.4s\n",
      "482:\tlearn: 0.3579486\ttotal: 43.3s\tremaining: 46.3s\n",
      "483:\tlearn: 0.3578639\ttotal: 43.4s\tremaining: 46.3s\n",
      "484:\tlearn: 0.3577827\ttotal: 43.5s\tremaining: 46.2s\n",
      "485:\tlearn: 0.3577076\ttotal: 43.6s\tremaining: 46.1s\n",
      "486:\tlearn: 0.3575912\ttotal: 43.6s\tremaining: 46s\n",
      "487:\tlearn: 0.3575106\ttotal: 43.7s\tremaining: 45.9s\n",
      "488:\tlearn: 0.3572651\ttotal: 43.8s\tremaining: 45.8s\n",
      "489:\tlearn: 0.3572025\ttotal: 43.9s\tremaining: 45.7s\n",
      "490:\tlearn: 0.3571155\ttotal: 44s\tremaining: 45.6s\n",
      "491:\tlearn: 0.3570377\ttotal: 44.1s\tremaining: 45.5s\n",
      "492:\tlearn: 0.3569396\ttotal: 44.2s\tremaining: 45.4s\n",
      "493:\tlearn: 0.3568657\ttotal: 44.3s\tremaining: 45.3s\n",
      "494:\tlearn: 0.3567847\ttotal: 44.4s\tremaining: 45.3s\n",
      "495:\tlearn: 0.3567447\ttotal: 44.4s\tremaining: 45.1s\n",
      "496:\tlearn: 0.3567307\ttotal: 44.5s\tremaining: 45s\n",
      "497:\tlearn: 0.3566761\ttotal: 44.6s\tremaining: 44.9s\n",
      "498:\tlearn: 0.3561700\ttotal: 44.7s\tremaining: 44.8s\n",
      "499:\tlearn: 0.3560760\ttotal: 44.7s\tremaining: 44.7s\n",
      "500:\tlearn: 0.3559821\ttotal: 44.8s\tremaining: 44.7s\n",
      "501:\tlearn: 0.3558863\ttotal: 44.9s\tremaining: 44.6s\n",
      "502:\tlearn: 0.3558132\ttotal: 45s\tremaining: 44.5s\n",
      "503:\tlearn: 0.3557078\ttotal: 45.1s\tremaining: 44.4s\n",
      "504:\tlearn: 0.3556437\ttotal: 45.2s\tremaining: 44.3s\n",
      "505:\tlearn: 0.3555431\ttotal: 45.3s\tremaining: 44.3s\n",
      "506:\tlearn: 0.3554755\ttotal: 45.4s\tremaining: 44.2s\n",
      "507:\tlearn: 0.3553112\ttotal: 45.5s\tremaining: 44.1s\n",
      "508:\tlearn: 0.3552392\ttotal: 45.6s\tremaining: 44s\n",
      "509:\tlearn: 0.3551600\ttotal: 45.7s\tremaining: 43.9s\n",
      "510:\tlearn: 0.3550824\ttotal: 45.8s\tremaining: 43.8s\n",
      "511:\tlearn: 0.3549907\ttotal: 45.9s\tremaining: 43.7s\n",
      "512:\tlearn: 0.3549091\ttotal: 46s\tremaining: 43.7s\n",
      "513:\tlearn: 0.3548287\ttotal: 46.1s\tremaining: 43.6s\n",
      "514:\tlearn: 0.3548170\ttotal: 46.2s\tremaining: 43.5s\n",
      "515:\tlearn: 0.3545337\ttotal: 46.3s\tremaining: 43.4s\n",
      "516:\tlearn: 0.3544379\ttotal: 46.4s\tremaining: 43.3s\n",
      "517:\tlearn: 0.3543874\ttotal: 46.5s\tremaining: 43.2s\n",
      "518:\tlearn: 0.3543065\ttotal: 46.6s\tremaining: 43.1s\n",
      "519:\tlearn: 0.3542285\ttotal: 46.7s\tremaining: 43.1s\n",
      "520:\tlearn: 0.3542060\ttotal: 46.7s\tremaining: 43s\n",
      "521:\tlearn: 0.3541361\ttotal: 46.8s\tremaining: 42.9s\n",
      "522:\tlearn: 0.3540720\ttotal: 46.9s\tremaining: 42.8s\n",
      "523:\tlearn: 0.3540307\ttotal: 47s\tremaining: 42.7s\n",
      "524:\tlearn: 0.3539729\ttotal: 47.1s\tremaining: 42.6s\n",
      "525:\tlearn: 0.3539001\ttotal: 47.2s\tremaining: 42.6s\n",
      "526:\tlearn: 0.3538183\ttotal: 47.4s\tremaining: 42.5s\n",
      "527:\tlearn: 0.3537651\ttotal: 47.5s\tremaining: 42.5s\n",
      "528:\tlearn: 0.3536876\ttotal: 47.6s\tremaining: 42.4s\n",
      "529:\tlearn: 0.3536047\ttotal: 47.7s\tremaining: 42.3s\n",
      "530:\tlearn: 0.3535482\ttotal: 47.8s\tremaining: 42.2s\n",
      "531:\tlearn: 0.3534708\ttotal: 47.9s\tremaining: 42.1s\n",
      "532:\tlearn: 0.3533819\ttotal: 48s\tremaining: 42.1s\n",
      "533:\tlearn: 0.3532941\ttotal: 48.1s\tremaining: 42s\n",
      "534:\tlearn: 0.3532401\ttotal: 48.2s\tremaining: 41.9s\n",
      "535:\tlearn: 0.3531605\ttotal: 48.3s\tremaining: 41.8s\n",
      "536:\tlearn: 0.3530907\ttotal: 48.4s\tremaining: 41.7s\n",
      "537:\tlearn: 0.3529910\ttotal: 48.5s\tremaining: 41.6s\n",
      "538:\tlearn: 0.3529279\ttotal: 48.6s\tremaining: 41.5s\n",
      "539:\tlearn: 0.3528489\ttotal: 48.7s\tremaining: 41.5s\n",
      "540:\tlearn: 0.3527480\ttotal: 48.8s\tremaining: 41.4s\n",
      "541:\tlearn: 0.3526685\ttotal: 48.9s\tremaining: 41.3s\n",
      "542:\tlearn: 0.3525994\ttotal: 49s\tremaining: 41.2s\n",
      "543:\tlearn: 0.3525197\ttotal: 49.1s\tremaining: 41.1s\n",
      "544:\tlearn: 0.3524565\ttotal: 49.2s\tremaining: 41s\n",
      "545:\tlearn: 0.3523851\ttotal: 49.3s\tremaining: 41s\n",
      "546:\tlearn: 0.3523006\ttotal: 49.4s\tremaining: 40.9s\n",
      "547:\tlearn: 0.3522401\ttotal: 49.5s\tremaining: 40.8s\n",
      "548:\tlearn: 0.3522087\ttotal: 49.6s\tremaining: 40.7s\n",
      "549:\tlearn: 0.3521414\ttotal: 49.7s\tremaining: 40.6s\n",
      "550:\tlearn: 0.3520603\ttotal: 49.8s\tremaining: 40.5s\n",
      "551:\tlearn: 0.3519726\ttotal: 49.9s\tremaining: 40.5s\n",
      "552:\tlearn: 0.3518736\ttotal: 50s\tremaining: 40.4s\n",
      "553:\tlearn: 0.3517316\ttotal: 50.1s\tremaining: 40.3s\n",
      "554:\tlearn: 0.3516511\ttotal: 50.1s\tremaining: 40.2s\n",
      "555:\tlearn: 0.3515870\ttotal: 50.2s\tremaining: 40.1s\n",
      "556:\tlearn: 0.3515268\ttotal: 50.4s\tremaining: 40s\n",
      "557:\tlearn: 0.3514472\ttotal: 50.5s\tremaining: 40s\n",
      "558:\tlearn: 0.3513843\ttotal: 50.6s\tremaining: 39.9s\n",
      "559:\tlearn: 0.3513137\ttotal: 50.7s\tremaining: 39.8s\n",
      "560:\tlearn: 0.3512409\ttotal: 50.9s\tremaining: 39.8s\n",
      "561:\tlearn: 0.3511593\ttotal: 51s\tremaining: 39.8s\n",
      "562:\tlearn: 0.3510818\ttotal: 51.2s\tremaining: 39.7s\n",
      "563:\tlearn: 0.3510273\ttotal: 51.2s\tremaining: 39.6s\n",
      "564:\tlearn: 0.3509510\ttotal: 51.4s\tremaining: 39.5s\n",
      "565:\tlearn: 0.3508609\ttotal: 51.5s\tremaining: 39.5s\n",
      "566:\tlearn: 0.3507782\ttotal: 51.6s\tremaining: 39.4s\n",
      "567:\tlearn: 0.3506832\ttotal: 51.7s\tremaining: 39.3s\n",
      "568:\tlearn: 0.3506254\ttotal: 51.8s\tremaining: 39.2s\n",
      "569:\tlearn: 0.3505505\ttotal: 51.9s\tremaining: 39.1s\n",
      "570:\tlearn: 0.3504710\ttotal: 51.9s\tremaining: 39s\n",
      "571:\tlearn: 0.3504604\ttotal: 52s\tremaining: 38.9s\n",
      "572:\tlearn: 0.3503776\ttotal: 52.1s\tremaining: 38.8s\n",
      "573:\tlearn: 0.3503257\ttotal: 52.2s\tremaining: 38.7s\n",
      "574:\tlearn: 0.3502780\ttotal: 52.3s\tremaining: 38.6s\n",
      "575:\tlearn: 0.3502023\ttotal: 52.3s\tremaining: 38.5s\n",
      "576:\tlearn: 0.3501174\ttotal: 52.4s\tremaining: 38.4s\n",
      "577:\tlearn: 0.3500535\ttotal: 52.5s\tremaining: 38.3s\n",
      "578:\tlearn: 0.3499950\ttotal: 52.6s\tremaining: 38.3s\n",
      "579:\tlearn: 0.3499216\ttotal: 52.7s\tremaining: 38.2s\n",
      "580:\tlearn: 0.3498369\ttotal: 52.8s\tremaining: 38.1s\n",
      "581:\tlearn: 0.3497665\ttotal: 52.9s\tremaining: 38s\n",
      "582:\tlearn: 0.3496956\ttotal: 53s\tremaining: 37.9s\n",
      "583:\tlearn: 0.3496300\ttotal: 53.1s\tremaining: 37.8s\n",
      "584:\tlearn: 0.3495628\ttotal: 53.2s\tremaining: 37.7s\n",
      "585:\tlearn: 0.3494958\ttotal: 53.3s\tremaining: 37.6s\n",
      "586:\tlearn: 0.3494186\ttotal: 53.4s\tremaining: 37.5s\n",
      "587:\tlearn: 0.3493569\ttotal: 53.4s\tremaining: 37.4s\n",
      "588:\tlearn: 0.3492636\ttotal: 53.5s\tremaining: 37.4s\n",
      "589:\tlearn: 0.3491648\ttotal: 53.6s\tremaining: 37.3s\n",
      "590:\tlearn: 0.3491163\ttotal: 53.7s\tremaining: 37.2s\n",
      "591:\tlearn: 0.3490439\ttotal: 53.8s\tremaining: 37.1s\n",
      "592:\tlearn: 0.3489983\ttotal: 53.9s\tremaining: 37s\n",
      "593:\tlearn: 0.3489237\ttotal: 54s\tremaining: 36.9s\n",
      "594:\tlearn: 0.3488043\ttotal: 54.1s\tremaining: 36.8s\n",
      "595:\tlearn: 0.3487174\ttotal: 54.1s\tremaining: 36.7s\n",
      "596:\tlearn: 0.3485771\ttotal: 54.2s\tremaining: 36.6s\n",
      "597:\tlearn: 0.3485189\ttotal: 54.3s\tremaining: 36.5s\n",
      "598:\tlearn: 0.3484790\ttotal: 54.4s\tremaining: 36.4s\n",
      "599:\tlearn: 0.3484193\ttotal: 54.5s\tremaining: 36.3s\n",
      "600:\tlearn: 0.3483463\ttotal: 54.6s\tremaining: 36.2s\n",
      "601:\tlearn: 0.3482863\ttotal: 54.6s\tremaining: 36.1s\n",
      "602:\tlearn: 0.3481759\ttotal: 54.7s\tremaining: 36s\n",
      "603:\tlearn: 0.3480976\ttotal: 54.8s\tremaining: 35.9s\n",
      "604:\tlearn: 0.3480038\ttotal: 54.9s\tremaining: 35.8s\n",
      "605:\tlearn: 0.3479292\ttotal: 55s\tremaining: 35.7s\n",
      "606:\tlearn: 0.3477947\ttotal: 55.1s\tremaining: 35.7s\n",
      "607:\tlearn: 0.3477173\ttotal: 55.2s\tremaining: 35.6s\n",
      "608:\tlearn: 0.3476578\ttotal: 55.2s\tremaining: 35.5s\n",
      "609:\tlearn: 0.3475819\ttotal: 55.3s\tremaining: 35.4s\n",
      "610:\tlearn: 0.3475108\ttotal: 55.4s\tremaining: 35.3s\n",
      "611:\tlearn: 0.3474335\ttotal: 55.5s\tremaining: 35.2s\n",
      "612:\tlearn: 0.3473196\ttotal: 55.6s\tremaining: 35.1s\n",
      "613:\tlearn: 0.3472383\ttotal: 55.7s\tremaining: 35s\n",
      "614:\tlearn: 0.3471776\ttotal: 55.8s\tremaining: 34.9s\n",
      "615:\tlearn: 0.3471188\ttotal: 55.9s\tremaining: 34.8s\n",
      "616:\tlearn: 0.3470555\ttotal: 56s\tremaining: 34.7s\n",
      "617:\tlearn: 0.3469822\ttotal: 56.1s\tremaining: 34.7s\n",
      "618:\tlearn: 0.3469588\ttotal: 56.1s\tremaining: 34.6s\n",
      "619:\tlearn: 0.3468803\ttotal: 56.2s\tremaining: 34.5s\n",
      "620:\tlearn: 0.3467911\ttotal: 56.3s\tremaining: 34.4s\n",
      "621:\tlearn: 0.3467058\ttotal: 56.4s\tremaining: 34.3s\n",
      "622:\tlearn: 0.3466336\ttotal: 56.5s\tremaining: 34.2s\n",
      "623:\tlearn: 0.3466169\ttotal: 56.6s\tremaining: 34.1s\n",
      "624:\tlearn: 0.3465662\ttotal: 56.7s\tremaining: 34s\n",
      "625:\tlearn: 0.3465022\ttotal: 56.8s\tremaining: 33.9s\n",
      "626:\tlearn: 0.3464156\ttotal: 56.9s\tremaining: 33.8s\n",
      "627:\tlearn: 0.3463276\ttotal: 57s\tremaining: 33.7s\n",
      "628:\tlearn: 0.3462373\ttotal: 57.1s\tremaining: 33.7s\n",
      "629:\tlearn: 0.3461585\ttotal: 57.2s\tremaining: 33.6s\n",
      "630:\tlearn: 0.3459983\ttotal: 57.3s\tremaining: 33.5s\n",
      "631:\tlearn: 0.3459154\ttotal: 57.3s\tremaining: 33.4s\n",
      "632:\tlearn: 0.3457709\ttotal: 57.4s\tremaining: 33.3s\n",
      "633:\tlearn: 0.3456846\ttotal: 57.5s\tremaining: 33.2s\n",
      "634:\tlearn: 0.3456011\ttotal: 57.6s\tremaining: 33.1s\n",
      "635:\tlearn: 0.3455390\ttotal: 57.7s\tremaining: 33s\n",
      "636:\tlearn: 0.3454051\ttotal: 57.8s\tremaining: 32.9s\n",
      "637:\tlearn: 0.3453389\ttotal: 58s\tremaining: 32.9s\n",
      "638:\tlearn: 0.3452699\ttotal: 58.1s\tremaining: 32.8s\n",
      "639:\tlearn: 0.3452040\ttotal: 58.1s\tremaining: 32.7s\n",
      "640:\tlearn: 0.3451284\ttotal: 58.2s\tremaining: 32.6s\n",
      "641:\tlearn: 0.3450577\ttotal: 58.3s\tremaining: 32.5s\n",
      "642:\tlearn: 0.3449817\ttotal: 58.4s\tremaining: 32.4s\n",
      "643:\tlearn: 0.3446993\ttotal: 58.5s\tremaining: 32.3s\n",
      "644:\tlearn: 0.3446339\ttotal: 58.6s\tremaining: 32.2s\n",
      "645:\tlearn: 0.3445336\ttotal: 58.7s\tremaining: 32.2s\n",
      "646:\tlearn: 0.3443673\ttotal: 58.8s\tremaining: 32.1s\n",
      "647:\tlearn: 0.3442761\ttotal: 58.8s\tremaining: 32s\n",
      "648:\tlearn: 0.3442088\ttotal: 58.9s\tremaining: 31.9s\n",
      "649:\tlearn: 0.3441317\ttotal: 59s\tremaining: 31.8s\n",
      "650:\tlearn: 0.3440676\ttotal: 59.1s\tremaining: 31.7s\n",
      "651:\tlearn: 0.3440216\ttotal: 59.2s\tremaining: 31.6s\n",
      "652:\tlearn: 0.3439585\ttotal: 59.3s\tremaining: 31.5s\n",
      "653:\tlearn: 0.3438910\ttotal: 59.4s\tremaining: 31.4s\n",
      "654:\tlearn: 0.3438493\ttotal: 59.4s\tremaining: 31.3s\n",
      "655:\tlearn: 0.3437726\ttotal: 59.5s\tremaining: 31.2s\n",
      "656:\tlearn: 0.3437070\ttotal: 59.6s\tremaining: 31.1s\n",
      "657:\tlearn: 0.3436332\ttotal: 59.7s\tremaining: 31s\n",
      "658:\tlearn: 0.3435610\ttotal: 59.8s\tremaining: 30.9s\n",
      "659:\tlearn: 0.3435000\ttotal: 59.9s\tremaining: 30.9s\n",
      "660:\tlearn: 0.3434896\ttotal: 60s\tremaining: 30.7s\n",
      "661:\tlearn: 0.3434207\ttotal: 1m\tremaining: 30.7s\n",
      "662:\tlearn: 0.3433455\ttotal: 1m\tremaining: 30.6s\n",
      "663:\tlearn: 0.3432678\ttotal: 1m\tremaining: 30.5s\n",
      "664:\tlearn: 0.3432247\ttotal: 1m\tremaining: 30.4s\n",
      "665:\tlearn: 0.3431582\ttotal: 1m\tremaining: 30.3s\n",
      "666:\tlearn: 0.3430980\ttotal: 1m\tremaining: 30.2s\n",
      "667:\tlearn: 0.3430424\ttotal: 1m\tremaining: 30.1s\n",
      "668:\tlearn: 0.3429877\ttotal: 1m\tremaining: 30s\n",
      "669:\tlearn: 0.3429336\ttotal: 1m\tremaining: 29.9s\n",
      "670:\tlearn: 0.3427392\ttotal: 1m\tremaining: 29.8s\n",
      "671:\tlearn: 0.3425903\ttotal: 1m\tremaining: 29.7s\n",
      "672:\tlearn: 0.3425220\ttotal: 1m\tremaining: 29.6s\n",
      "673:\tlearn: 0.3425018\ttotal: 1m 1s\tremaining: 29.5s\n",
      "674:\tlearn: 0.3424361\ttotal: 1m 1s\tremaining: 29.4s\n",
      "675:\tlearn: 0.3423845\ttotal: 1m 1s\tremaining: 29.3s\n",
      "676:\tlearn: 0.3423188\ttotal: 1m 1s\tremaining: 29.2s\n",
      "677:\tlearn: 0.3422702\ttotal: 1m 1s\tremaining: 29.2s\n",
      "678:\tlearn: 0.3421960\ttotal: 1m 1s\tremaining: 29.1s\n",
      "679:\tlearn: 0.3421324\ttotal: 1m 1s\tremaining: 29s\n",
      "680:\tlearn: 0.3420689\ttotal: 1m 1s\tremaining: 28.9s\n",
      "681:\tlearn: 0.3420056\ttotal: 1m 1s\tremaining: 28.8s\n",
      "682:\tlearn: 0.3419456\ttotal: 1m 1s\tremaining: 28.7s\n",
      "683:\tlearn: 0.3417563\ttotal: 1m 1s\tremaining: 28.6s\n",
      "684:\tlearn: 0.3416968\ttotal: 1m 2s\tremaining: 28.5s\n",
      "685:\tlearn: 0.3416753\ttotal: 1m 2s\tremaining: 28.4s\n",
      "686:\tlearn: 0.3416092\ttotal: 1m 2s\tremaining: 28.4s\n",
      "687:\tlearn: 0.3415312\ttotal: 1m 2s\tremaining: 28.3s\n",
      "688:\tlearn: 0.3414647\ttotal: 1m 2s\tremaining: 28.2s\n",
      "689:\tlearn: 0.3414018\ttotal: 1m 2s\tremaining: 28.1s\n",
      "690:\tlearn: 0.3413484\ttotal: 1m 2s\tremaining: 28s\n",
      "691:\tlearn: 0.3412740\ttotal: 1m 2s\tremaining: 28s\n",
      "692:\tlearn: 0.3412108\ttotal: 1m 2s\tremaining: 27.9s\n",
      "693:\tlearn: 0.3411484\ttotal: 1m 2s\tremaining: 27.8s\n",
      "694:\tlearn: 0.3410720\ttotal: 1m 3s\tremaining: 27.7s\n",
      "695:\tlearn: 0.3410036\ttotal: 1m 3s\tremaining: 27.6s\n",
      "696:\tlearn: 0.3409384\ttotal: 1m 3s\tremaining: 27.5s\n",
      "697:\tlearn: 0.3408654\ttotal: 1m 3s\tremaining: 27.5s\n",
      "698:\tlearn: 0.3407983\ttotal: 1m 3s\tremaining: 27.4s\n",
      "699:\tlearn: 0.3407112\ttotal: 1m 3s\tremaining: 27.3s\n",
      "700:\tlearn: 0.3406463\ttotal: 1m 3s\tremaining: 27.2s\n",
      "701:\tlearn: 0.3405966\ttotal: 1m 3s\tremaining: 27.1s\n",
      "702:\tlearn: 0.3405346\ttotal: 1m 3s\tremaining: 27s\n",
      "703:\tlearn: 0.3404747\ttotal: 1m 4s\tremaining: 26.9s\n",
      "704:\tlearn: 0.3403756\ttotal: 1m 4s\tremaining: 26.9s\n",
      "705:\tlearn: 0.3403675\ttotal: 1m 4s\tremaining: 26.8s\n",
      "706:\tlearn: 0.3403263\ttotal: 1m 4s\tremaining: 26.7s\n",
      "707:\tlearn: 0.3402740\ttotal: 1m 4s\tremaining: 26.6s\n",
      "708:\tlearn: 0.3400054\ttotal: 1m 4s\tremaining: 26.5s\n",
      "709:\tlearn: 0.3398937\ttotal: 1m 4s\tremaining: 26.4s\n",
      "710:\tlearn: 0.3398409\ttotal: 1m 4s\tremaining: 26.3s\n",
      "711:\tlearn: 0.3397446\ttotal: 1m 4s\tremaining: 26.2s\n",
      "712:\tlearn: 0.3396816\ttotal: 1m 4s\tremaining: 26.1s\n",
      "713:\tlearn: 0.3395923\ttotal: 1m 5s\tremaining: 26s\n",
      "714:\tlearn: 0.3395124\ttotal: 1m 5s\tremaining: 26s\n",
      "715:\tlearn: 0.3395016\ttotal: 1m 5s\tremaining: 25.9s\n",
      "716:\tlearn: 0.3394311\ttotal: 1m 5s\tremaining: 25.8s\n",
      "717:\tlearn: 0.3393499\ttotal: 1m 5s\tremaining: 25.7s\n",
      "718:\tlearn: 0.3392851\ttotal: 1m 5s\tremaining: 25.6s\n",
      "719:\tlearn: 0.3392372\ttotal: 1m 5s\tremaining: 25.5s\n",
      "720:\tlearn: 0.3391777\ttotal: 1m 5s\tremaining: 25.4s\n",
      "721:\tlearn: 0.3391249\ttotal: 1m 5s\tremaining: 25.4s\n",
      "722:\tlearn: 0.3390465\ttotal: 1m 5s\tremaining: 25.3s\n",
      "723:\tlearn: 0.3390050\ttotal: 1m 6s\tremaining: 25.2s\n",
      "724:\tlearn: 0.3388225\ttotal: 1m 6s\tremaining: 25.1s\n",
      "725:\tlearn: 0.3387415\ttotal: 1m 6s\tremaining: 25s\n",
      "726:\tlearn: 0.3386654\ttotal: 1m 6s\tremaining: 24.9s\n",
      "727:\tlearn: 0.3385089\ttotal: 1m 6s\tremaining: 24.8s\n",
      "728:\tlearn: 0.3384187\ttotal: 1m 6s\tremaining: 24.7s\n",
      "729:\tlearn: 0.3382959\ttotal: 1m 6s\tremaining: 24.6s\n",
      "730:\tlearn: 0.3382159\ttotal: 1m 6s\tremaining: 24.6s\n",
      "731:\tlearn: 0.3381486\ttotal: 1m 6s\tremaining: 24.5s\n",
      "732:\tlearn: 0.3380817\ttotal: 1m 6s\tremaining: 24.4s\n",
      "733:\tlearn: 0.3380585\ttotal: 1m 7s\tremaining: 24.3s\n",
      "734:\tlearn: 0.3380095\ttotal: 1m 7s\tremaining: 24.2s\n",
      "735:\tlearn: 0.3379403\ttotal: 1m 7s\tremaining: 24.1s\n",
      "736:\tlearn: 0.3378723\ttotal: 1m 7s\tremaining: 24s\n",
      "737:\tlearn: 0.3378081\ttotal: 1m 7s\tremaining: 23.9s\n",
      "738:\tlearn: 0.3377537\ttotal: 1m 7s\tremaining: 23.8s\n",
      "739:\tlearn: 0.3376909\ttotal: 1m 7s\tremaining: 23.8s\n",
      "740:\tlearn: 0.3376452\ttotal: 1m 7s\tremaining: 23.7s\n",
      "741:\tlearn: 0.3376214\ttotal: 1m 7s\tremaining: 23.6s\n",
      "742:\tlearn: 0.3375586\ttotal: 1m 7s\tremaining: 23.5s\n",
      "743:\tlearn: 0.3374928\ttotal: 1m 7s\tremaining: 23.4s\n",
      "744:\tlearn: 0.3374322\ttotal: 1m 8s\tremaining: 23.3s\n",
      "745:\tlearn: 0.3374231\ttotal: 1m 8s\tremaining: 23.2s\n",
      "746:\tlearn: 0.3373596\ttotal: 1m 8s\tremaining: 23.1s\n",
      "747:\tlearn: 0.3373120\ttotal: 1m 8s\tremaining: 23s\n",
      "748:\tlearn: 0.3372518\ttotal: 1m 8s\tremaining: 22.9s\n",
      "749:\tlearn: 0.3371869\ttotal: 1m 8s\tremaining: 22.8s\n",
      "750:\tlearn: 0.3371057\ttotal: 1m 8s\tremaining: 22.8s\n",
      "751:\tlearn: 0.3370110\ttotal: 1m 8s\tremaining: 22.7s\n",
      "752:\tlearn: 0.3369522\ttotal: 1m 8s\tremaining: 22.6s\n",
      "753:\tlearn: 0.3367762\ttotal: 1m 8s\tremaining: 22.5s\n",
      "754:\tlearn: 0.3367187\ttotal: 1m 8s\tremaining: 22.4s\n",
      "755:\tlearn: 0.3366596\ttotal: 1m 9s\tremaining: 22.3s\n",
      "756:\tlearn: 0.3365153\ttotal: 1m 9s\tremaining: 22.2s\n",
      "757:\tlearn: 0.3364533\ttotal: 1m 9s\tremaining: 22.1s\n",
      "758:\tlearn: 0.3364170\ttotal: 1m 9s\tremaining: 22s\n",
      "759:\tlearn: 0.3363509\ttotal: 1m 9s\tremaining: 21.9s\n",
      "760:\tlearn: 0.3363143\ttotal: 1m 9s\tremaining: 21.8s\n",
      "761:\tlearn: 0.3362474\ttotal: 1m 9s\tremaining: 21.8s\n",
      "762:\tlearn: 0.3361839\ttotal: 1m 9s\tremaining: 21.7s\n",
      "763:\tlearn: 0.3361129\ttotal: 1m 9s\tremaining: 21.6s\n",
      "764:\tlearn: 0.3360465\ttotal: 1m 9s\tremaining: 21.5s\n",
      "765:\tlearn: 0.3359698\ttotal: 1m 10s\tremaining: 21.4s\n",
      "766:\tlearn: 0.3359068\ttotal: 1m 10s\tremaining: 21.3s\n",
      "767:\tlearn: 0.3356841\ttotal: 1m 10s\tremaining: 21.2s\n",
      "768:\tlearn: 0.3356028\ttotal: 1m 10s\tremaining: 21.1s\n",
      "769:\tlearn: 0.3355332\ttotal: 1m 10s\tremaining: 21s\n",
      "770:\tlearn: 0.3354939\ttotal: 1m 10s\tremaining: 20.9s\n",
      "771:\tlearn: 0.3354286\ttotal: 1m 10s\tremaining: 20.9s\n",
      "772:\tlearn: 0.3353783\ttotal: 1m 10s\tremaining: 20.8s\n",
      "773:\tlearn: 0.3353124\ttotal: 1m 10s\tremaining: 20.7s\n",
      "774:\tlearn: 0.3352636\ttotal: 1m 10s\tremaining: 20.6s\n",
      "775:\tlearn: 0.3352575\ttotal: 1m 10s\tremaining: 20.5s\n",
      "776:\tlearn: 0.3351844\ttotal: 1m 11s\tremaining: 20.4s\n",
      "777:\tlearn: 0.3351245\ttotal: 1m 11s\tremaining: 20.3s\n",
      "778:\tlearn: 0.3350762\ttotal: 1m 11s\tremaining: 20.2s\n",
      "779:\tlearn: 0.3350284\ttotal: 1m 11s\tremaining: 20.2s\n",
      "780:\tlearn: 0.3349805\ttotal: 1m 11s\tremaining: 20.1s\n",
      "781:\tlearn: 0.3349345\ttotal: 1m 11s\tremaining: 20s\n",
      "782:\tlearn: 0.3347840\ttotal: 1m 11s\tremaining: 19.9s\n",
      "783:\tlearn: 0.3347786\ttotal: 1m 11s\tremaining: 19.8s\n",
      "784:\tlearn: 0.3347142\ttotal: 1m 11s\tremaining: 19.7s\n",
      "785:\tlearn: 0.3346956\ttotal: 1m 11s\tremaining: 19.6s\n",
      "786:\tlearn: 0.3346322\ttotal: 1m 12s\tremaining: 19.5s\n",
      "787:\tlearn: 0.3345852\ttotal: 1m 12s\tremaining: 19.4s\n",
      "788:\tlearn: 0.3345045\ttotal: 1m 12s\tremaining: 19.3s\n",
      "789:\tlearn: 0.3344437\ttotal: 1m 12s\tremaining: 19.2s\n",
      "790:\tlearn: 0.3344364\ttotal: 1m 12s\tremaining: 19.1s\n",
      "791:\tlearn: 0.3343756\ttotal: 1m 12s\tremaining: 19s\n",
      "792:\tlearn: 0.3343315\ttotal: 1m 12s\tremaining: 18.9s\n",
      "793:\tlearn: 0.3342831\ttotal: 1m 12s\tremaining: 18.8s\n",
      "794:\tlearn: 0.3341740\ttotal: 1m 12s\tremaining: 18.8s\n",
      "795:\tlearn: 0.3341087\ttotal: 1m 12s\tremaining: 18.7s\n",
      "796:\tlearn: 0.3340504\ttotal: 1m 12s\tremaining: 18.6s\n",
      "797:\tlearn: 0.3339926\ttotal: 1m 13s\tremaining: 18.5s\n",
      "798:\tlearn: 0.3339019\ttotal: 1m 13s\tremaining: 18.4s\n",
      "799:\tlearn: 0.3338297\ttotal: 1m 13s\tremaining: 18.3s\n",
      "800:\tlearn: 0.3337721\ttotal: 1m 13s\tremaining: 18.2s\n",
      "801:\tlearn: 0.3337255\ttotal: 1m 13s\tremaining: 18.1s\n",
      "802:\tlearn: 0.3336394\ttotal: 1m 13s\tremaining: 18s\n",
      "803:\tlearn: 0.3335786\ttotal: 1m 13s\tremaining: 17.9s\n",
      "804:\tlearn: 0.3335685\ttotal: 1m 13s\tremaining: 17.8s\n",
      "805:\tlearn: 0.3335055\ttotal: 1m 13s\tremaining: 17.7s\n",
      "806:\tlearn: 0.3334285\ttotal: 1m 13s\tremaining: 17.6s\n",
      "807:\tlearn: 0.3333782\ttotal: 1m 13s\tremaining: 17.6s\n",
      "808:\tlearn: 0.3332960\ttotal: 1m 13s\tremaining: 17.5s\n",
      "809:\tlearn: 0.3332475\ttotal: 1m 14s\tremaining: 17.4s\n",
      "810:\tlearn: 0.3331696\ttotal: 1m 14s\tremaining: 17.3s\n",
      "811:\tlearn: 0.3330926\ttotal: 1m 14s\tremaining: 17.2s\n",
      "812:\tlearn: 0.3330059\ttotal: 1m 14s\tremaining: 17.1s\n",
      "813:\tlearn: 0.3329460\ttotal: 1m 14s\tremaining: 17s\n",
      "814:\tlearn: 0.3328904\ttotal: 1m 14s\tremaining: 16.9s\n",
      "815:\tlearn: 0.3328331\ttotal: 1m 14s\tremaining: 16.8s\n",
      "816:\tlearn: 0.3327631\ttotal: 1m 14s\tremaining: 16.7s\n",
      "817:\tlearn: 0.3326778\ttotal: 1m 14s\tremaining: 16.6s\n",
      "818:\tlearn: 0.3326215\ttotal: 1m 14s\tremaining: 16.5s\n",
      "819:\tlearn: 0.3325755\ttotal: 1m 14s\tremaining: 16.5s\n",
      "820:\tlearn: 0.3324991\ttotal: 1m 15s\tremaining: 16.4s\n",
      "821:\tlearn: 0.3324363\ttotal: 1m 15s\tremaining: 16.3s\n",
      "822:\tlearn: 0.3323722\ttotal: 1m 15s\tremaining: 16.2s\n",
      "823:\tlearn: 0.3323347\ttotal: 1m 15s\tremaining: 16.1s\n",
      "824:\tlearn: 0.3323292\ttotal: 1m 15s\tremaining: 16s\n",
      "825:\tlearn: 0.3322869\ttotal: 1m 15s\tremaining: 15.9s\n",
      "826:\tlearn: 0.3322387\ttotal: 1m 15s\tremaining: 15.8s\n",
      "827:\tlearn: 0.3321838\ttotal: 1m 15s\tremaining: 15.7s\n",
      "828:\tlearn: 0.3321108\ttotal: 1m 15s\tremaining: 15.6s\n",
      "829:\tlearn: 0.3320642\ttotal: 1m 15s\tremaining: 15.5s\n",
      "830:\tlearn: 0.3320018\ttotal: 1m 15s\tremaining: 15.4s\n",
      "831:\tlearn: 0.3319231\ttotal: 1m 15s\tremaining: 15.3s\n",
      "832:\tlearn: 0.3318459\ttotal: 1m 16s\tremaining: 15.3s\n",
      "833:\tlearn: 0.3317927\ttotal: 1m 16s\tremaining: 15.2s\n",
      "834:\tlearn: 0.3317265\ttotal: 1m 16s\tremaining: 15.1s\n",
      "835:\tlearn: 0.3316892\ttotal: 1m 16s\tremaining: 15s\n",
      "836:\tlearn: 0.3316400\ttotal: 1m 16s\tremaining: 14.9s\n",
      "837:\tlearn: 0.3315778\ttotal: 1m 16s\tremaining: 14.8s\n",
      "838:\tlearn: 0.3315066\ttotal: 1m 16s\tremaining: 14.7s\n",
      "839:\tlearn: 0.3313103\ttotal: 1m 16s\tremaining: 14.6s\n",
      "840:\tlearn: 0.3312571\ttotal: 1m 16s\tremaining: 14.5s\n",
      "841:\tlearn: 0.3311916\ttotal: 1m 16s\tremaining: 14.4s\n",
      "842:\tlearn: 0.3311405\ttotal: 1m 17s\tremaining: 14.3s\n",
      "843:\tlearn: 0.3310852\ttotal: 1m 17s\tremaining: 14.3s\n",
      "844:\tlearn: 0.3309972\ttotal: 1m 17s\tremaining: 14.2s\n",
      "845:\tlearn: 0.3309573\ttotal: 1m 17s\tremaining: 14.1s\n",
      "846:\tlearn: 0.3308975\ttotal: 1m 17s\tremaining: 14s\n",
      "847:\tlearn: 0.3308106\ttotal: 1m 17s\tremaining: 13.9s\n",
      "848:\tlearn: 0.3307635\ttotal: 1m 17s\tremaining: 13.8s\n",
      "849:\tlearn: 0.3307180\ttotal: 1m 17s\tremaining: 13.7s\n",
      "850:\tlearn: 0.3306829\ttotal: 1m 17s\tremaining: 13.6s\n",
      "851:\tlearn: 0.3306109\ttotal: 1m 17s\tremaining: 13.5s\n",
      "852:\tlearn: 0.3305432\ttotal: 1m 17s\tremaining: 13.4s\n",
      "853:\tlearn: 0.3304967\ttotal: 1m 18s\tremaining: 13.3s\n",
      "854:\tlearn: 0.3304420\ttotal: 1m 18s\tremaining: 13.3s\n",
      "855:\tlearn: 0.3303519\ttotal: 1m 18s\tremaining: 13.2s\n",
      "856:\tlearn: 0.3303031\ttotal: 1m 18s\tremaining: 13.1s\n",
      "857:\tlearn: 0.3302689\ttotal: 1m 18s\tremaining: 13s\n",
      "858:\tlearn: 0.3302089\ttotal: 1m 18s\tremaining: 12.9s\n",
      "859:\tlearn: 0.3301275\ttotal: 1m 18s\tremaining: 12.8s\n",
      "860:\tlearn: 0.3300997\ttotal: 1m 18s\tremaining: 12.7s\n",
      "861:\tlearn: 0.3300381\ttotal: 1m 18s\tremaining: 12.6s\n",
      "862:\tlearn: 0.3299618\ttotal: 1m 18s\tremaining: 12.5s\n",
      "863:\tlearn: 0.3299382\ttotal: 1m 18s\tremaining: 12.4s\n",
      "864:\tlearn: 0.3298776\ttotal: 1m 19s\tremaining: 12.3s\n",
      "865:\tlearn: 0.3298726\ttotal: 1m 19s\tremaining: 12.2s\n",
      "866:\tlearn: 0.3298195\ttotal: 1m 19s\tremaining: 12.2s\n",
      "867:\tlearn: 0.3297430\ttotal: 1m 19s\tremaining: 12.1s\n",
      "868:\tlearn: 0.3296781\ttotal: 1m 19s\tremaining: 12s\n",
      "869:\tlearn: 0.3296099\ttotal: 1m 19s\tremaining: 11.9s\n",
      "870:\tlearn: 0.3295450\ttotal: 1m 19s\tremaining: 11.8s\n",
      "871:\tlearn: 0.3294151\ttotal: 1m 19s\tremaining: 11.7s\n",
      "872:\tlearn: 0.3293518\ttotal: 1m 19s\tremaining: 11.6s\n",
      "873:\tlearn: 0.3293081\ttotal: 1m 19s\tremaining: 11.5s\n",
      "874:\tlearn: 0.3292368\ttotal: 1m 19s\tremaining: 11.4s\n",
      "875:\tlearn: 0.3291468\ttotal: 1m 20s\tremaining: 11.3s\n",
      "876:\tlearn: 0.3290726\ttotal: 1m 20s\tremaining: 11.2s\n",
      "877:\tlearn: 0.3290193\ttotal: 1m 20s\tremaining: 11.2s\n",
      "878:\tlearn: 0.3289550\ttotal: 1m 20s\tremaining: 11.1s\n",
      "879:\tlearn: 0.3289153\ttotal: 1m 20s\tremaining: 11s\n",
      "880:\tlearn: 0.3288388\ttotal: 1m 20s\tremaining: 10.9s\n",
      "881:\tlearn: 0.3287816\ttotal: 1m 20s\tremaining: 10.8s\n",
      "882:\tlearn: 0.3287644\ttotal: 1m 20s\tremaining: 10.7s\n",
      "883:\tlearn: 0.3287146\ttotal: 1m 20s\tremaining: 10.6s\n",
      "884:\tlearn: 0.3286688\ttotal: 1m 20s\tremaining: 10.5s\n",
      "885:\tlearn: 0.3285705\ttotal: 1m 20s\tremaining: 10.4s\n",
      "886:\tlearn: 0.3284976\ttotal: 1m 21s\tremaining: 10.3s\n",
      "887:\tlearn: 0.3284320\ttotal: 1m 21s\tremaining: 10.2s\n",
      "888:\tlearn: 0.3283993\ttotal: 1m 21s\tremaining: 10.1s\n",
      "889:\tlearn: 0.3283377\ttotal: 1m 21s\tremaining: 10s\n",
      "890:\tlearn: 0.3282701\ttotal: 1m 21s\tremaining: 9.96s\n",
      "891:\tlearn: 0.3282036\ttotal: 1m 21s\tremaining: 9.87s\n",
      "892:\tlearn: 0.3281313\ttotal: 1m 21s\tremaining: 9.79s\n",
      "893:\tlearn: 0.3280772\ttotal: 1m 21s\tremaining: 9.7s\n",
      "894:\tlearn: 0.3280286\ttotal: 1m 21s\tremaining: 9.61s\n",
      "895:\tlearn: 0.3279558\ttotal: 1m 22s\tremaining: 9.52s\n",
      "896:\tlearn: 0.3278958\ttotal: 1m 22s\tremaining: 9.43s\n",
      "897:\tlearn: 0.3278906\ttotal: 1m 22s\tremaining: 9.34s\n",
      "898:\tlearn: 0.3278316\ttotal: 1m 22s\tremaining: 9.25s\n",
      "899:\tlearn: 0.3277914\ttotal: 1m 22s\tremaining: 9.16s\n",
      "900:\tlearn: 0.3277403\ttotal: 1m 22s\tremaining: 9.06s\n",
      "901:\tlearn: 0.3277168\ttotal: 1m 22s\tremaining: 8.97s\n",
      "902:\tlearn: 0.3276551\ttotal: 1m 22s\tremaining: 8.88s\n",
      "903:\tlearn: 0.3275888\ttotal: 1m 22s\tremaining: 8.79s\n",
      "904:\tlearn: 0.3275283\ttotal: 1m 22s\tremaining: 8.7s\n",
      "905:\tlearn: 0.3275205\ttotal: 1m 22s\tremaining: 8.61s\n",
      "906:\tlearn: 0.3275156\ttotal: 1m 23s\tremaining: 8.52s\n",
      "907:\tlearn: 0.3274601\ttotal: 1m 23s\tremaining: 8.43s\n",
      "908:\tlearn: 0.3273954\ttotal: 1m 23s\tremaining: 8.34s\n",
      "909:\tlearn: 0.3273180\ttotal: 1m 23s\tremaining: 8.24s\n",
      "910:\tlearn: 0.3272622\ttotal: 1m 23s\tremaining: 8.15s\n",
      "911:\tlearn: 0.3271964\ttotal: 1m 23s\tremaining: 8.06s\n",
      "912:\tlearn: 0.3271434\ttotal: 1m 23s\tremaining: 7.97s\n",
      "913:\tlearn: 0.3270698\ttotal: 1m 23s\tremaining: 7.88s\n",
      "914:\tlearn: 0.3270082\ttotal: 1m 23s\tremaining: 7.79s\n",
      "915:\tlearn: 0.3269378\ttotal: 1m 23s\tremaining: 7.7s\n",
      "916:\tlearn: 0.3268994\ttotal: 1m 24s\tremaining: 7.61s\n",
      "917:\tlearn: 0.3268201\ttotal: 1m 24s\tremaining: 7.52s\n",
      "918:\tlearn: 0.3267477\ttotal: 1m 24s\tremaining: 7.43s\n",
      "919:\tlearn: 0.3267431\ttotal: 1m 24s\tremaining: 7.33s\n",
      "920:\tlearn: 0.3266796\ttotal: 1m 24s\tremaining: 7.24s\n",
      "921:\tlearn: 0.3266326\ttotal: 1m 24s\tremaining: 7.15s\n",
      "922:\tlearn: 0.3265613\ttotal: 1m 24s\tremaining: 7.06s\n",
      "923:\tlearn: 0.3264974\ttotal: 1m 24s\tremaining: 6.97s\n",
      "924:\tlearn: 0.3264611\ttotal: 1m 24s\tremaining: 6.88s\n",
      "925:\tlearn: 0.3263999\ttotal: 1m 24s\tremaining: 6.78s\n",
      "926:\tlearn: 0.3263280\ttotal: 1m 24s\tremaining: 6.69s\n",
      "927:\tlearn: 0.3262495\ttotal: 1m 25s\tremaining: 6.6s\n",
      "928:\tlearn: 0.3262188\ttotal: 1m 25s\tremaining: 6.51s\n",
      "929:\tlearn: 0.3261611\ttotal: 1m 25s\tremaining: 6.42s\n",
      "930:\tlearn: 0.3261113\ttotal: 1m 25s\tremaining: 6.33s\n",
      "931:\tlearn: 0.3260373\ttotal: 1m 25s\tremaining: 6.24s\n",
      "932:\tlearn: 0.3260308\ttotal: 1m 25s\tremaining: 6.15s\n",
      "933:\tlearn: 0.3259902\ttotal: 1m 25s\tremaining: 6.06s\n",
      "934:\tlearn: 0.3259336\ttotal: 1m 25s\tremaining: 5.97s\n",
      "935:\tlearn: 0.3258702\ttotal: 1m 25s\tremaining: 5.88s\n",
      "936:\tlearn: 0.3257960\ttotal: 1m 26s\tremaining: 5.79s\n",
      "937:\tlearn: 0.3257038\ttotal: 1m 26s\tremaining: 5.7s\n",
      "938:\tlearn: 0.3256288\ttotal: 1m 26s\tremaining: 5.61s\n",
      "939:\tlearn: 0.3255653\ttotal: 1m 26s\tremaining: 5.51s\n",
      "940:\tlearn: 0.3254861\ttotal: 1m 26s\tremaining: 5.42s\n",
      "941:\tlearn: 0.3254304\ttotal: 1m 26s\tremaining: 5.33s\n",
      "942:\tlearn: 0.3253702\ttotal: 1m 26s\tremaining: 5.24s\n",
      "943:\tlearn: 0.3252596\ttotal: 1m 26s\tremaining: 5.15s\n",
      "944:\tlearn: 0.3252127\ttotal: 1m 26s\tremaining: 5.05s\n",
      "945:\tlearn: 0.3251942\ttotal: 1m 26s\tremaining: 4.96s\n",
      "946:\tlearn: 0.3251281\ttotal: 1m 26s\tremaining: 4.87s\n",
      "947:\tlearn: 0.3250576\ttotal: 1m 27s\tremaining: 4.78s\n",
      "948:\tlearn: 0.3249679\ttotal: 1m 27s\tremaining: 4.69s\n",
      "949:\tlearn: 0.3249070\ttotal: 1m 27s\tremaining: 4.6s\n",
      "950:\tlearn: 0.3248452\ttotal: 1m 27s\tremaining: 4.5s\n",
      "951:\tlearn: 0.3247830\ttotal: 1m 27s\tremaining: 4.41s\n",
      "952:\tlearn: 0.3246478\ttotal: 1m 27s\tremaining: 4.32s\n",
      "953:\tlearn: 0.3245899\ttotal: 1m 27s\tremaining: 4.23s\n",
      "954:\tlearn: 0.3245357\ttotal: 1m 27s\tremaining: 4.14s\n",
      "955:\tlearn: 0.3244974\ttotal: 1m 27s\tremaining: 4.05s\n",
      "956:\tlearn: 0.3244750\ttotal: 1m 28s\tremaining: 3.96s\n",
      "957:\tlearn: 0.3244198\ttotal: 1m 28s\tremaining: 3.86s\n",
      "958:\tlearn: 0.3243873\ttotal: 1m 28s\tremaining: 3.77s\n",
      "959:\tlearn: 0.3243293\ttotal: 1m 28s\tremaining: 3.68s\n",
      "960:\tlearn: 0.3241727\ttotal: 1m 28s\tremaining: 3.59s\n",
      "961:\tlearn: 0.3240796\ttotal: 1m 28s\tremaining: 3.49s\n",
      "962:\tlearn: 0.3238861\ttotal: 1m 28s\tremaining: 3.4s\n",
      "963:\tlearn: 0.3238143\ttotal: 1m 28s\tremaining: 3.31s\n",
      "964:\tlearn: 0.3237494\ttotal: 1m 28s\tremaining: 3.22s\n",
      "965:\tlearn: 0.3236490\ttotal: 1m 28s\tremaining: 3.13s\n",
      "966:\tlearn: 0.3235441\ttotal: 1m 28s\tremaining: 3.04s\n",
      "967:\tlearn: 0.3234927\ttotal: 1m 29s\tremaining: 2.94s\n",
      "968:\tlearn: 0.3234161\ttotal: 1m 29s\tremaining: 2.85s\n",
      "969:\tlearn: 0.3233384\ttotal: 1m 29s\tremaining: 2.76s\n",
      "970:\tlearn: 0.3232820\ttotal: 1m 29s\tremaining: 2.67s\n",
      "971:\tlearn: 0.3232772\ttotal: 1m 29s\tremaining: 2.58s\n",
      "972:\tlearn: 0.3232117\ttotal: 1m 29s\tremaining: 2.48s\n",
      "973:\tlearn: 0.3231402\ttotal: 1m 29s\tremaining: 2.39s\n",
      "974:\tlearn: 0.3230769\ttotal: 1m 29s\tremaining: 2.3s\n",
      "975:\tlearn: 0.3229916\ttotal: 1m 29s\tremaining: 2.21s\n",
      "976:\tlearn: 0.3229822\ttotal: 1m 30s\tremaining: 2.12s\n",
      "977:\tlearn: 0.3229309\ttotal: 1m 30s\tremaining: 2.03s\n",
      "978:\tlearn: 0.3228637\ttotal: 1m 30s\tremaining: 1.93s\n",
      "979:\tlearn: 0.3227970\ttotal: 1m 30s\tremaining: 1.84s\n",
      "980:\tlearn: 0.3227593\ttotal: 1m 30s\tremaining: 1.75s\n",
      "981:\tlearn: 0.3227296\ttotal: 1m 30s\tremaining: 1.66s\n",
      "982:\tlearn: 0.3226860\ttotal: 1m 30s\tremaining: 1.56s\n",
      "983:\tlearn: 0.3226534\ttotal: 1m 30s\tremaining: 1.47s\n",
      "984:\tlearn: 0.3226487\ttotal: 1m 30s\tremaining: 1.38s\n",
      "985:\tlearn: 0.3225918\ttotal: 1m 30s\tremaining: 1.29s\n",
      "986:\tlearn: 0.3225172\ttotal: 1m 30s\tremaining: 1.2s\n",
      "987:\tlearn: 0.3224697\ttotal: 1m 30s\tremaining: 1.1s\n",
      "988:\tlearn: 0.3224624\ttotal: 1m 31s\tremaining: 1.01s\n",
      "989:\tlearn: 0.3224001\ttotal: 1m 31s\tremaining: 920ms\n",
      "990:\tlearn: 0.3223456\ttotal: 1m 31s\tremaining: 828ms\n",
      "991:\tlearn: 0.3222936\ttotal: 1m 31s\tremaining: 736ms\n",
      "992:\tlearn: 0.3222352\ttotal: 1m 31s\tremaining: 644ms\n",
      "993:\tlearn: 0.3221350\ttotal: 1m 31s\tremaining: 552ms\n",
      "994:\tlearn: 0.3220814\ttotal: 1m 31s\tremaining: 460ms\n",
      "995:\tlearn: 0.3220457\ttotal: 1m 31s\tremaining: 368ms\n",
      "996:\tlearn: 0.3220413\ttotal: 1m 31s\tremaining: 276ms\n",
      "997:\tlearn: 0.3219779\ttotal: 1m 31s\tremaining: 184ms\n",
      "998:\tlearn: 0.3219291\ttotal: 1m 31s\tremaining: 91.9ms\n",
      "999:\tlearn: 0.3219245\ttotal: 1m 31s\tremaining: 0us\n",
      "accuracy score: 0.8401863874753855\n",
      "f1 score: 0.8234660693903043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train with catboost\n",
    "model_catboost = train_catboost(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper func: get_mean_word2vec() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function get mean word2vec vector for a narrative\n",
    "\n",
    "def get_mean_word2vec(narrative):\n",
    "\n",
    "    # initialize vector\n",
    "    vector = np.zeros(300)\n",
    "\n",
    "    # get all words in narrative\n",
    "    words = narrative.split()\n",
    "    num_words = len(words)\n",
    "\n",
    "    if num_words == 0:  # edge case: empty narrative\n",
    "        return vector\n",
    "\n",
    "    # calculate word vectors using list comprehension\n",
    "    word_vectors = [word2vec[word] for word in words if word in word2vec]\n",
    "\n",
    "    if word_vectors:\n",
    "        vector = np.mean(word_vectors, axis=0)\n",
    "\n",
    "    return vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper func: get_mean_tfidf_weighted_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function get mean tfidf weighted word2vec vector for a narrative\n",
    "def get_mean_tfidf_weighted_word2vec(narrative):\n",
    "\n",
    "    # initialize vector\n",
    "    vector = np.zeros(300)\n",
    "\n",
    "    # get all words in narrative\n",
    "    words = narrative.split()\n",
    "    num_words = len(words)\n",
    "\n",
    "    if num_words == 0:  # edge case: empty narrative\n",
    "        return vector\n",
    "\n",
    "    # pre-calculate word-to-index mapping for tfidf_features for O(1) lookup\n",
    "    word_to_index = {word: idx for idx, word in enumerate(tfidf_features)}\n",
    "\n",
    "    # calculate word vectors using list comprehension\n",
    "    word_vectors = [\n",
    "        word2vec[word] * tfidf_weights[word_to_index[word]]\n",
    "        for word in words if word in word2vec and word in word_to_index\n",
    "    ]\n",
    "\n",
    "    if word_vectors:\n",
    "        vector = np.sum(word_vectors, axis=0) / num_words\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143228/143228 [00:51<00:00, 2776.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute mean word2vec for each narrative\n",
    "df1['narrative_w2v'] = df1['narrative_processed'].progress_apply(get_mean_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143228, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040331</td>\n",
       "      <td>0.026620</td>\n",
       "      <td>0.014841</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>-0.043637</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>-0.029973</td>\n",
       "      <td>0.068532</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047396</td>\n",
       "      <td>0.058287</td>\n",
       "      <td>-0.093701</td>\n",
       "      <td>0.012768</td>\n",
       "      <td>-0.012773</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>-0.056080</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>-0.043245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015335</td>\n",
       "      <td>0.045654</td>\n",
       "      <td>0.027914</td>\n",
       "      <td>0.039537</td>\n",
       "      <td>-0.039236</td>\n",
       "      <td>-0.039606</td>\n",
       "      <td>0.051731</td>\n",
       "      <td>-0.051289</td>\n",
       "      <td>0.112583</td>\n",
       "      <td>0.051383</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062239</td>\n",
       "      <td>0.056668</td>\n",
       "      <td>-0.069291</td>\n",
       "      <td>0.020533</td>\n",
       "      <td>-0.026596</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>0.012877</td>\n",
       "      <td>-0.029749</td>\n",
       "      <td>0.014188</td>\n",
       "      <td>-0.035395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.032917</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.039849</td>\n",
       "      <td>-0.040777</td>\n",
       "      <td>-0.038908</td>\n",
       "      <td>0.052282</td>\n",
       "      <td>-0.054552</td>\n",
       "      <td>0.101980</td>\n",
       "      <td>0.038749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052209</td>\n",
       "      <td>0.040833</td>\n",
       "      <td>-0.063407</td>\n",
       "      <td>0.006547</td>\n",
       "      <td>-0.003008</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>-0.004655</td>\n",
       "      <td>-0.052341</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>-0.044008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007624</td>\n",
       "      <td>0.030079</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>-0.014753</td>\n",
       "      <td>-0.055659</td>\n",
       "      <td>0.012858</td>\n",
       "      <td>0.047460</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.096759</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033893</td>\n",
       "      <td>0.043902</td>\n",
       "      <td>-0.037971</td>\n",
       "      <td>-0.002617</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>-0.008392</td>\n",
       "      <td>0.022653</td>\n",
       "      <td>-0.047779</td>\n",
       "      <td>-0.038818</td>\n",
       "      <td>-0.049850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.032506</td>\n",
       "      <td>0.058801</td>\n",
       "      <td>0.044047</td>\n",
       "      <td>0.027130</td>\n",
       "      <td>-0.017242</td>\n",
       "      <td>-0.051772</td>\n",
       "      <td>0.073264</td>\n",
       "      <td>-0.029725</td>\n",
       "      <td>0.123022</td>\n",
       "      <td>0.024894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090568</td>\n",
       "      <td>0.064280</td>\n",
       "      <td>-0.081249</td>\n",
       "      <td>0.023555</td>\n",
       "      <td>-0.011626</td>\n",
       "      <td>0.061293</td>\n",
       "      <td>0.005938</td>\n",
       "      <td>-0.036182</td>\n",
       "      <td>0.024811</td>\n",
       "      <td>-0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.020267</td>\n",
       "      <td>0.028293</td>\n",
       "      <td>-0.020422</td>\n",
       "      <td>-0.037262</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>0.022784</td>\n",
       "      <td>0.038125</td>\n",
       "      <td>-0.018504</td>\n",
       "      <td>0.159571</td>\n",
       "      <td>-0.024027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037330</td>\n",
       "      <td>0.026582</td>\n",
       "      <td>-0.030571</td>\n",
       "      <td>0.012251</td>\n",
       "      <td>-0.001732</td>\n",
       "      <td>-0.007499</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>-0.034742</td>\n",
       "      <td>-0.005102</td>\n",
       "      <td>-0.053988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005637</td>\n",
       "      <td>0.041543</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>0.065450</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>-0.070378</td>\n",
       "      <td>0.080315</td>\n",
       "      <td>-0.018860</td>\n",
       "      <td>0.075489</td>\n",
       "      <td>0.016204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043222</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>-0.046497</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.034166</td>\n",
       "      <td>0.056444</td>\n",
       "      <td>-0.008661</td>\n",
       "      <td>-0.053928</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>-0.022484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.073286</td>\n",
       "      <td>0.006870</td>\n",
       "      <td>0.045102</td>\n",
       "      <td>-0.107278</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>-0.044868</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>-0.030029</td>\n",
       "      <td>0.142415</td>\n",
       "      <td>0.024306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027323</td>\n",
       "      <td>-0.038622</td>\n",
       "      <td>-0.044881</td>\n",
       "      <td>0.002360</td>\n",
       "      <td>0.076959</td>\n",
       "      <td>0.027995</td>\n",
       "      <td>0.014343</td>\n",
       "      <td>-0.102898</td>\n",
       "      <td>-0.048720</td>\n",
       "      <td>-0.020528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.013750</td>\n",
       "      <td>0.042846</td>\n",
       "      <td>-0.022878</td>\n",
       "      <td>0.030510</td>\n",
       "      <td>0.058003</td>\n",
       "      <td>-0.070757</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>-0.057863</td>\n",
       "      <td>0.120751</td>\n",
       "      <td>0.033679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049232</td>\n",
       "      <td>0.052852</td>\n",
       "      <td>-0.032314</td>\n",
       "      <td>0.011897</td>\n",
       "      <td>0.044683</td>\n",
       "      <td>0.023794</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>-0.051543</td>\n",
       "      <td>0.025996</td>\n",
       "      <td>-0.102859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.006047</td>\n",
       "      <td>0.064650</td>\n",
       "      <td>0.034185</td>\n",
       "      <td>-0.007944</td>\n",
       "      <td>-0.045458</td>\n",
       "      <td>-0.023677</td>\n",
       "      <td>0.062364</td>\n",
       "      <td>-0.009629</td>\n",
       "      <td>0.088057</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091455</td>\n",
       "      <td>0.040417</td>\n",
       "      <td>-0.077361</td>\n",
       "      <td>0.011054</td>\n",
       "      <td>0.040952</td>\n",
       "      <td>0.009603</td>\n",
       "      <td>-0.021489</td>\n",
       "      <td>-0.023653</td>\n",
       "      <td>-0.035569</td>\n",
       "      <td>-0.033850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.040331  0.026620  0.014841  0.014846 -0.043637  0.000967  0.057010   \n",
       "1  0.015335  0.045654  0.027914  0.039537 -0.039236 -0.039606  0.051731   \n",
       "2  0.015713  0.032917  0.015895  0.039849 -0.040777 -0.038908  0.052282   \n",
       "3  0.007624  0.030079  0.028481 -0.014753 -0.055659  0.012858  0.047460   \n",
       "4  0.032506  0.058801  0.044047  0.027130 -0.017242 -0.051772  0.073264   \n",
       "5 -0.020267  0.028293 -0.020422 -0.037262 -0.023846  0.022784  0.038125   \n",
       "6  0.005637  0.041543  0.053054  0.065450  0.005728 -0.070378  0.080315   \n",
       "7 -0.073286  0.006870  0.045102 -0.107278  0.017687 -0.044868  0.003883   \n",
       "8 -0.013750  0.042846 -0.022878  0.030510  0.058003 -0.070757  0.002632   \n",
       "9  0.006047  0.064650  0.034185 -0.007944 -0.045458 -0.023677  0.062364   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0 -0.029973  0.068532  0.010967  ... -0.047396  0.058287 -0.093701  0.012768   \n",
       "1 -0.051289  0.112583  0.051383  ... -0.062239  0.056668 -0.069291  0.020533   \n",
       "2 -0.054552  0.101980  0.038749  ... -0.052209  0.040833 -0.063407  0.006547   \n",
       "3  0.002450  0.096759 -0.000364  ... -0.033893  0.043902 -0.037971 -0.002617   \n",
       "4 -0.029725  0.123022  0.024894  ... -0.090568  0.064280 -0.081249  0.023555   \n",
       "5 -0.018504  0.159571 -0.024027  ... -0.037330  0.026582 -0.030571  0.012251   \n",
       "6 -0.018860  0.075489  0.016204  ... -0.043222  0.056497 -0.046497  0.008965   \n",
       "7 -0.030029  0.142415  0.024306  ...  0.027323 -0.038622 -0.044881  0.002360   \n",
       "8 -0.057863  0.120751  0.033679  ... -0.049232  0.052852 -0.032314  0.011897   \n",
       "9 -0.009629  0.088057  0.026872  ... -0.091455  0.040417 -0.077361  0.011054   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0 -0.012773  0.007259  0.010107 -0.056080  0.007923 -0.043245  \n",
       "1 -0.026596  0.005863  0.012877 -0.029749  0.014188 -0.035395  \n",
       "2 -0.003008  0.004260 -0.004655 -0.052341  0.006344 -0.044008  \n",
       "3 -0.003508 -0.008392  0.022653 -0.047779 -0.038818 -0.049850  \n",
       "4 -0.011626  0.061293  0.005938 -0.036182  0.024811 -0.050300  \n",
       "5 -0.001732 -0.007499  0.022391 -0.034742 -0.005102 -0.053988  \n",
       "6  0.034166  0.056444 -0.008661 -0.053928  0.017788 -0.022484  \n",
       "7  0.076959  0.027995  0.014343 -0.102898 -0.048720 -0.020528  \n",
       "8  0.044683  0.023794 -0.016121 -0.051543  0.025996 -0.102859  \n",
       "9  0.040952  0.009603 -0.021489 -0.023653 -0.035569 -0.033850  \n",
       "\n",
       "[10 rows x 300 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert df1['narrative_w2v']  to numpy array X_w2v\n",
    "X_w2v = np.array(df1.narrative_w2v.tolist())\n",
    "\n",
    "# print shape of X_w2v\n",
    "print(X_w2v.shape)\n",
    "\n",
    "pd.DataFrame(X_w2v).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214842, 300) (214842,)\n",
      "1    107421\n",
      "0    107421\n",
      "Name: disputed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# apply smote to oversample minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_w2v_smote, y_w2v_smote = smote.fit_resample(X_w2v, y)\n",
    "\n",
    "# print shape of X_w2v_smote, y_w2v_smote\n",
    "print(X_w2v_smote.shape, y_w2v_smote.shape)\n",
    "\n",
    "# print value counts of y_w2v_smote\n",
    "print(pd.Series(y_w2v_smote).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split: X_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_train, y_w2v_smote_test, 70% train, 30% test, random_state=0, stratify=y_w2v_smote\n",
    "X_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_train, y_w2v_smote_test = train_test_split(X_w2v_smote, y_w2v_smote, stratify=y_w2v_smote, random_state=0, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression w2v: 0.593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with logistic regression with smoted word2vec data\n",
    "model_lr = train_lr(X_w2v_smote_train, y_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier w2v 0.714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with xgboost\n",
    "model_xgb = train_xgb(X_w2v_smote_train, y_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoostClassifier w2v, f1 score: 0.715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with catboost\n",
    "model_catboost = train_catboost(X_w2v_smote_train, y_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN w2v, f1: 0.653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 23:18:44.574099: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/apple/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/keras/src/engine/data_adapter.py:1798: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3760/3760 [==============================] - 123s 33ms/step - loss: 0.6600 - accuracy: 0.6079 - val_loss: 0.6469 - val_accuracy: 0.6276\n",
      "Epoch 2/10\n",
      "3760/3760 [==============================] - 115s 31ms/step - loss: 0.6436 - accuracy: 0.6307 - val_loss: 0.6427 - val_accuracy: 0.6211\n",
      "Epoch 3/10\n",
      "3760/3760 [==============================] - 116s 31ms/step - loss: 0.6372 - accuracy: 0.6368 - val_loss: 0.6385 - val_accuracy: 0.6265\n",
      "Epoch 4/10\n",
      "3760/3760 [==============================] - 118s 31ms/step - loss: 0.6332 - accuracy: 0.6403 - val_loss: 0.6334 - val_accuracy: 0.6446\n",
      "Epoch 5/10\n",
      "3760/3760 [==============================] - 119s 32ms/step - loss: 0.6306 - accuracy: 0.6429 - val_loss: 0.6386 - val_accuracy: 0.6266\n",
      "Epoch 6/10\n",
      "3760/3760 [==============================] - 123s 33ms/step - loss: 0.6288 - accuracy: 0.6453 - val_loss: 0.6278 - val_accuracy: 0.6487\n",
      "Epoch 7/10\n",
      "3760/3760 [==============================] - 118s 31ms/step - loss: 0.6264 - accuracy: 0.6474 - val_loss: 0.6268 - val_accuracy: 0.6480\n",
      "Epoch 8/10\n",
      "3760/3760 [==============================] - 119s 32ms/step - loss: 0.6248 - accuracy: 0.6487 - val_loss: 0.6316 - val_accuracy: 0.6384\n",
      "Epoch 9/10\n",
      "3760/3760 [==============================] - 129s 34ms/step - loss: 0.6233 - accuracy: 0.6507 - val_loss: 0.6271 - val_accuracy: 0.6499\n",
      "Epoch 10/10\n",
      "3760/3760 [==============================] - 130s 34ms/step - loss: 0.6218 - accuracy: 0.6525 - val_loss: 0.6271 - val_accuracy: 0.6492\n",
      "Epoch 10: early stopping\n",
      "2015/2015 [==============================] - 16s 8ms/step\n",
      "f1 score: 0.653944831564725\n"
     ]
    }
   ],
   "source": [
    "# train with deep learning CNN model with word2vec data with smote, wrap in a function\n",
    "\n",
    "def train_cnn(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # define CNN model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(128, 3, activation='relu', input_shape=(300, 1)))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[es])\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = model.predict(X_test.reshape(X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # print f1 score\n",
    "    print(f\"f1 score: {f1_score(y_test, y_pred.round())}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with deep learning CNN model with weighted word2vec data with smote\n",
    "model_cnn = train_cnn(X_w2v_smote_train, y_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM w2v, f1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/keras/src/engine/data_adapter.py:1798: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3172/3760 [========================>.....] - ETA: 2:48 - loss: 0.6933 - accuracy: 0.5020"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb Cell 121\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y246sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y246sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# train with deep learning LSTM model with word2vec data with smote\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y246sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m model_lstm \u001b[39m=\u001b[39m train_lstm(X_w2v_smote_train, y_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_test)\n",
      "\u001b[1;32m/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb Cell 121\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y246sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m es \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y246sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# fit model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y246sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train\u001b[39m.\u001b[39;49mreshape(X_train\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], X_train\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], \u001b[39m1\u001b[39;49m), y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[es])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y246sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# predict on test set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y246sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test\u001b[39m.\u001b[39mreshape(X_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], X_test\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train with deep learning LSTM model with word2vec data with smote, wrap in a function\n",
    "\n",
    "def train_lstm(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # define LSTM model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(300, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[es])\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = model.predict(X_test.reshape(X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # print f1 score\n",
    "    print(f\"f1 score: {f1_score(y_test, y_pred.round())}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with deep learning LSTM model with word2vec data with smote\n",
    "model_lstm = train_lstm(X_w2v_smote_train, y_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN w2v f1 0.662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with deep learning ANN model with word2vec data with smote, wrap in a function\n",
    "\n",
    "def train_ann(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # define ANN model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(300,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[es])\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # print f1 score\n",
    "    print(f\"f1 score: {f1_score(y_test, y_pred.round())}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with deep learning ANN model with word2vec data with smote\n",
    "model_ann = train_ann(X_w2v_smote_train, y_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict a new complaint narrative with every model\n",
    "\n",
    "# define a function to predict with every model\n",
    "\n",
    "def predict_complaint(narrative):\n",
    "\n",
    "    # preprocess narrative\n",
    "    narrative_processed = preprocess_narrative(narrative)\n",
    "\n",
    "    # get word2vec vector\n",
    "    w2v_vector = get_mean_word2vec(narrative_processed)\n",
    "\n",
    "    # get predictions\n",
    "    lr_pred = model_lr.predict(w2v_vector.reshape(1, 300))\n",
    "    # nb_pred = model_nb.predict(w2v_vector)\n",
    "    # xgb_pred = model_xgb.predict(w2v_vector)\n",
    "    catboost_pred = model_catboost.predict(w2v_vector.reshape(1, 300))\n",
    "    model_cnn_pred = model_cnn.predict(w2v_vector.reshape(1, 300, 1))\n",
    "    model_ann_pred = model_ann.predict(w2v_vector.reshape(1, 300))\n",
    "\n",
    "    # print predictions\n",
    "    print(f\"lr_pred: {lr_pred}\")\n",
    "    # print(f\"nb_pred: {nb_pred}\")\n",
    "    # print(f\"xgb_pred: {xgb_pred}\")\n",
    "    print(f\"catboost_pred: {catboost_pred}\")\n",
    "    print(f\"model_cnn_pred: {model_cnn_pred}\")\n",
    "    print(f\"model_ann_pred: {model_ann_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "lr_pred: [0]\n",
      "catboost_pred: [1]\n",
      "model_cnn_pred: [[0.43557966]]\n",
      "model_ann_pred: [[0.53490543]]\n"
     ]
    }
   ],
   "source": [
    "# predict with every model\n",
    "\n",
    "predict_complaint(\"someone used my information to open a Bank of America credit card account without my consent. I have never had an account with Bank of America and I have never authorized anyone to open an account on my behalf. I have contacted Bank of America and they have refused to close the account and remove it from my credit report. I have also filed a police report and submitted it to Bank of America. I have also submitted a complaint to the CFPB and the FTC. I am requesting that the CFPB investigate this matter and take appropriate action against Bank of America.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3057419: 'applied go school chose shoot degree nt job time living parents applying immediately accepted offered starter pack types art supplies begin journey learning creativity nt dawn look back see friendly staff signing papers loans attended classes constantly questioned self really related striving constantly called office sign paperwork tuition staff assured would pay back two years racking debt attending mediocre classes pessimistic instructors called office counselor told could attend school anymore needed go get general education classes somewhere else return year reapply wait knowing better discontinued going went life left began work became productive citizen year kicked began receive call collection agencies loan unable pay rent time let alone student loans years gone bye applying deferments forbeance find debt certainly aware would blow like research knowing involved many predatory lending strongly believe victim',\n",
       " 1261053: 'refinanced va loan lowvaratescom process began streamline va refinance alot paperwork done advised would nt take long close several occasions finally got closing date signed closing papers friday understood payment included payoff would make payment went business received email broker sent email pm read somewhere around advised mistake paperwork resign papers sent notary us around sign corrected paperwork paperwork corrected realized mistake paperwork delayed funding payoff old loan happen point payment told paid payoff house days late spent month expecting close one occasion make payment supposed close told would included next morning emailed broker asked would considered days late exact words nt said lender report provide letter clear ok couple weeks later checked credit saw payment reported days late reduced scores nearly points broker wrote letter mortgage company explaining told payment made mortgage company refused make change reporting broker told us could happen advised us pay payment immediately although already tried contact us month payment likely already late',\n",
       " 1293705: 'lease auto loan want payoff pending amount bank banco popular de puerto rico popular bank calculating cancellation amount using rule addition claim cancellation fee therefore receiving double penalty using rule cancellation claim federal code allow use rule like penalty consumer want pay loan loan time using double penalty allowed',\n",
       " 3003672: 'equifax currently reporting tax lien released withdrawn longer exists public record needs deleted report entirely refuse investigate report verified years withdrawn must start new investigation provide contact information verifying refused investigating',\n",
       " 2588289: 'data furnisher hereby referenced credit reporting agency cra declines comply fcra b usc c block upon receipt filled police report confirming identity theft cra violation fcra monetary penalties legal damages administrative enforcement civil liability willful noncompliance usc n civil liability negligent noncompliance usc o data reported cra compiled using wrong social security number wrong address furthermore cra previously deleted information consumer file confirming alleged true name identity theft occasions cra records past deleted information due true name identity theft enclosure presents verification debt provided reporting agent consumer address reported match consumer report information ss employment insurance incorrect enclosure legal action debt collector cfpb complaint consumer remedy comply block information consumer report pursuant fcra b usc c within four business days',\n",
       " 1424822: 'refinanced mortgage held bank america another lender b loan paid knowing loan going paid removed authorization automatic payments b mortgage made unauthorized withdrawl account already paid loan told would refunded days along escrow balance nice steal use money days returning fraud punishable problem closing account immediately longer able access b mortgage account online could verify charges put wait see final statement days also refi process charged fee expedited payoff balance expedited meaning via email paid postage mail would free regulated researched time website said payoff could updated addional charge could give payoff balance anyone without approval well happened payoff balance apparently expired charged additional new lender get another one called told charge payoff balance processed cost say owed take additional money anyway actual payoff total meaningless requesting refund pay service charges fraud hope legal action taken prevent similar situations future wo nt happen never business bank america',\n",
       " 1538877: 'midland funding attempting collect debt little validation received bill knowledge believe identity fraud decided go research texas midland funding active bond collect debt state texas huge violation section texas prohibits thirdparty debt collector credit bureau engaging debt collection texas unless thirdparty debt collector obtained surety bond filed copy bond office secretary state please go verify fact regarding bonding status true texas',\n",
       " 1690006: 'company continues report credit report sent letter telling account mine idea belongs asked proof signed contract asked license collect state asked copies information referenced debt still date received anything harassment company debt',\n",
       " 1471265: 'enrolled institute take program promised job placing great job pay back student loan received grant completed program call ask diploma certificate told would mail call back never received either found school lawsuit shut charged program added forced debt scam students',\n",
       " 1099858: 'bought initially want buy seeing way handled road convinced nothing trouble since purchased last year mainly customer service department nissan motor acceptance corporation ranged rude downright unprofessional made arrangement executive pay first several back payments days made arrangement contacted repo agent showed apartment repossess car update information took car contacted company retrieve repo agent came back next day along sorry excuse apology went recovery company hired get sort compensation put tried make amends terms reconciling delinquent account well way fulfilling involved motor vehicle accident required car fixed atfault insurance company wrote check amount damage body shop would paid used repair money pay asked assistance help secure vehicle paying body shop still sitting still owe initially refused help asked executive specialist refund money pay owner said would asked let speak someone refused let talk someone last point contact also spoke repossession department one coerced repossessing vehicle whose exact words food thought permission car seized body shop pay fees would need speak reinstatement department possibly make arrangements pay storage fees may able work payment plan rest account good standing want try help told give time raise money took route unsuccessful raising reluctantly tearfully car seized never sent letter retrieve vehicle possessions told car sold auction found illinois attorney general office also made unsuccessful complaint chose hounded harassed humiliated customer service representatives supervisors managers treated like criminal found late payments even made payments owed still found way treat horribly know car repossessed account good standing illegal also know making initial arrangement pay late payment executive specialist car repossessed mistake days later fight like get car back supposed pay error pay either attaching copies bank statements extension agreement letter illinois attorney general office well escalation letter sent behalf also working get repossession charge removed credit reports'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.narrative_processed.sample(10).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df1.disputed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tfidf_matrix: (143228, 87792)\n",
      "['aa', 'aaa', 'aaaa', 'aaaaan', 'aaaaargh', 'aaaallllll', 'aaadvantage', 'aaaked', 'aaaratings', 'aaarm']\n",
      "['zonemapping', 'zones', 'zonesnot', 'zonethanking', 'zoning', 'zoo', 'zoom', 'zooms', 'ztuff', 'zwicker']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# tfidf on narrative_processed column\n",
    "\n",
    "# initialize tfidf vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# fit and transform tfidf vectorizer on narrative_processed column\n",
    "tfidf.fit(df1.narrative_processed)\n",
    "tfidf_matrix = tfidf.transform(df1.narrative_processed)\n",
    "\n",
    "# print shape of tfidf_matrix\n",
    "print(f\"shape of tfidf_matrix: {tfidf_matrix.shape}\")\n",
    "\n",
    "# get tfidf feature names\n",
    "tfidf_features = tfidf.get_feature_names()\n",
    "\n",
    "# print first 10 and last 10 feature names\n",
    "print(tfidf_features[:10]), print(tfidf_features[-10:]),\n",
    "\n",
    "# get tfidf weights\n",
    "tfidf_weights = tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214842, 87792) (214842,)\n",
      "1    107421\n",
      "0    107421\n",
      "Name: disputed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# smote to oversample minority class for tfidf data\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(tfidf_matrix, y)\n",
    "\n",
    "# print shape of X_smote, y_smote\n",
    "print(X_smote.shape, y_smote.shape)\n",
    "\n",
    "# print value counts of y_smote\n",
    "print(pd.Series(y_smote).value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143228/143228 [1:01:56<00:00, 38.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute mean tfidf weighted word2vec for each narrative\n",
    "\n",
    "df1['narrative_tfidf_w2v'] = df1['narrative_processed'].progress_apply(get_mean_tfidf_weighted_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143228, 300)\n",
      "(143228,)\n"
     ]
    }
   ],
   "source": [
    "# convert df1['narrative_tfidf_w2v']  to numpy array X_tfidf_w2v\n",
    "X_tfidf_w2v = np.array(df1.narrative_tfidf_w2v.tolist())\n",
    "\n",
    "# print shape of X_tfidf_w2v\n",
    "print(X_tfidf_w2v.shape)\n",
    "\n",
    "# print shape of y\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214842, 300) (214842,)\n",
      "1    107421\n",
      "0    107421\n",
      "Name: disputed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# apply smote to oversample minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_tfidf_w2v_smote, y_tfidf_w2v_smote = smote.fit_resample(X_tfidf_w2v, y)\n",
    "\n",
    "# print shape of X_tfidf_w2v_smote, y_tfidf_w2v_smote\n",
    "print(X_tfidf_w2v_smote.shape, y_tfidf_w2v_smote.shape)\n",
    "\n",
    "# print value counts of y_tfidf_w2v_smote\n",
    "print(pd.Series(y_tfidf_w2v_smote).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171873, 300) (42969, 300) (171873,) (42969,)\n"
     ]
    }
   ],
   "source": [
    "# train test split, 20% test, random_state=0, stratify=y_tfidf_w2v_smote\n",
    "X_tfidf_w2v_smote_train, X_tfidf_w2v_smote_test, y_tfidf_w2v_smote_train, y_tfidf_w2v_smote_test = train_test_split(X_tfidf_w2v_smote, y_tfidf_w2v_smote, stratify=y_tfidf_w2v_smote, random_state=0, test_size=0.2)\n",
    "\n",
    "# print shape of X_tfidf_w2v_smote_train, X_tfidf_w2v_smote_test, y_tfidf_w2v_smote_train, y_tfidf_w2v_smote_test\n",
    "print(X_tfidf_w2v_smote_train.shape, X_tfidf_w2v_smote_test.shape, y_tfidf_w2v_smote_train.shape, y_tfidf_w2v_smote_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression tfidf w2v, f1 score: 0.583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train with logistic regression with smoted tfidf weighted word2vec data\n",
    "\n",
    "model_lr = train_lr(X_tfidf_w2v_smote_train, y_tfidf_w2v_smote_train, X_tfidf_w2v_smote_test, y_tfidf_w2v_smote_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "model_lr = train_lr(scaler.fit_transform(X_tfidf_w2v_smote_train), y_tfidf_w2v_smote_train, scaler.transform(X_tfidf_w2v_smote_test), y_tfidf_w2v_smote_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier TFIDF W2V f1: 0.709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train with xgboost\n",
    "model_xgb = train_xgb(X_tfidf_w2v_smote_train, y_tfidf_w2v_smote_train, X_tfidf_w2v_smote_test, y_tfidf_w2v_smote_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoostClassifier TFIDF W2v f1: 0.709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train with catboost\n",
    "model_catboost = train_catboost(X_tfidf_w2v_smote_train, y_tfidf_w2v_smote_train, X_tfidf_w2v_smote_test, y_tfidf_w2v_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN TFIDF W2V f1: 0.706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_ann = train_ann(X_tfidf_w2v_smote_train, y_tfidf_w2v_smote_train, X_tfidf_w2v_smote_test, y_tfidf_w2v_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN TFIDF W2C f1: 0.609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train with CNN\n",
    "\n",
    "model_cnn = train_cnn(X_tfidf_w2v_smote_train, y_tfidf_w2v_smote_train, X_tfidf_w2v_smote_test, y_tfidf_w2v_smote_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment. All narrative && all disputed. SMOTE full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to narrative processed and disputed\n",
    "\n",
    "df2 = df2[['narrative', 'disputed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     128227\n",
       "Yes     35807\n",
       "Name: disputed, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check value counts of disputed column\n",
    "\n",
    "df2.disputed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map values in disputed column to 0 and 1\n",
    "\n",
    "df2.disputed = df2.disputed.map({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164034/164034 [00:15<00:00, 10565.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocess text column\n",
    "\n",
    "df2['narrative_processed'] = df2.narrative.progress_apply(preprocess_narrative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF ONLY Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tfidf_matrix: (164034, 94466)\n",
      "['aa', 'aaa', 'aaaa', 'aaaaan', 'aaaaargh', 'aaaallllll', 'aaabank', 'aaadvantage', 'aaaked', 'aaaratings']\n",
      "['zonemapping', 'zones', 'zonesnot', 'zonethanking', 'zoning', 'zoo', 'zoom', 'zooms', 'ztuff', 'zwicker']\n"
     ]
    }
   ],
   "source": [
    "# tfidf on narrative_processed column\n",
    "\n",
    "# initialize tfidf vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# fit and transform tfidf vectorizer on narrative_processed column\n",
    "tfidf.fit(df2.narrative_processed)\n",
    "tfidf_matrix = tfidf.transform(df2.narrative_processed)\n",
    "\n",
    "# print shape of tfidf_matrix\n",
    "print(f\"shape of tfidf_matrix: {tfidf_matrix.shape}\")\n",
    "\n",
    "# get tfidf feature names\n",
    "tfidf_features = tfidf.get_feature_names()\n",
    "\n",
    "# print first 10 and last 10 feature names\n",
    "print(tfidf_features[:10]), print(tfidf_features[-10:]),\n",
    "\n",
    "# get tfidf weights\n",
    "tfidf_weights = tfidf.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smote to oversample minority class (full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256454, 94466) (256454,)\n",
      "1    128227\n",
      "0    128227\n",
      "Name: disputed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# smote to oversample minority class for tfidf data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(tfidf_matrix, df2.disputed)\n",
    "\n",
    "# print shape of X_smote, y_smote\n",
    "print(X_smote.shape, y_smote.shape)\n",
    "\n",
    "# print value counts of y_smote\n",
    "print(pd.Series(y_smote).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train test split on SMOTE-ed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205163, 94466) (51291, 94466) (205163,) (51291,)\n"
     ]
    }
   ],
   "source": [
    "# train test split, 20% test, random_state=0, stratify=y_smote\n",
    "X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, stratify=y_smote, random_state=0, test_size=0.2)\n",
    "\n",
    "# print shape of X_smote_train, X_smote_test, y_smote_train, y_smote_test\n",
    "print(X_smote_train.shape, X_smote_test.shape, y_smote_train.shape, y_smote_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression tfidf: 0.674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train with logistic regression\n",
    "model_lr = train_lr(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature_names  feature_importance\n",
      "82191          tcpa            6.225606\n",
      "74597     scottrade            4.247667\n",
      "10725          boat            3.519599\n",
      "26050            dl            3.140639\n",
      "29255         equal            3.087012\n",
      "30478     executive            2.999209\n",
      "29297       equifax            2.940859\n",
      "36492        google            2.922883\n",
      "2383     affiliates            2.903436\n",
      "24885   disappeared            2.886327\n"
     ]
    }
   ],
   "source": [
    "# feature importance with logistic regression\n",
    "\n",
    "# get feature names\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "# get feature importance\n",
    "feature_importance = model_lr.coef_[0]\n",
    "\n",
    "# create dataframe of feature names and feature importance\n",
    "df_feature_importance = pd.DataFrame({'feature_names': feature_names, 'feature_importance': feature_importance})\n",
    "\n",
    "# sort dataframe by feature importance\n",
    "df_feature_importance.sort_values(by='feature_importance', ascending=False, inplace=True)\n",
    "\n",
    "# print top 10 features\n",
    "print(df_feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_feature_importance = df_feature_importance.sort_values(by='feature_importance', ascending=False)\n",
    "\n",
    "# plot top 20 features\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x='feature_importance', y='feature_names', data=df_feature_importance.head(20))\n",
    "plt.title('Top 20 Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes tfidf: 0.735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.734605295438676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# since naive bayes doesn't work with negative values, we scale the values\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "model_lr = train_lr(scaler.fit_transform(X_smote_train), y_smote_train, scaler.transform(X_smote_test), y_smote_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoostClassifier tfidf: 0.794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.100046\n",
      "0:\tlearn: 0.6757410\ttotal: 1.07s\tremaining: 17m 50s\n",
      "1:\tlearn: 0.6601302\ttotal: 2.05s\tremaining: 17m 3s\n",
      "2:\tlearn: 0.6449992\ttotal: 3.02s\tremaining: 16m 43s\n",
      "3:\tlearn: 0.6337022\ttotal: 3.95s\tremaining: 16m 24s\n",
      "4:\tlearn: 0.6222814\ttotal: 4.92s\tremaining: 16m 20s\n",
      "5:\tlearn: 0.6124706\ttotal: 5.83s\tremaining: 16m 6s\n",
      "6:\tlearn: 0.6047401\ttotal: 6.7s\tremaining: 15m 49s\n",
      "7:\tlearn: 0.5980568\ttotal: 7.58s\tremaining: 15m 40s\n",
      "8:\tlearn: 0.5903439\ttotal: 8.49s\tremaining: 15m 34s\n",
      "9:\tlearn: 0.5841083\ttotal: 9.39s\tremaining: 15m 29s\n",
      "10:\tlearn: 0.5773165\ttotal: 10.3s\tremaining: 15m 25s\n",
      "11:\tlearn: 0.5724892\ttotal: 11.2s\tremaining: 15m 18s\n",
      "12:\tlearn: 0.5669780\ttotal: 12.1s\tremaining: 15m 16s\n",
      "13:\tlearn: 0.5629218\ttotal: 13s\tremaining: 15m 14s\n",
      "14:\tlearn: 0.5591755\ttotal: 13.9s\tremaining: 15m 12s\n",
      "15:\tlearn: 0.5566081\ttotal: 14.8s\tremaining: 15m 7s\n",
      "16:\tlearn: 0.5531827\ttotal: 15.6s\tremaining: 15m 4s\n",
      "17:\tlearn: 0.5507534\ttotal: 16.5s\tremaining: 15m 1s\n",
      "18:\tlearn: 0.5478491\ttotal: 17.4s\tremaining: 14m 59s\n",
      "19:\tlearn: 0.5459582\ttotal: 18.3s\tremaining: 14m 57s\n",
      "20:\tlearn: 0.5428667\ttotal: 19.2s\tremaining: 14m 54s\n",
      "21:\tlearn: 0.5389518\ttotal: 20.1s\tremaining: 14m 52s\n",
      "22:\tlearn: 0.5356207\ttotal: 21s\tremaining: 14m 53s\n",
      "23:\tlearn: 0.5320525\ttotal: 21.9s\tremaining: 14m 52s\n",
      "24:\tlearn: 0.5302796\ttotal: 22.8s\tremaining: 14m 50s\n",
      "25:\tlearn: 0.5281834\ttotal: 23.7s\tremaining: 14m 48s\n",
      "26:\tlearn: 0.5261551\ttotal: 24.6s\tremaining: 14m 47s\n",
      "27:\tlearn: 0.5239407\ttotal: 25.5s\tremaining: 14m 44s\n",
      "28:\tlearn: 0.5222412\ttotal: 26.4s\tremaining: 14m 42s\n",
      "29:\tlearn: 0.5207162\ttotal: 27.2s\tremaining: 14m 40s\n",
      "30:\tlearn: 0.5187689\ttotal: 28.1s\tremaining: 14m 38s\n",
      "31:\tlearn: 0.5171168\ttotal: 29s\tremaining: 14m 37s\n",
      "32:\tlearn: 0.5162113\ttotal: 29.9s\tremaining: 14m 35s\n",
      "33:\tlearn: 0.5143099\ttotal: 30.8s\tremaining: 14m 33s\n",
      "34:\tlearn: 0.5135001\ttotal: 31.6s\tremaining: 14m 30s\n",
      "35:\tlearn: 0.5126624\ttotal: 32.5s\tremaining: 14m 29s\n",
      "36:\tlearn: 0.5119988\ttotal: 33.4s\tremaining: 14m 28s\n",
      "37:\tlearn: 0.5114256\ttotal: 34.2s\tremaining: 14m 26s\n",
      "38:\tlearn: 0.5099134\ttotal: 35.1s\tremaining: 14m 24s\n",
      "39:\tlearn: 0.5087671\ttotal: 35.9s\tremaining: 14m 22s\n",
      "40:\tlearn: 0.5071657\ttotal: 36.8s\tremaining: 14m 21s\n",
      "41:\tlearn: 0.5065409\ttotal: 37.7s\tremaining: 14m 19s\n",
      "42:\tlearn: 0.5058508\ttotal: 38.5s\tremaining: 14m 17s\n",
      "43:\tlearn: 0.5053884\ttotal: 39.3s\tremaining: 14m 14s\n",
      "44:\tlearn: 0.5049593\ttotal: 40.2s\tremaining: 14m 12s\n",
      "45:\tlearn: 0.5044621\ttotal: 41.1s\tremaining: 14m 11s\n",
      "46:\tlearn: 0.5039966\ttotal: 41.9s\tremaining: 14m 9s\n",
      "47:\tlearn: 0.5031401\ttotal: 42.7s\tremaining: 14m 7s\n",
      "48:\tlearn: 0.5021336\ttotal: 43.6s\tremaining: 14m 6s\n",
      "49:\tlearn: 0.5016892\ttotal: 44.4s\tremaining: 14m 4s\n",
      "50:\tlearn: 0.5009139\ttotal: 45.4s\tremaining: 14m 4s\n",
      "51:\tlearn: 0.5005861\ttotal: 46.3s\tremaining: 14m 3s\n",
      "52:\tlearn: 0.4998096\ttotal: 47.2s\tremaining: 14m 2s\n",
      "53:\tlearn: 0.4992749\ttotal: 48.1s\tremaining: 14m 3s\n",
      "54:\tlearn: 0.4989250\ttotal: 49.1s\tremaining: 14m 2s\n",
      "55:\tlearn: 0.4985571\ttotal: 50s\tremaining: 14m 2s\n",
      "56:\tlearn: 0.4980036\ttotal: 51.2s\tremaining: 14m 6s\n",
      "57:\tlearn: 0.4976631\ttotal: 52.3s\tremaining: 14m 8s\n",
      "58:\tlearn: 0.4972331\ttotal: 53.3s\tremaining: 14m 10s\n",
      "59:\tlearn: 0.4969019\ttotal: 54.8s\tremaining: 14m 18s\n",
      "60:\tlearn: 0.4965597\ttotal: 56.7s\tremaining: 14m 32s\n",
      "61:\tlearn: 0.4961150\ttotal: 58s\tremaining: 14m 37s\n",
      "62:\tlearn: 0.4953568\ttotal: 59.6s\tremaining: 14m 45s\n",
      "63:\tlearn: 0.4951130\ttotal: 1m 1s\tremaining: 14m 52s\n",
      "64:\tlearn: 0.4940934\ttotal: 1m 2s\tremaining: 15m 1s\n",
      "65:\tlearn: 0.4938500\ttotal: 1m 4s\tremaining: 15m 13s\n",
      "66:\tlearn: 0.4927576\ttotal: 1m 6s\tremaining: 15m 27s\n",
      "67:\tlearn: 0.4924661\ttotal: 1m 8s\tremaining: 15m 42s\n",
      "68:\tlearn: 0.4921941\ttotal: 1m 10s\tremaining: 15m 57s\n",
      "69:\tlearn: 0.4916639\ttotal: 1m 13s\tremaining: 16m 13s\n",
      "70:\tlearn: 0.4914170\ttotal: 1m 16s\tremaining: 16m 36s\n",
      "71:\tlearn: 0.4911279\ttotal: 1m 19s\tremaining: 16m 58s\n",
      "72:\tlearn: 0.4908893\ttotal: 1m 21s\tremaining: 17m 15s\n",
      "73:\tlearn: 0.4904974\ttotal: 1m 24s\tremaining: 17m 33s\n",
      "74:\tlearn: 0.4902711\ttotal: 1m 26s\tremaining: 17m 51s\n",
      "75:\tlearn: 0.4900158\ttotal: 1m 29s\tremaining: 18m 11s\n",
      "76:\tlearn: 0.4897733\ttotal: 1m 32s\tremaining: 18m 28s\n",
      "77:\tlearn: 0.4885523\ttotal: 1m 35s\tremaining: 18m 44s\n",
      "78:\tlearn: 0.4882896\ttotal: 1m 37s\tremaining: 18m 55s\n",
      "79:\tlearn: 0.4880048\ttotal: 1m 39s\tremaining: 19m 5s\n",
      "80:\tlearn: 0.4873545\ttotal: 1m 41s\tremaining: 19m 16s\n",
      "81:\tlearn: 0.4871159\ttotal: 1m 43s\tremaining: 19m 22s\n",
      "82:\tlearn: 0.4855545\ttotal: 1m 45s\tremaining: 19m 27s\n",
      "83:\tlearn: 0.4852795\ttotal: 1m 47s\tremaining: 19m 29s\n",
      "84:\tlearn: 0.4850648\ttotal: 1m 48s\tremaining: 19m 29s\n",
      "85:\tlearn: 0.4848475\ttotal: 1m 49s\tremaining: 19m 28s\n",
      "86:\tlearn: 0.4844178\ttotal: 1m 51s\tremaining: 19m 30s\n",
      "87:\tlearn: 0.4841813\ttotal: 1m 52s\tremaining: 19m 28s\n",
      "88:\tlearn: 0.4839603\ttotal: 1m 54s\tremaining: 19m 27s\n",
      "89:\tlearn: 0.4837137\ttotal: 1m 55s\tremaining: 19m 25s\n",
      "90:\tlearn: 0.4834948\ttotal: 1m 56s\tremaining: 19m 22s\n",
      "91:\tlearn: 0.4832765\ttotal: 1m 57s\tremaining: 19m 20s\n",
      "92:\tlearn: 0.4829637\ttotal: 1m 58s\tremaining: 19m 17s\n",
      "93:\tlearn: 0.4827918\ttotal: 1m 59s\tremaining: 19m 14s\n",
      "94:\tlearn: 0.4825786\ttotal: 2m\tremaining: 19m 11s\n",
      "95:\tlearn: 0.4823950\ttotal: 2m 1s\tremaining: 19m 8s\n",
      "96:\tlearn: 0.4819153\ttotal: 2m 3s\tremaining: 19m 5s\n",
      "97:\tlearn: 0.4817758\ttotal: 2m 4s\tremaining: 19m 2s\n",
      "98:\tlearn: 0.4809262\ttotal: 2m 5s\tremaining: 18m 59s\n",
      "99:\tlearn: 0.4806962\ttotal: 2m 6s\tremaining: 18m 55s\n",
      "100:\tlearn: 0.4800801\ttotal: 2m 7s\tremaining: 18m 52s\n",
      "101:\tlearn: 0.4799336\ttotal: 2m 8s\tremaining: 18m 49s\n",
      "102:\tlearn: 0.4797754\ttotal: 2m 9s\tremaining: 18m 46s\n",
      "103:\tlearn: 0.4795558\ttotal: 2m 10s\tremaining: 18m 42s\n",
      "104:\tlearn: 0.4793501\ttotal: 2m 11s\tremaining: 18m 39s\n",
      "105:\tlearn: 0.4791112\ttotal: 2m 12s\tremaining: 18m 36s\n",
      "106:\tlearn: 0.4783667\ttotal: 2m 13s\tremaining: 18m 33s\n",
      "107:\tlearn: 0.4781982\ttotal: 2m 14s\tremaining: 18m 30s\n",
      "108:\tlearn: 0.4780536\ttotal: 2m 15s\tremaining: 18m 27s\n",
      "109:\tlearn: 0.4778728\ttotal: 2m 16s\tremaining: 18m 24s\n",
      "110:\tlearn: 0.4773356\ttotal: 2m 17s\tremaining: 18m 21s\n",
      "111:\tlearn: 0.4771384\ttotal: 2m 18s\tremaining: 18m 18s\n",
      "112:\tlearn: 0.4769696\ttotal: 2m 19s\tremaining: 18m 15s\n",
      "113:\tlearn: 0.4768177\ttotal: 2m 20s\tremaining: 18m 11s\n",
      "114:\tlearn: 0.4766539\ttotal: 2m 21s\tremaining: 18m 8s\n",
      "115:\tlearn: 0.4764910\ttotal: 2m 22s\tremaining: 18m 5s\n",
      "116:\tlearn: 0.4763193\ttotal: 2m 23s\tremaining: 18m 1s\n",
      "117:\tlearn: 0.4761382\ttotal: 2m 24s\tremaining: 17m 58s\n",
      "118:\tlearn: 0.4759516\ttotal: 2m 25s\tremaining: 17m 55s\n",
      "119:\tlearn: 0.4757817\ttotal: 2m 26s\tremaining: 17m 52s\n",
      "120:\tlearn: 0.4756270\ttotal: 2m 27s\tremaining: 17m 49s\n",
      "121:\tlearn: 0.4754433\ttotal: 2m 28s\tremaining: 17m 46s\n",
      "122:\tlearn: 0.4749216\ttotal: 2m 29s\tremaining: 17m 42s\n",
      "123:\tlearn: 0.4740917\ttotal: 2m 30s\tremaining: 17m 40s\n",
      "124:\tlearn: 0.4739158\ttotal: 2m 31s\tremaining: 17m 37s\n",
      "125:\tlearn: 0.4737573\ttotal: 2m 32s\tremaining: 17m 34s\n",
      "126:\tlearn: 0.4735699\ttotal: 2m 33s\tremaining: 17m 32s\n",
      "127:\tlearn: 0.4734162\ttotal: 2m 34s\tremaining: 17m 29s\n",
      "128:\tlearn: 0.4732447\ttotal: 2m 34s\tremaining: 17m 26s\n",
      "129:\tlearn: 0.4730031\ttotal: 2m 35s\tremaining: 17m 23s\n",
      "130:\tlearn: 0.4727240\ttotal: 2m 37s\tremaining: 17m 21s\n",
      "131:\tlearn: 0.4725139\ttotal: 2m 38s\tremaining: 17m 20s\n",
      "132:\tlearn: 0.4723378\ttotal: 2m 39s\tremaining: 17m 17s\n",
      "133:\tlearn: 0.4720729\ttotal: 2m 40s\tremaining: 17m 15s\n",
      "134:\tlearn: 0.4718612\ttotal: 2m 41s\tremaining: 17m 13s\n",
      "135:\tlearn: 0.4711146\ttotal: 2m 42s\tremaining: 17m 13s\n",
      "136:\tlearn: 0.4708927\ttotal: 2m 43s\tremaining: 17m 12s\n",
      "137:\tlearn: 0.4705170\ttotal: 2m 45s\tremaining: 17m 11s\n",
      "138:\tlearn: 0.4703257\ttotal: 2m 46s\tremaining: 17m 9s\n",
      "139:\tlearn: 0.4700793\ttotal: 2m 47s\tremaining: 17m 8s\n",
      "140:\tlearn: 0.4698994\ttotal: 2m 48s\tremaining: 17m 7s\n",
      "141:\tlearn: 0.4693613\ttotal: 2m 49s\tremaining: 17m 6s\n",
      "142:\tlearn: 0.4687151\ttotal: 2m 51s\tremaining: 17m 5s\n",
      "143:\tlearn: 0.4685455\ttotal: 2m 52s\tremaining: 17m 4s\n",
      "144:\tlearn: 0.4683743\ttotal: 2m 53s\tremaining: 17m 3s\n",
      "145:\tlearn: 0.4681916\ttotal: 2m 55s\tremaining: 17m 3s\n",
      "146:\tlearn: 0.4680180\ttotal: 2m 56s\tremaining: 17m 2s\n",
      "147:\tlearn: 0.4678182\ttotal: 2m 57s\tremaining: 17m 1s\n",
      "148:\tlearn: 0.4676483\ttotal: 2m 58s\tremaining: 17m\n",
      "149:\tlearn: 0.4674818\ttotal: 2m 59s\tremaining: 16m 59s\n",
      "150:\tlearn: 0.4672601\ttotal: 3m 1s\tremaining: 16m 59s\n",
      "151:\tlearn: 0.4670577\ttotal: 3m 2s\tremaining: 16m 58s\n",
      "152:\tlearn: 0.4669143\ttotal: 3m 3s\tremaining: 16m 57s\n",
      "153:\tlearn: 0.4662735\ttotal: 3m 5s\tremaining: 16m 56s\n",
      "154:\tlearn: 0.4661140\ttotal: 3m 6s\tremaining: 16m 55s\n",
      "155:\tlearn: 0.4659344\ttotal: 3m 7s\tremaining: 16m 55s\n",
      "156:\tlearn: 0.4657773\ttotal: 3m 9s\tremaining: 16m 54s\n",
      "157:\tlearn: 0.4655595\ttotal: 3m 10s\tremaining: 16m 54s\n",
      "158:\tlearn: 0.4653895\ttotal: 3m 11s\tremaining: 16m 53s\n",
      "159:\tlearn: 0.4651466\ttotal: 3m 13s\tremaining: 16m 54s\n",
      "160:\tlearn: 0.4649615\ttotal: 3m 14s\tremaining: 16m 53s\n",
      "161:\tlearn: 0.4647805\ttotal: 3m 15s\tremaining: 16m 52s\n",
      "162:\tlearn: 0.4645814\ttotal: 3m 17s\tremaining: 16m 52s\n",
      "163:\tlearn: 0.4644401\ttotal: 3m 18s\tremaining: 16m 51s\n",
      "164:\tlearn: 0.4642301\ttotal: 3m 19s\tremaining: 16m 51s\n",
      "165:\tlearn: 0.4640603\ttotal: 3m 21s\tremaining: 16m 50s\n",
      "166:\tlearn: 0.4638872\ttotal: 3m 22s\tremaining: 16m 49s\n",
      "167:\tlearn: 0.4637317\ttotal: 3m 23s\tremaining: 16m 48s\n",
      "168:\tlearn: 0.4635413\ttotal: 3m 25s\tremaining: 16m 48s\n",
      "169:\tlearn: 0.4632886\ttotal: 3m 26s\tremaining: 16m 49s\n",
      "170:\tlearn: 0.4628998\ttotal: 3m 28s\tremaining: 16m 48s\n",
      "171:\tlearn: 0.4627472\ttotal: 3m 29s\tremaining: 16m 47s\n",
      "172:\tlearn: 0.4626012\ttotal: 3m 30s\tremaining: 16m 46s\n",
      "173:\tlearn: 0.4624263\ttotal: 3m 31s\tremaining: 16m 46s\n",
      "174:\tlearn: 0.4622435\ttotal: 3m 33s\tremaining: 16m 45s\n",
      "175:\tlearn: 0.4620574\ttotal: 3m 34s\tremaining: 16m 45s\n",
      "176:\tlearn: 0.4615910\ttotal: 3m 36s\tremaining: 16m 44s\n",
      "177:\tlearn: 0.4614404\ttotal: 3m 37s\tremaining: 16m 44s\n",
      "178:\tlearn: 0.4612809\ttotal: 3m 38s\tremaining: 16m 43s\n",
      "179:\tlearn: 0.4610960\ttotal: 3m 40s\tremaining: 16m 43s\n",
      "180:\tlearn: 0.4606320\ttotal: 3m 41s\tremaining: 16m 42s\n",
      "181:\tlearn: 0.4601863\ttotal: 3m 42s\tremaining: 16m 42s\n",
      "182:\tlearn: 0.4600188\ttotal: 3m 44s\tremaining: 16m 41s\n",
      "183:\tlearn: 0.4598407\ttotal: 3m 45s\tremaining: 16m 40s\n",
      "184:\tlearn: 0.4596500\ttotal: 3m 47s\tremaining: 16m 40s\n",
      "185:\tlearn: 0.4595049\ttotal: 3m 48s\tremaining: 16m 39s\n",
      "186:\tlearn: 0.4593497\ttotal: 3m 49s\tremaining: 16m 38s\n",
      "187:\tlearn: 0.4591935\ttotal: 3m 50s\tremaining: 16m 37s\n",
      "188:\tlearn: 0.4590349\ttotal: 3m 52s\tremaining: 16m 36s\n",
      "189:\tlearn: 0.4588367\ttotal: 3m 53s\tremaining: 16m 36s\n",
      "190:\tlearn: 0.4586977\ttotal: 3m 54s\tremaining: 16m 34s\n",
      "191:\tlearn: 0.4585387\ttotal: 3m 56s\tremaining: 16m 33s\n",
      "192:\tlearn: 0.4583913\ttotal: 3m 57s\tremaining: 16m 31s\n",
      "193:\tlearn: 0.4582577\ttotal: 3m 58s\tremaining: 16m 30s\n",
      "194:\tlearn: 0.4581101\ttotal: 3m 59s\tremaining: 16m 29s\n",
      "195:\tlearn: 0.4579254\ttotal: 4m\tremaining: 16m 28s\n",
      "196:\tlearn: 0.4577540\ttotal: 4m 2s\tremaining: 16m 26s\n",
      "197:\tlearn: 0.4576118\ttotal: 4m 3s\tremaining: 16m 25s\n",
      "198:\tlearn: 0.4574089\ttotal: 4m 4s\tremaining: 16m 23s\n",
      "199:\tlearn: 0.4572472\ttotal: 4m 5s\tremaining: 16m 22s\n",
      "200:\tlearn: 0.4568734\ttotal: 4m 6s\tremaining: 16m 21s\n",
      "201:\tlearn: 0.4567261\ttotal: 4m 7s\tremaining: 16m 19s\n",
      "202:\tlearn: 0.4565912\ttotal: 4m 9s\tremaining: 16m 17s\n",
      "203:\tlearn: 0.4564284\ttotal: 4m 10s\tremaining: 16m 16s\n",
      "204:\tlearn: 0.4562690\ttotal: 4m 11s\tremaining: 16m 14s\n",
      "205:\tlearn: 0.4560866\ttotal: 4m 12s\tremaining: 16m 13s\n",
      "206:\tlearn: 0.4559565\ttotal: 4m 13s\tremaining: 16m 11s\n",
      "207:\tlearn: 0.4558375\ttotal: 4m 14s\tremaining: 16m 9s\n",
      "208:\tlearn: 0.4556609\ttotal: 4m 15s\tremaining: 16m 7s\n",
      "209:\tlearn: 0.4555302\ttotal: 4m 16s\tremaining: 16m 5s\n",
      "210:\tlearn: 0.4553762\ttotal: 4m 17s\tremaining: 16m 3s\n",
      "211:\tlearn: 0.4552156\ttotal: 4m 19s\tremaining: 16m 2s\n",
      "212:\tlearn: 0.4550776\ttotal: 4m 20s\tremaining: 16m\n",
      "213:\tlearn: 0.4548902\ttotal: 4m 21s\tremaining: 15m 59s\n",
      "214:\tlearn: 0.4547605\ttotal: 4m 22s\tremaining: 15m 58s\n",
      "215:\tlearn: 0.4546066\ttotal: 4m 23s\tremaining: 15m 56s\n",
      "216:\tlearn: 0.4541812\ttotal: 4m 24s\tremaining: 15m 55s\n",
      "217:\tlearn: 0.4540601\ttotal: 4m 25s\tremaining: 15m 53s\n",
      "218:\tlearn: 0.4539051\ttotal: 4m 26s\tremaining: 15m 51s\n",
      "219:\tlearn: 0.4537602\ttotal: 4m 27s\tremaining: 15m 49s\n",
      "220:\tlearn: 0.4535730\ttotal: 4m 29s\tremaining: 15m 49s\n",
      "221:\tlearn: 0.4534190\ttotal: 4m 30s\tremaining: 15m 47s\n",
      "222:\tlearn: 0.4532671\ttotal: 4m 31s\tremaining: 15m 46s\n",
      "223:\tlearn: 0.4531400\ttotal: 4m 32s\tremaining: 15m 44s\n",
      "224:\tlearn: 0.4526772\ttotal: 4m 34s\tremaining: 15m 43s\n",
      "225:\tlearn: 0.4525434\ttotal: 4m 35s\tremaining: 15m 42s\n",
      "226:\tlearn: 0.4524176\ttotal: 4m 36s\tremaining: 15m 41s\n",
      "227:\tlearn: 0.4522586\ttotal: 4m 37s\tremaining: 15m 39s\n",
      "228:\tlearn: 0.4520058\ttotal: 4m 39s\tremaining: 15m 39s\n",
      "229:\tlearn: 0.4518825\ttotal: 4m 40s\tremaining: 15m 37s\n",
      "230:\tlearn: 0.4517366\ttotal: 4m 41s\tremaining: 15m 36s\n",
      "231:\tlearn: 0.4516230\ttotal: 4m 42s\tremaining: 15m 35s\n",
      "232:\tlearn: 0.4514880\ttotal: 4m 43s\tremaining: 15m 33s\n",
      "233:\tlearn: 0.4513754\ttotal: 4m 44s\tremaining: 15m 32s\n",
      "234:\tlearn: 0.4512186\ttotal: 4m 46s\tremaining: 15m 31s\n",
      "235:\tlearn: 0.4511076\ttotal: 4m 47s\tremaining: 15m 30s\n",
      "236:\tlearn: 0.4509849\ttotal: 4m 48s\tremaining: 15m 28s\n",
      "237:\tlearn: 0.4508496\ttotal: 4m 49s\tremaining: 15m 27s\n",
      "238:\tlearn: 0.4507204\ttotal: 4m 50s\tremaining: 15m 26s\n",
      "239:\tlearn: 0.4505710\ttotal: 4m 52s\tremaining: 15m 25s\n",
      "240:\tlearn: 0.4504011\ttotal: 4m 53s\tremaining: 15m 24s\n",
      "241:\tlearn: 0.4502800\ttotal: 4m 54s\tremaining: 15m 23s\n",
      "242:\tlearn: 0.4498457\ttotal: 4m 56s\tremaining: 15m 22s\n",
      "243:\tlearn: 0.4497298\ttotal: 4m 57s\tremaining: 15m 20s\n",
      "244:\tlearn: 0.4495979\ttotal: 4m 58s\tremaining: 15m 20s\n",
      "245:\tlearn: 0.4494740\ttotal: 4m 59s\tremaining: 15m 19s\n",
      "246:\tlearn: 0.4493499\ttotal: 5m 1s\tremaining: 15m 18s\n",
      "247:\tlearn: 0.4492223\ttotal: 5m 2s\tremaining: 15m 16s\n",
      "248:\tlearn: 0.4491021\ttotal: 5m 3s\tremaining: 15m 15s\n",
      "249:\tlearn: 0.4489660\ttotal: 5m 4s\tremaining: 15m 14s\n",
      "250:\tlearn: 0.4488221\ttotal: 5m 6s\tremaining: 15m 13s\n",
      "251:\tlearn: 0.4486936\ttotal: 5m 7s\tremaining: 15m 12s\n",
      "252:\tlearn: 0.4485650\ttotal: 5m 8s\tremaining: 15m 11s\n",
      "253:\tlearn: 0.4484103\ttotal: 5m 10s\tremaining: 15m 10s\n",
      "254:\tlearn: 0.4482770\ttotal: 5m 11s\tremaining: 15m 9s\n",
      "255:\tlearn: 0.4480869\ttotal: 5m 12s\tremaining: 15m 9s\n",
      "256:\tlearn: 0.4479760\ttotal: 5m 14s\tremaining: 15m 8s\n",
      "257:\tlearn: 0.4478655\ttotal: 5m 15s\tremaining: 15m 7s\n",
      "258:\tlearn: 0.4477450\ttotal: 5m 16s\tremaining: 15m 6s\n",
      "259:\tlearn: 0.4476187\ttotal: 5m 18s\tremaining: 15m 6s\n",
      "260:\tlearn: 0.4475086\ttotal: 5m 19s\tremaining: 15m 5s\n",
      "261:\tlearn: 0.4473701\ttotal: 5m 21s\tremaining: 15m 4s\n",
      "262:\tlearn: 0.4472544\ttotal: 5m 22s\tremaining: 15m 3s\n",
      "263:\tlearn: 0.4470675\ttotal: 5m 23s\tremaining: 15m 2s\n",
      "264:\tlearn: 0.4469270\ttotal: 5m 25s\tremaining: 15m 2s\n",
      "265:\tlearn: 0.4466136\ttotal: 5m 26s\tremaining: 15m 1s\n",
      "266:\tlearn: 0.4464915\ttotal: 5m 28s\tremaining: 15m\n",
      "267:\tlearn: 0.4463728\ttotal: 5m 29s\tremaining: 14m 59s\n",
      "268:\tlearn: 0.4462320\ttotal: 5m 31s\tremaining: 14m 59s\n",
      "269:\tlearn: 0.4461181\ttotal: 5m 32s\tremaining: 14m 58s\n",
      "270:\tlearn: 0.4460010\ttotal: 5m 33s\tremaining: 14m 58s\n",
      "271:\tlearn: 0.4458950\ttotal: 5m 35s\tremaining: 14m 57s\n",
      "272:\tlearn: 0.4457489\ttotal: 5m 36s\tremaining: 14m 56s\n",
      "273:\tlearn: 0.4456384\ttotal: 5m 38s\tremaining: 14m 56s\n",
      "274:\tlearn: 0.4455149\ttotal: 5m 39s\tremaining: 14m 55s\n",
      "275:\tlearn: 0.4453904\ttotal: 5m 41s\tremaining: 14m 54s\n",
      "276:\tlearn: 0.4452710\ttotal: 5m 42s\tremaining: 14m 53s\n",
      "277:\tlearn: 0.4451634\ttotal: 5m 43s\tremaining: 14m 53s\n",
      "278:\tlearn: 0.4449974\ttotal: 5m 45s\tremaining: 14m 53s\n",
      "279:\tlearn: 0.4448352\ttotal: 5m 47s\tremaining: 14m 52s\n",
      "280:\tlearn: 0.4447061\ttotal: 5m 48s\tremaining: 14m 51s\n",
      "281:\tlearn: 0.4445584\ttotal: 5m 49s\tremaining: 14m 50s\n",
      "282:\tlearn: 0.4441859\ttotal: 5m 51s\tremaining: 14m 50s\n",
      "283:\tlearn: 0.4440815\ttotal: 5m 52s\tremaining: 14m 49s\n",
      "284:\tlearn: 0.4439830\ttotal: 5m 54s\tremaining: 14m 48s\n",
      "285:\tlearn: 0.4438771\ttotal: 5m 55s\tremaining: 14m 47s\n",
      "286:\tlearn: 0.4437055\ttotal: 5m 56s\tremaining: 14m 46s\n",
      "287:\tlearn: 0.4435880\ttotal: 5m 57s\tremaining: 14m 44s\n",
      "288:\tlearn: 0.4434774\ttotal: 5m 59s\tremaining: 14m 43s\n",
      "289:\tlearn: 0.4433779\ttotal: 6m\tremaining: 14m 42s\n",
      "290:\tlearn: 0.4432687\ttotal: 6m 1s\tremaining: 14m 40s\n",
      "291:\tlearn: 0.4431684\ttotal: 6m 2s\tremaining: 14m 39s\n",
      "292:\tlearn: 0.4430658\ttotal: 6m 3s\tremaining: 14m 37s\n",
      "293:\tlearn: 0.4429161\ttotal: 6m 5s\tremaining: 14m 36s\n",
      "294:\tlearn: 0.4428244\ttotal: 6m 6s\tremaining: 14m 35s\n",
      "295:\tlearn: 0.4427163\ttotal: 6m 7s\tremaining: 14m 33s\n",
      "296:\tlearn: 0.4426039\ttotal: 6m 8s\tremaining: 14m 32s\n",
      "297:\tlearn: 0.4424873\ttotal: 6m 9s\tremaining: 14m 30s\n",
      "298:\tlearn: 0.4423187\ttotal: 6m 10s\tremaining: 14m 29s\n",
      "299:\tlearn: 0.4422263\ttotal: 6m 11s\tremaining: 14m 27s\n",
      "300:\tlearn: 0.4421257\ttotal: 6m 12s\tremaining: 14m 25s\n",
      "301:\tlearn: 0.4420251\ttotal: 6m 13s\tremaining: 14m 24s\n",
      "302:\tlearn: 0.4419044\ttotal: 6m 14s\tremaining: 14m 22s\n",
      "303:\tlearn: 0.4414362\ttotal: 6m 16s\tremaining: 14m 21s\n",
      "304:\tlearn: 0.4413346\ttotal: 6m 17s\tremaining: 14m 19s\n",
      "305:\tlearn: 0.4412400\ttotal: 6m 18s\tremaining: 14m 17s\n",
      "306:\tlearn: 0.4411306\ttotal: 6m 19s\tremaining: 14m 16s\n",
      "307:\tlearn: 0.4410259\ttotal: 6m 20s\tremaining: 14m 14s\n",
      "308:\tlearn: 0.4409229\ttotal: 6m 21s\tremaining: 14m 13s\n",
      "309:\tlearn: 0.4408240\ttotal: 6m 22s\tremaining: 14m 11s\n",
      "310:\tlearn: 0.4407061\ttotal: 6m 23s\tremaining: 14m 10s\n",
      "311:\tlearn: 0.4405930\ttotal: 6m 24s\tremaining: 14m 8s\n",
      "312:\tlearn: 0.4405008\ttotal: 6m 25s\tremaining: 14m 6s\n",
      "313:\tlearn: 0.4403984\ttotal: 6m 26s\tremaining: 14m 4s\n",
      "314:\tlearn: 0.4402755\ttotal: 6m 27s\tremaining: 14m 3s\n",
      "315:\tlearn: 0.4401545\ttotal: 6m 29s\tremaining: 14m 2s\n",
      "316:\tlearn: 0.4400483\ttotal: 6m 30s\tremaining: 14m\n",
      "317:\tlearn: 0.4399473\ttotal: 6m 31s\tremaining: 13m 58s\n",
      "318:\tlearn: 0.4398496\ttotal: 6m 32s\tremaining: 13m 57s\n",
      "319:\tlearn: 0.4397178\ttotal: 6m 33s\tremaining: 13m 55s\n",
      "320:\tlearn: 0.4396063\ttotal: 6m 34s\tremaining: 13m 54s\n",
      "321:\tlearn: 0.4395022\ttotal: 6m 35s\tremaining: 13m 52s\n",
      "322:\tlearn: 0.4393883\ttotal: 6m 36s\tremaining: 13m 51s\n",
      "323:\tlearn: 0.4392802\ttotal: 6m 37s\tremaining: 13m 49s\n",
      "324:\tlearn: 0.4391579\ttotal: 6m 38s\tremaining: 13m 48s\n",
      "325:\tlearn: 0.4390479\ttotal: 6m 39s\tremaining: 13m 46s\n",
      "326:\tlearn: 0.4389474\ttotal: 6m 40s\tremaining: 13m 44s\n",
      "327:\tlearn: 0.4388582\ttotal: 6m 41s\tremaining: 13m 43s\n",
      "328:\tlearn: 0.4387489\ttotal: 6m 42s\tremaining: 13m 41s\n",
      "329:\tlearn: 0.4386613\ttotal: 6m 44s\tremaining: 13m 40s\n",
      "330:\tlearn: 0.4385570\ttotal: 6m 45s\tremaining: 13m 38s\n",
      "331:\tlearn: 0.4384635\ttotal: 6m 46s\tremaining: 13m 37s\n",
      "332:\tlearn: 0.4383222\ttotal: 6m 47s\tremaining: 13m 35s\n",
      "333:\tlearn: 0.4382051\ttotal: 6m 48s\tremaining: 13m 34s\n",
      "334:\tlearn: 0.4381132\ttotal: 6m 49s\tremaining: 13m 33s\n",
      "335:\tlearn: 0.4379790\ttotal: 6m 51s\tremaining: 13m 32s\n",
      "336:\tlearn: 0.4378567\ttotal: 6m 52s\tremaining: 13m 30s\n",
      "337:\tlearn: 0.4377365\ttotal: 6m 53s\tremaining: 13m 29s\n",
      "338:\tlearn: 0.4376185\ttotal: 6m 54s\tremaining: 13m 28s\n",
      "339:\tlearn: 0.4375029\ttotal: 6m 56s\tremaining: 13m 27s\n",
      "340:\tlearn: 0.4373790\ttotal: 6m 57s\tremaining: 13m 26s\n",
      "341:\tlearn: 0.4372622\ttotal: 6m 58s\tremaining: 13m 25s\n",
      "342:\tlearn: 0.4371579\ttotal: 7m\tremaining: 13m 24s\n",
      "343:\tlearn: 0.4370532\ttotal: 7m 1s\tremaining: 13m 23s\n",
      "344:\tlearn: 0.4369561\ttotal: 7m 2s\tremaining: 13m 22s\n",
      "345:\tlearn: 0.4368505\ttotal: 7m 4s\tremaining: 13m 21s\n",
      "346:\tlearn: 0.4367451\ttotal: 7m 5s\tremaining: 13m 20s\n",
      "347:\tlearn: 0.4366512\ttotal: 7m 6s\tremaining: 13m 19s\n",
      "348:\tlearn: 0.4365561\ttotal: 7m 7s\tremaining: 13m 18s\n",
      "349:\tlearn: 0.4364456\ttotal: 7m 9s\tremaining: 13m 17s\n",
      "350:\tlearn: 0.4363303\ttotal: 7m 10s\tremaining: 13m 16s\n",
      "351:\tlearn: 0.4362317\ttotal: 7m 11s\tremaining: 13m 15s\n",
      "352:\tlearn: 0.4361395\ttotal: 7m 13s\tremaining: 13m 14s\n",
      "353:\tlearn: 0.4360297\ttotal: 7m 15s\tremaining: 13m 13s\n",
      "354:\tlearn: 0.4359324\ttotal: 7m 16s\tremaining: 13m 12s\n",
      "355:\tlearn: 0.4358184\ttotal: 7m 17s\tremaining: 13m 11s\n",
      "356:\tlearn: 0.4357055\ttotal: 7m 19s\tremaining: 13m 11s\n",
      "357:\tlearn: 0.4355886\ttotal: 7m 20s\tremaining: 13m 10s\n",
      "358:\tlearn: 0.4354909\ttotal: 7m 22s\tremaining: 13m 9s\n",
      "359:\tlearn: 0.4353932\ttotal: 7m 23s\tremaining: 13m 8s\n",
      "360:\tlearn: 0.4352123\ttotal: 7m 25s\tremaining: 13m 8s\n",
      "361:\tlearn: 0.4350858\ttotal: 7m 26s\tremaining: 13m 7s\n",
      "362:\tlearn: 0.4349705\ttotal: 7m 28s\tremaining: 13m 6s\n",
      "363:\tlearn: 0.4348626\ttotal: 7m 29s\tremaining: 13m 5s\n",
      "364:\tlearn: 0.4347732\ttotal: 7m 31s\tremaining: 13m 4s\n",
      "365:\tlearn: 0.4346499\ttotal: 7m 32s\tremaining: 13m 4s\n",
      "366:\tlearn: 0.4345541\ttotal: 7m 34s\tremaining: 13m 3s\n",
      "367:\tlearn: 0.4344598\ttotal: 7m 35s\tremaining: 13m 2s\n",
      "368:\tlearn: 0.4343371\ttotal: 7m 37s\tremaining: 13m 1s\n",
      "369:\tlearn: 0.4342398\ttotal: 7m 38s\tremaining: 13m\n",
      "370:\tlearn: 0.4341002\ttotal: 7m 39s\tremaining: 12m 59s\n",
      "371:\tlearn: 0.4339859\ttotal: 7m 41s\tremaining: 12m 58s\n",
      "372:\tlearn: 0.4338677\ttotal: 7m 42s\tremaining: 12m 57s\n",
      "373:\tlearn: 0.4337746\ttotal: 7m 44s\tremaining: 12m 56s\n",
      "374:\tlearn: 0.4336555\ttotal: 7m 45s\tremaining: 12m 55s\n",
      "375:\tlearn: 0.4335540\ttotal: 7m 46s\tremaining: 12m 54s\n",
      "376:\tlearn: 0.4334595\ttotal: 7m 48s\tremaining: 12m 53s\n",
      "377:\tlearn: 0.4333518\ttotal: 7m 49s\tremaining: 12m 52s\n",
      "378:\tlearn: 0.4332109\ttotal: 7m 50s\tremaining: 12m 51s\n",
      "379:\tlearn: 0.4330983\ttotal: 7m 51s\tremaining: 12m 49s\n",
      "380:\tlearn: 0.4330188\ttotal: 7m 53s\tremaining: 12m 48s\n",
      "381:\tlearn: 0.4329455\ttotal: 7m 54s\tremaining: 12m 47s\n",
      "382:\tlearn: 0.4328476\ttotal: 7m 55s\tremaining: 12m 45s\n",
      "383:\tlearn: 0.4327486\ttotal: 7m 56s\tremaining: 12m 44s\n",
      "384:\tlearn: 0.4326546\ttotal: 7m 57s\tremaining: 12m 42s\n",
      "385:\tlearn: 0.4325661\ttotal: 7m 58s\tremaining: 12m 41s\n",
      "386:\tlearn: 0.4322319\ttotal: 8m\tremaining: 12m 40s\n",
      "387:\tlearn: 0.4321376\ttotal: 8m 1s\tremaining: 12m 38s\n",
      "388:\tlearn: 0.4320510\ttotal: 8m 2s\tremaining: 12m 37s\n",
      "389:\tlearn: 0.4319571\ttotal: 8m 3s\tremaining: 12m 35s\n",
      "390:\tlearn: 0.4318306\ttotal: 8m 4s\tremaining: 12m 34s\n",
      "391:\tlearn: 0.4317127\ttotal: 8m 5s\tremaining: 12m 33s\n",
      "392:\tlearn: 0.4316014\ttotal: 8m 6s\tremaining: 12m 31s\n",
      "393:\tlearn: 0.4315165\ttotal: 8m 7s\tremaining: 12m 30s\n",
      "394:\tlearn: 0.4314044\ttotal: 8m 8s\tremaining: 12m 28s\n",
      "395:\tlearn: 0.4312962\ttotal: 8m 10s\tremaining: 12m 27s\n",
      "396:\tlearn: 0.4312026\ttotal: 8m 11s\tremaining: 12m 26s\n",
      "397:\tlearn: 0.4310390\ttotal: 8m 12s\tremaining: 12m 24s\n",
      "398:\tlearn: 0.4309445\ttotal: 8m 13s\tremaining: 12m 23s\n",
      "399:\tlearn: 0.4308497\ttotal: 8m 14s\tremaining: 12m 21s\n",
      "400:\tlearn: 0.4307366\ttotal: 8m 15s\tremaining: 12m 20s\n",
      "401:\tlearn: 0.4306306\ttotal: 8m 17s\tremaining: 12m 19s\n",
      "402:\tlearn: 0.4305354\ttotal: 8m 18s\tremaining: 12m 17s\n",
      "403:\tlearn: 0.4304810\ttotal: 8m 19s\tremaining: 12m 16s\n",
      "404:\tlearn: 0.4303942\ttotal: 8m 20s\tremaining: 12m 14s\n",
      "405:\tlearn: 0.4302707\ttotal: 8m 21s\tremaining: 12m 13s\n",
      "406:\tlearn: 0.4301588\ttotal: 8m 22s\tremaining: 12m 12s\n",
      "407:\tlearn: 0.4300374\ttotal: 8m 23s\tremaining: 12m 10s\n",
      "408:\tlearn: 0.4299527\ttotal: 8m 24s\tremaining: 12m 9s\n",
      "409:\tlearn: 0.4298398\ttotal: 8m 25s\tremaining: 12m 7s\n",
      "410:\tlearn: 0.4296783\ttotal: 8m 27s\tremaining: 12m 6s\n",
      "411:\tlearn: 0.4295717\ttotal: 8m 28s\tremaining: 12m 5s\n",
      "412:\tlearn: 0.4294784\ttotal: 8m 29s\tremaining: 12m 3s\n",
      "413:\tlearn: 0.4294022\ttotal: 8m 30s\tremaining: 12m 2s\n",
      "414:\tlearn: 0.4293092\ttotal: 8m 31s\tremaining: 12m\n",
      "415:\tlearn: 0.4292120\ttotal: 8m 32s\tremaining: 11m 59s\n",
      "416:\tlearn: 0.4291244\ttotal: 8m 33s\tremaining: 11m 58s\n",
      "417:\tlearn: 0.4290166\ttotal: 8m 34s\tremaining: 11m 57s\n",
      "418:\tlearn: 0.4289516\ttotal: 8m 35s\tremaining: 11m 55s\n",
      "419:\tlearn: 0.4288434\ttotal: 8m 37s\tremaining: 11m 54s\n",
      "420:\tlearn: 0.4287527\ttotal: 8m 38s\tremaining: 11m 52s\n",
      "421:\tlearn: 0.4286596\ttotal: 8m 39s\tremaining: 11m 51s\n",
      "422:\tlearn: 0.4285897\ttotal: 8m 40s\tremaining: 11m 50s\n",
      "423:\tlearn: 0.4285055\ttotal: 8m 41s\tremaining: 11m 49s\n",
      "424:\tlearn: 0.4284136\ttotal: 8m 43s\tremaining: 11m 47s\n",
      "425:\tlearn: 0.4283154\ttotal: 8m 44s\tremaining: 11m 46s\n",
      "426:\tlearn: 0.4282200\ttotal: 8m 45s\tremaining: 11m 45s\n",
      "427:\tlearn: 0.4281293\ttotal: 8m 46s\tremaining: 11m 44s\n",
      "428:\tlearn: 0.4279957\ttotal: 8m 48s\tremaining: 11m 43s\n",
      "429:\tlearn: 0.4279175\ttotal: 8m 49s\tremaining: 11m 41s\n",
      "430:\tlearn: 0.4277923\ttotal: 8m 51s\tremaining: 11m 41s\n",
      "431:\tlearn: 0.4276732\ttotal: 8m 52s\tremaining: 11m 39s\n",
      "432:\tlearn: 0.4275864\ttotal: 8m 53s\tremaining: 11m 38s\n",
      "433:\tlearn: 0.4274943\ttotal: 8m 54s\tremaining: 11m 37s\n",
      "434:\tlearn: 0.4271550\ttotal: 8m 56s\tremaining: 11m 36s\n",
      "435:\tlearn: 0.4270675\ttotal: 8m 57s\tremaining: 11m 35s\n",
      "436:\tlearn: 0.4269581\ttotal: 8m 59s\tremaining: 11m 34s\n",
      "437:\tlearn: 0.4268264\ttotal: 9m\tremaining: 11m 33s\n",
      "438:\tlearn: 0.4267344\ttotal: 9m 1s\tremaining: 11m 32s\n",
      "439:\tlearn: 0.4266526\ttotal: 9m 3s\tremaining: 11m 31s\n",
      "440:\tlearn: 0.4265565\ttotal: 9m 4s\tremaining: 11m 30s\n",
      "441:\tlearn: 0.4264386\ttotal: 9m 5s\tremaining: 11m 29s\n",
      "442:\tlearn: 0.4263505\ttotal: 9m 7s\tremaining: 11m 28s\n",
      "443:\tlearn: 0.4262477\ttotal: 9m 8s\tremaining: 11m 26s\n",
      "444:\tlearn: 0.4261782\ttotal: 9m 9s\tremaining: 11m 25s\n",
      "445:\tlearn: 0.4260798\ttotal: 9m 11s\tremaining: 11m 24s\n",
      "446:\tlearn: 0.4260017\ttotal: 9m 12s\tremaining: 11m 23s\n",
      "447:\tlearn: 0.4258688\ttotal: 9m 13s\tremaining: 11m 22s\n",
      "448:\tlearn: 0.4257920\ttotal: 9m 15s\tremaining: 11m 21s\n",
      "449:\tlearn: 0.4256891\ttotal: 9m 16s\tremaining: 11m 20s\n",
      "450:\tlearn: 0.4255841\ttotal: 9m 17s\tremaining: 11m 19s\n",
      "451:\tlearn: 0.4254785\ttotal: 9m 19s\tremaining: 11m 18s\n",
      "452:\tlearn: 0.4253819\ttotal: 9m 20s\tremaining: 11m 16s\n",
      "453:\tlearn: 0.4253178\ttotal: 9m 21s\tremaining: 11m 15s\n",
      "454:\tlearn: 0.4252341\ttotal: 9m 23s\tremaining: 11m 14s\n",
      "455:\tlearn: 0.4251339\ttotal: 9m 24s\tremaining: 11m 13s\n",
      "456:\tlearn: 0.4250380\ttotal: 9m 25s\tremaining: 11m 12s\n",
      "457:\tlearn: 0.4249453\ttotal: 9m 27s\tremaining: 11m 11s\n",
      "458:\tlearn: 0.4248250\ttotal: 9m 28s\tremaining: 11m 10s\n",
      "459:\tlearn: 0.4247313\ttotal: 9m 30s\tremaining: 11m 9s\n",
      "460:\tlearn: 0.4246631\ttotal: 9m 31s\tremaining: 11m 7s\n",
      "461:\tlearn: 0.4245769\ttotal: 9m 32s\tremaining: 11m 6s\n",
      "462:\tlearn: 0.4244980\ttotal: 9m 33s\tremaining: 11m 5s\n",
      "463:\tlearn: 0.4243852\ttotal: 9m 35s\tremaining: 11m 4s\n",
      "464:\tlearn: 0.4242758\ttotal: 9m 36s\tremaining: 11m 3s\n",
      "465:\tlearn: 0.4241819\ttotal: 9m 37s\tremaining: 11m 2s\n",
      "466:\tlearn: 0.4240929\ttotal: 9m 39s\tremaining: 11m 1s\n",
      "467:\tlearn: 0.4239983\ttotal: 9m 40s\tremaining: 10m 59s\n",
      "468:\tlearn: 0.4239155\ttotal: 9m 41s\tremaining: 10m 58s\n",
      "469:\tlearn: 0.4238368\ttotal: 9m 43s\tremaining: 10m 57s\n",
      "470:\tlearn: 0.4237482\ttotal: 9m 44s\tremaining: 10m 56s\n",
      "471:\tlearn: 0.4236441\ttotal: 9m 45s\tremaining: 10m 55s\n",
      "472:\tlearn: 0.4235730\ttotal: 9m 46s\tremaining: 10m 53s\n",
      "473:\tlearn: 0.4234879\ttotal: 9m 48s\tremaining: 10m 52s\n",
      "474:\tlearn: 0.4233904\ttotal: 9m 49s\tremaining: 10m 51s\n",
      "475:\tlearn: 0.4230732\ttotal: 9m 50s\tremaining: 10m 50s\n",
      "476:\tlearn: 0.4230171\ttotal: 9m 52s\tremaining: 10m 49s\n",
      "477:\tlearn: 0.4229367\ttotal: 9m 53s\tremaining: 10m 47s\n",
      "478:\tlearn: 0.4228404\ttotal: 9m 54s\tremaining: 10m 46s\n",
      "479:\tlearn: 0.4227613\ttotal: 9m 55s\tremaining: 10m 45s\n",
      "480:\tlearn: 0.4226506\ttotal: 9m 56s\tremaining: 10m 43s\n",
      "481:\tlearn: 0.4225693\ttotal: 9m 57s\tremaining: 10m 42s\n",
      "482:\tlearn: 0.4224978\ttotal: 9m 59s\tremaining: 10m 41s\n",
      "483:\tlearn: 0.4224033\ttotal: 10m\tremaining: 10m 39s\n",
      "484:\tlearn: 0.4223246\ttotal: 10m 1s\tremaining: 10m 38s\n",
      "485:\tlearn: 0.4222063\ttotal: 10m 2s\tremaining: 10m 37s\n",
      "486:\tlearn: 0.4221032\ttotal: 10m 3s\tremaining: 10m 36s\n",
      "487:\tlearn: 0.4219933\ttotal: 10m 5s\tremaining: 10m 34s\n",
      "488:\tlearn: 0.4219225\ttotal: 10m 6s\tremaining: 10m 33s\n",
      "489:\tlearn: 0.4218479\ttotal: 10m 7s\tremaining: 10m 32s\n",
      "490:\tlearn: 0.4217522\ttotal: 10m 8s\tremaining: 10m 30s\n",
      "491:\tlearn: 0.4216739\ttotal: 10m 9s\tremaining: 10m 29s\n",
      "492:\tlearn: 0.4215885\ttotal: 10m 10s\tremaining: 10m 28s\n",
      "493:\tlearn: 0.4215111\ttotal: 10m 11s\tremaining: 10m 26s\n",
      "494:\tlearn: 0.4214281\ttotal: 10m 12s\tremaining: 10m 25s\n",
      "495:\tlearn: 0.4213392\ttotal: 10m 14s\tremaining: 10m 23s\n",
      "496:\tlearn: 0.4212619\ttotal: 10m 15s\tremaining: 10m 22s\n",
      "497:\tlearn: 0.4211450\ttotal: 10m 16s\tremaining: 10m 21s\n",
      "498:\tlearn: 0.4210968\ttotal: 10m 17s\tremaining: 10m 19s\n",
      "499:\tlearn: 0.4209923\ttotal: 10m 18s\tremaining: 10m 18s\n",
      "500:\tlearn: 0.4209035\ttotal: 10m 19s\tremaining: 10m 17s\n",
      "501:\tlearn: 0.4208268\ttotal: 10m 20s\tremaining: 10m 15s\n",
      "502:\tlearn: 0.4207174\ttotal: 10m 22s\tremaining: 10m 14s\n",
      "503:\tlearn: 0.4206407\ttotal: 10m 23s\tremaining: 10m 13s\n",
      "504:\tlearn: 0.4205436\ttotal: 10m 24s\tremaining: 10m 12s\n",
      "505:\tlearn: 0.4204568\ttotal: 10m 25s\tremaining: 10m 10s\n",
      "506:\tlearn: 0.4203585\ttotal: 10m 26s\tremaining: 10m 9s\n",
      "507:\tlearn: 0.4202703\ttotal: 10m 27s\tremaining: 10m 8s\n",
      "508:\tlearn: 0.4201764\ttotal: 10m 29s\tremaining: 10m 6s\n",
      "509:\tlearn: 0.4201080\ttotal: 10m 30s\tremaining: 10m 5s\n",
      "510:\tlearn: 0.4200027\ttotal: 10m 31s\tremaining: 10m 4s\n",
      "511:\tlearn: 0.4198957\ttotal: 10m 32s\tremaining: 10m 2s\n",
      "512:\tlearn: 0.4197998\ttotal: 10m 33s\tremaining: 10m 1s\n",
      "513:\tlearn: 0.4197134\ttotal: 10m 34s\tremaining: 10m\n",
      "514:\tlearn: 0.4196214\ttotal: 10m 35s\tremaining: 9m 58s\n",
      "515:\tlearn: 0.4195204\ttotal: 10m 37s\tremaining: 9m 57s\n",
      "516:\tlearn: 0.4194448\ttotal: 10m 38s\tremaining: 9m 56s\n",
      "517:\tlearn: 0.4193679\ttotal: 10m 39s\tremaining: 9m 54s\n",
      "518:\tlearn: 0.4192811\ttotal: 10m 40s\tremaining: 9m 53s\n",
      "519:\tlearn: 0.4192084\ttotal: 10m 41s\tremaining: 9m 52s\n",
      "520:\tlearn: 0.4191373\ttotal: 10m 42s\tremaining: 9m 50s\n",
      "521:\tlearn: 0.4190363\ttotal: 10m 44s\tremaining: 9m 49s\n",
      "522:\tlearn: 0.4189395\ttotal: 10m 45s\tremaining: 9m 48s\n",
      "523:\tlearn: 0.4188545\ttotal: 10m 46s\tremaining: 9m 47s\n",
      "524:\tlearn: 0.4187780\ttotal: 10m 47s\tremaining: 9m 46s\n",
      "525:\tlearn: 0.4187012\ttotal: 10m 48s\tremaining: 9m 44s\n",
      "526:\tlearn: 0.4186402\ttotal: 10m 50s\tremaining: 9m 43s\n",
      "527:\tlearn: 0.4185442\ttotal: 10m 51s\tremaining: 9m 42s\n",
      "528:\tlearn: 0.4184840\ttotal: 10m 52s\tremaining: 9m 41s\n",
      "529:\tlearn: 0.4184071\ttotal: 10m 53s\tremaining: 9m 39s\n",
      "530:\tlearn: 0.4183362\ttotal: 10m 55s\tremaining: 9m 38s\n",
      "531:\tlearn: 0.4182657\ttotal: 10m 56s\tremaining: 9m 37s\n",
      "532:\tlearn: 0.4181866\ttotal: 10m 57s\tremaining: 9m 36s\n",
      "533:\tlearn: 0.4180938\ttotal: 10m 59s\tremaining: 9m 35s\n",
      "534:\tlearn: 0.4180216\ttotal: 11m\tremaining: 9m 34s\n",
      "535:\tlearn: 0.4178691\ttotal: 11m 2s\tremaining: 9m 33s\n",
      "536:\tlearn: 0.4178053\ttotal: 11m 3s\tremaining: 9m 31s\n",
      "537:\tlearn: 0.4177157\ttotal: 11m 4s\tremaining: 9m 30s\n",
      "538:\tlearn: 0.4176372\ttotal: 11m 5s\tremaining: 9m 29s\n",
      "539:\tlearn: 0.4175636\ttotal: 11m 7s\tremaining: 9m 28s\n",
      "540:\tlearn: 0.4174728\ttotal: 11m 8s\tremaining: 9m 27s\n",
      "541:\tlearn: 0.4174259\ttotal: 11m 9s\tremaining: 9m 26s\n",
      "542:\tlearn: 0.4173497\ttotal: 11m 11s\tremaining: 9m 24s\n",
      "543:\tlearn: 0.4172603\ttotal: 11m 12s\tremaining: 9m 23s\n",
      "544:\tlearn: 0.4171908\ttotal: 11m 13s\tremaining: 9m 22s\n",
      "545:\tlearn: 0.4171106\ttotal: 11m 15s\tremaining: 9m 21s\n",
      "546:\tlearn: 0.4170261\ttotal: 11m 16s\tremaining: 9m 20s\n",
      "547:\tlearn: 0.4169622\ttotal: 11m 17s\tremaining: 9m 18s\n",
      "548:\tlearn: 0.4168947\ttotal: 11m 18s\tremaining: 9m 17s\n",
      "549:\tlearn: 0.4168241\ttotal: 11m 20s\tremaining: 9m 16s\n",
      "550:\tlearn: 0.4167371\ttotal: 11m 21s\tremaining: 9m 15s\n",
      "551:\tlearn: 0.4166632\ttotal: 11m 22s\tremaining: 9m 14s\n",
      "552:\tlearn: 0.4165877\ttotal: 11m 23s\tremaining: 9m 12s\n",
      "553:\tlearn: 0.4164561\ttotal: 11m 25s\tremaining: 9m 11s\n",
      "554:\tlearn: 0.4164025\ttotal: 11m 26s\tremaining: 9m 10s\n",
      "555:\tlearn: 0.4163222\ttotal: 11m 27s\tremaining: 9m 9s\n",
      "556:\tlearn: 0.4162193\ttotal: 11m 28s\tremaining: 9m 7s\n",
      "557:\tlearn: 0.4161438\ttotal: 11m 29s\tremaining: 9m 6s\n",
      "558:\tlearn: 0.4160679\ttotal: 11m 31s\tremaining: 9m 5s\n",
      "559:\tlearn: 0.4159852\ttotal: 11m 32s\tremaining: 9m 4s\n",
      "560:\tlearn: 0.4159188\ttotal: 11m 33s\tremaining: 9m 2s\n",
      "561:\tlearn: 0.4158503\ttotal: 11m 34s\tremaining: 9m 1s\n",
      "562:\tlearn: 0.4157667\ttotal: 11m 35s\tremaining: 9m\n",
      "563:\tlearn: 0.4156832\ttotal: 11m 36s\tremaining: 8m 58s\n",
      "564:\tlearn: 0.4153208\ttotal: 11m 38s\tremaining: 8m 57s\n",
      "565:\tlearn: 0.4152520\ttotal: 11m 39s\tremaining: 8m 56s\n",
      "566:\tlearn: 0.4151629\ttotal: 11m 40s\tremaining: 8m 54s\n",
      "567:\tlearn: 0.4150929\ttotal: 11m 41s\tremaining: 8m 53s\n",
      "568:\tlearn: 0.4150193\ttotal: 11m 42s\tremaining: 8m 52s\n",
      "569:\tlearn: 0.4149487\ttotal: 11m 44s\tremaining: 8m 51s\n",
      "570:\tlearn: 0.4148726\ttotal: 11m 45s\tremaining: 8m 49s\n",
      "571:\tlearn: 0.4148314\ttotal: 11m 46s\tremaining: 8m 48s\n",
      "572:\tlearn: 0.4147672\ttotal: 11m 47s\tremaining: 8m 47s\n",
      "573:\tlearn: 0.4146791\ttotal: 11m 48s\tremaining: 8m 45s\n",
      "574:\tlearn: 0.4145778\ttotal: 11m 49s\tremaining: 8m 44s\n",
      "575:\tlearn: 0.4145174\ttotal: 11m 50s\tremaining: 8m 43s\n",
      "576:\tlearn: 0.4144298\ttotal: 11m 51s\tremaining: 8m 41s\n",
      "577:\tlearn: 0.4143401\ttotal: 11m 53s\tremaining: 8m 40s\n",
      "578:\tlearn: 0.4142489\ttotal: 11m 54s\tremaining: 8m 39s\n",
      "579:\tlearn: 0.4141663\ttotal: 11m 55s\tremaining: 8m 38s\n",
      "580:\tlearn: 0.4140741\ttotal: 11m 56s\tremaining: 8m 36s\n",
      "581:\tlearn: 0.4139750\ttotal: 11m 57s\tremaining: 8m 35s\n",
      "582:\tlearn: 0.4138931\ttotal: 11m 58s\tremaining: 8m 34s\n",
      "583:\tlearn: 0.4138302\ttotal: 12m\tremaining: 8m 32s\n",
      "584:\tlearn: 0.4135812\ttotal: 12m 1s\tremaining: 8m 31s\n",
      "585:\tlearn: 0.4134944\ttotal: 12m 2s\tremaining: 8m 30s\n",
      "586:\tlearn: 0.4134259\ttotal: 12m 3s\tremaining: 8m 29s\n",
      "587:\tlearn: 0.4133554\ttotal: 12m 4s\tremaining: 8m 27s\n",
      "588:\tlearn: 0.4132854\ttotal: 12m 5s\tremaining: 8m 26s\n",
      "589:\tlearn: 0.4132260\ttotal: 12m 6s\tremaining: 8m 25s\n",
      "590:\tlearn: 0.4131322\ttotal: 12m 8s\tremaining: 8m 23s\n",
      "591:\tlearn: 0.4130378\ttotal: 12m 9s\tremaining: 8m 22s\n",
      "592:\tlearn: 0.4129601\ttotal: 12m 10s\tremaining: 8m 21s\n",
      "593:\tlearn: 0.4128701\ttotal: 12m 11s\tremaining: 8m 20s\n",
      "594:\tlearn: 0.4127950\ttotal: 12m 13s\tremaining: 8m 18s\n",
      "595:\tlearn: 0.4127193\ttotal: 12m 14s\tremaining: 8m 17s\n",
      "596:\tlearn: 0.4126404\ttotal: 12m 15s\tremaining: 8m 16s\n",
      "597:\tlearn: 0.4125660\ttotal: 12m 16s\tremaining: 8m 15s\n",
      "598:\tlearn: 0.4124741\ttotal: 12m 17s\tremaining: 8m 14s\n",
      "599:\tlearn: 0.4123994\ttotal: 12m 19s\tremaining: 8m 12s\n",
      "600:\tlearn: 0.4123202\ttotal: 12m 20s\tremaining: 8m 11s\n",
      "601:\tlearn: 0.4122474\ttotal: 12m 21s\tremaining: 8m 10s\n",
      "602:\tlearn: 0.4121785\ttotal: 12m 22s\tremaining: 8m 8s\n",
      "603:\tlearn: 0.4120854\ttotal: 12m 24s\tremaining: 8m 7s\n",
      "604:\tlearn: 0.4120088\ttotal: 12m 25s\tremaining: 8m 6s\n",
      "605:\tlearn: 0.4119209\ttotal: 12m 26s\tremaining: 8m 5s\n",
      "606:\tlearn: 0.4118218\ttotal: 12m 27s\tremaining: 8m 4s\n",
      "607:\tlearn: 0.4117600\ttotal: 12m 29s\tremaining: 8m 2s\n",
      "608:\tlearn: 0.4116896\ttotal: 12m 30s\tremaining: 8m 1s\n",
      "609:\tlearn: 0.4115886\ttotal: 12m 31s\tremaining: 8m\n",
      "610:\tlearn: 0.4115017\ttotal: 12m 32s\tremaining: 7m 59s\n",
      "611:\tlearn: 0.4114292\ttotal: 12m 33s\tremaining: 7m 58s\n",
      "612:\tlearn: 0.4113443\ttotal: 12m 35s\tremaining: 7m 56s\n",
      "613:\tlearn: 0.4112512\ttotal: 12m 36s\tremaining: 7m 55s\n",
      "614:\tlearn: 0.4111724\ttotal: 12m 37s\tremaining: 7m 54s\n",
      "615:\tlearn: 0.4110860\ttotal: 12m 38s\tremaining: 7m 53s\n",
      "616:\tlearn: 0.4110428\ttotal: 12m 40s\tremaining: 7m 51s\n",
      "617:\tlearn: 0.4109697\ttotal: 12m 41s\tremaining: 7m 50s\n",
      "618:\tlearn: 0.4108975\ttotal: 12m 42s\tremaining: 7m 49s\n",
      "619:\tlearn: 0.4108355\ttotal: 12m 43s\tremaining: 7m 47s\n",
      "620:\tlearn: 0.4107378\ttotal: 12m 44s\tremaining: 7m 46s\n",
      "621:\tlearn: 0.4107008\ttotal: 12m 45s\tremaining: 7m 45s\n",
      "622:\tlearn: 0.4106134\ttotal: 12m 47s\tremaining: 7m 44s\n",
      "623:\tlearn: 0.4104933\ttotal: 12m 48s\tremaining: 7m 42s\n",
      "624:\tlearn: 0.4104120\ttotal: 12m 49s\tremaining: 7m 41s\n",
      "625:\tlearn: 0.4103622\ttotal: 12m 50s\tremaining: 7m 40s\n",
      "626:\tlearn: 0.4102481\ttotal: 12m 51s\tremaining: 7m 39s\n",
      "627:\tlearn: 0.4101028\ttotal: 12m 53s\tremaining: 7m 38s\n",
      "628:\tlearn: 0.4100016\ttotal: 12m 54s\tremaining: 7m 36s\n",
      "629:\tlearn: 0.4099471\ttotal: 12m 55s\tremaining: 7m 35s\n",
      "630:\tlearn: 0.4098827\ttotal: 12m 56s\tremaining: 7m 34s\n",
      "631:\tlearn: 0.4097995\ttotal: 12m 57s\tremaining: 7m 32s\n",
      "632:\tlearn: 0.4097064\ttotal: 12m 59s\tremaining: 7m 31s\n",
      "633:\tlearn: 0.4096234\ttotal: 13m\tremaining: 7m 30s\n",
      "634:\tlearn: 0.4095551\ttotal: 13m 1s\tremaining: 7m 29s\n",
      "635:\tlearn: 0.4094230\ttotal: 13m 2s\tremaining: 7m 27s\n",
      "636:\tlearn: 0.4093456\ttotal: 13m 3s\tremaining: 7m 26s\n",
      "637:\tlearn: 0.4092960\ttotal: 13m 4s\tremaining: 7m 25s\n",
      "638:\tlearn: 0.4092193\ttotal: 13m 5s\tremaining: 7m 24s\n",
      "639:\tlearn: 0.4091302\ttotal: 13m 7s\tremaining: 7m 22s\n",
      "640:\tlearn: 0.4090838\ttotal: 13m 8s\tremaining: 7m 21s\n",
      "641:\tlearn: 0.4089987\ttotal: 13m 9s\tremaining: 7m 20s\n",
      "642:\tlearn: 0.4089310\ttotal: 13m 10s\tremaining: 7m 18s\n",
      "643:\tlearn: 0.4088708\ttotal: 13m 11s\tremaining: 7m 17s\n",
      "644:\tlearn: 0.4087949\ttotal: 13m 13s\tremaining: 7m 16s\n",
      "645:\tlearn: 0.4087061\ttotal: 13m 14s\tremaining: 7m 15s\n",
      "646:\tlearn: 0.4086227\ttotal: 13m 15s\tremaining: 7m 14s\n",
      "647:\tlearn: 0.4085330\ttotal: 13m 16s\tremaining: 7m 12s\n",
      "648:\tlearn: 0.4084571\ttotal: 13m 17s\tremaining: 7m 11s\n",
      "649:\tlearn: 0.4083652\ttotal: 13m 19s\tremaining: 7m 10s\n",
      "650:\tlearn: 0.4083155\ttotal: 13m 20s\tremaining: 7m 9s\n",
      "651:\tlearn: 0.4082459\ttotal: 13m 21s\tremaining: 7m 8s\n",
      "652:\tlearn: 0.4081637\ttotal: 13m 23s\tremaining: 7m 6s\n",
      "653:\tlearn: 0.4081068\ttotal: 13m 24s\tremaining: 7m 5s\n",
      "654:\tlearn: 0.4080724\ttotal: 13m 25s\tremaining: 7m 4s\n",
      "655:\tlearn: 0.4079934\ttotal: 13m 27s\tremaining: 7m 3s\n",
      "656:\tlearn: 0.4079096\ttotal: 13m 28s\tremaining: 7m 1s\n",
      "657:\tlearn: 0.4078270\ttotal: 13m 29s\tremaining: 7m\n",
      "658:\tlearn: 0.4077553\ttotal: 13m 30s\tremaining: 6m 59s\n",
      "659:\tlearn: 0.4076574\ttotal: 13m 32s\tremaining: 6m 58s\n",
      "660:\tlearn: 0.4075842\ttotal: 13m 33s\tremaining: 6m 57s\n",
      "661:\tlearn: 0.4074885\ttotal: 13m 34s\tremaining: 6m 56s\n",
      "662:\tlearn: 0.4074007\ttotal: 13m 36s\tremaining: 6m 54s\n",
      "663:\tlearn: 0.4073285\ttotal: 13m 37s\tremaining: 6m 53s\n",
      "664:\tlearn: 0.4072672\ttotal: 13m 38s\tremaining: 6m 52s\n",
      "665:\tlearn: 0.4071979\ttotal: 13m 40s\tremaining: 6m 51s\n",
      "666:\tlearn: 0.4071263\ttotal: 13m 41s\tremaining: 6m 50s\n",
      "667:\tlearn: 0.4070352\ttotal: 13m 42s\tremaining: 6m 48s\n",
      "668:\tlearn: 0.4069847\ttotal: 13m 43s\tremaining: 6m 47s\n",
      "669:\tlearn: 0.4069062\ttotal: 13m 45s\tremaining: 6m 46s\n",
      "670:\tlearn: 0.4068203\ttotal: 13m 46s\tremaining: 6m 45s\n",
      "671:\tlearn: 0.4067367\ttotal: 13m 47s\tremaining: 6m 44s\n",
      "672:\tlearn: 0.4066694\ttotal: 13m 48s\tremaining: 6m 42s\n",
      "673:\tlearn: 0.4065908\ttotal: 13m 50s\tremaining: 6m 41s\n",
      "674:\tlearn: 0.4065217\ttotal: 13m 51s\tremaining: 6m 40s\n",
      "675:\tlearn: 0.4064268\ttotal: 13m 52s\tremaining: 6m 39s\n",
      "676:\tlearn: 0.4063679\ttotal: 13m 53s\tremaining: 6m 37s\n",
      "677:\tlearn: 0.4062835\ttotal: 13m 55s\tremaining: 6m 36s\n",
      "678:\tlearn: 0.4062239\ttotal: 13m 56s\tremaining: 6m 35s\n",
      "679:\tlearn: 0.4061438\ttotal: 13m 57s\tremaining: 6m 34s\n",
      "680:\tlearn: 0.4060705\ttotal: 13m 58s\tremaining: 6m 32s\n",
      "681:\tlearn: 0.4060010\ttotal: 13m 59s\tremaining: 6m 31s\n",
      "682:\tlearn: 0.4059398\ttotal: 14m\tremaining: 6m 30s\n",
      "683:\tlearn: 0.4058499\ttotal: 14m 2s\tremaining: 6m 29s\n",
      "684:\tlearn: 0.4057550\ttotal: 14m 3s\tremaining: 6m 27s\n",
      "685:\tlearn: 0.4056830\ttotal: 14m 4s\tremaining: 6m 26s\n",
      "686:\tlearn: 0.4055938\ttotal: 14m 5s\tremaining: 6m 25s\n",
      "687:\tlearn: 0.4055271\ttotal: 14m 6s\tremaining: 6m 23s\n",
      "688:\tlearn: 0.4054374\ttotal: 14m 7s\tremaining: 6m 22s\n",
      "689:\tlearn: 0.4053642\ttotal: 14m 8s\tremaining: 6m 21s\n",
      "690:\tlearn: 0.4052952\ttotal: 14m 10s\tremaining: 6m 20s\n",
      "691:\tlearn: 0.4052527\ttotal: 14m 11s\tremaining: 6m 18s\n",
      "692:\tlearn: 0.4051752\ttotal: 14m 12s\tremaining: 6m 17s\n",
      "693:\tlearn: 0.4051140\ttotal: 14m 13s\tremaining: 6m 16s\n",
      "694:\tlearn: 0.4050458\ttotal: 14m 14s\tremaining: 6m 14s\n",
      "695:\tlearn: 0.4049755\ttotal: 14m 15s\tremaining: 6m 13s\n",
      "696:\tlearn: 0.4048955\ttotal: 14m 16s\tremaining: 6m 12s\n",
      "697:\tlearn: 0.4048232\ttotal: 14m 17s\tremaining: 6m 11s\n",
      "698:\tlearn: 0.4046942\ttotal: 14m 19s\tremaining: 6m 9s\n",
      "699:\tlearn: 0.4046083\ttotal: 14m 20s\tremaining: 6m 8s\n",
      "700:\tlearn: 0.4045282\ttotal: 14m 21s\tremaining: 6m 7s\n",
      "701:\tlearn: 0.4044566\ttotal: 14m 22s\tremaining: 6m 6s\n",
      "702:\tlearn: 0.4043834\ttotal: 14m 23s\tremaining: 6m 4s\n",
      "703:\tlearn: 0.4042946\ttotal: 14m 25s\tremaining: 6m 3s\n",
      "704:\tlearn: 0.4042043\ttotal: 14m 26s\tremaining: 6m 2s\n",
      "705:\tlearn: 0.4041466\ttotal: 14m 27s\tremaining: 6m 1s\n",
      "706:\tlearn: 0.4040745\ttotal: 14m 28s\tremaining: 5m 59s\n",
      "707:\tlearn: 0.4040055\ttotal: 14m 29s\tremaining: 5m 58s\n",
      "708:\tlearn: 0.4039412\ttotal: 14m 30s\tremaining: 5m 57s\n",
      "709:\tlearn: 0.4038693\ttotal: 14m 31s\tremaining: 5m 56s\n",
      "710:\tlearn: 0.4037922\ttotal: 14m 32s\tremaining: 5m 54s\n",
      "711:\tlearn: 0.4037298\ttotal: 14m 34s\tremaining: 5m 53s\n",
      "712:\tlearn: 0.4036621\ttotal: 14m 35s\tremaining: 5m 52s\n",
      "713:\tlearn: 0.4035870\ttotal: 14m 36s\tremaining: 5m 51s\n",
      "714:\tlearn: 0.4035062\ttotal: 14m 37s\tremaining: 5m 49s\n",
      "715:\tlearn: 0.4034491\ttotal: 14m 38s\tremaining: 5m 48s\n",
      "716:\tlearn: 0.4033820\ttotal: 14m 39s\tremaining: 5m 47s\n",
      "717:\tlearn: 0.4033193\ttotal: 14m 41s\tremaining: 5m 46s\n",
      "718:\tlearn: 0.4032505\ttotal: 14m 42s\tremaining: 5m 44s\n",
      "719:\tlearn: 0.4031673\ttotal: 14m 43s\tremaining: 5m 43s\n",
      "720:\tlearn: 0.4030986\ttotal: 14m 44s\tremaining: 5m 42s\n",
      "721:\tlearn: 0.4030198\ttotal: 14m 45s\tremaining: 5m 41s\n",
      "722:\tlearn: 0.4029535\ttotal: 14m 46s\tremaining: 5m 39s\n",
      "723:\tlearn: 0.4028614\ttotal: 14m 48s\tremaining: 5m 38s\n",
      "724:\tlearn: 0.4027732\ttotal: 14m 49s\tremaining: 5m 37s\n",
      "725:\tlearn: 0.4025978\ttotal: 14m 50s\tremaining: 5m 36s\n",
      "726:\tlearn: 0.4025478\ttotal: 14m 51s\tremaining: 5m 34s\n",
      "727:\tlearn: 0.4024619\ttotal: 14m 52s\tremaining: 5m 33s\n",
      "728:\tlearn: 0.4023875\ttotal: 14m 53s\tremaining: 5m 32s\n",
      "729:\tlearn: 0.4023432\ttotal: 14m 54s\tremaining: 5m 30s\n",
      "730:\tlearn: 0.4022545\ttotal: 14m 56s\tremaining: 5m 29s\n",
      "731:\tlearn: 0.4022165\ttotal: 14m 57s\tremaining: 5m 28s\n",
      "732:\tlearn: 0.4021437\ttotal: 14m 58s\tremaining: 5m 27s\n",
      "733:\tlearn: 0.4020621\ttotal: 14m 59s\tremaining: 5m 25s\n",
      "734:\tlearn: 0.4020279\ttotal: 15m\tremaining: 5m 24s\n",
      "735:\tlearn: 0.4019557\ttotal: 15m 1s\tremaining: 5m 23s\n",
      "736:\tlearn: 0.4018864\ttotal: 15m 2s\tremaining: 5m 22s\n",
      "737:\tlearn: 0.4018445\ttotal: 15m 3s\tremaining: 5m 20s\n",
      "738:\tlearn: 0.4017722\ttotal: 15m 4s\tremaining: 5m 19s\n",
      "739:\tlearn: 0.4016860\ttotal: 15m 6s\tremaining: 5m 18s\n",
      "740:\tlearn: 0.4016171\ttotal: 15m 7s\tremaining: 5m 17s\n",
      "741:\tlearn: 0.4015488\ttotal: 15m 8s\tremaining: 5m 15s\n",
      "742:\tlearn: 0.4014699\ttotal: 15m 9s\tremaining: 5m 14s\n",
      "743:\tlearn: 0.4013872\ttotal: 15m 10s\tremaining: 5m 13s\n",
      "744:\tlearn: 0.4013260\ttotal: 15m 11s\tremaining: 5m 12s\n",
      "745:\tlearn: 0.4012319\ttotal: 15m 12s\tremaining: 5m 10s\n",
      "746:\tlearn: 0.4011968\ttotal: 15m 14s\tremaining: 5m 9s\n",
      "747:\tlearn: 0.4011356\ttotal: 15m 15s\tremaining: 5m 8s\n",
      "748:\tlearn: 0.4010359\ttotal: 15m 16s\tremaining: 5m 7s\n",
      "749:\tlearn: 0.4009689\ttotal: 15m 17s\tremaining: 5m 5s\n",
      "750:\tlearn: 0.4009018\ttotal: 15m 18s\tremaining: 5m 4s\n",
      "751:\tlearn: 0.4008317\ttotal: 15m 20s\tremaining: 5m 3s\n",
      "752:\tlearn: 0.4007588\ttotal: 15m 21s\tremaining: 5m 2s\n",
      "753:\tlearn: 0.4006895\ttotal: 15m 22s\tremaining: 5m\n",
      "754:\tlearn: 0.4005959\ttotal: 15m 23s\tremaining: 4m 59s\n",
      "755:\tlearn: 0.4005249\ttotal: 15m 24s\tremaining: 4m 58s\n",
      "756:\tlearn: 0.4004403\ttotal: 15m 26s\tremaining: 4m 57s\n",
      "757:\tlearn: 0.4003720\ttotal: 15m 27s\tremaining: 4m 56s\n",
      "758:\tlearn: 0.4003175\ttotal: 15m 28s\tremaining: 4m 54s\n",
      "759:\tlearn: 0.4002569\ttotal: 15m 29s\tremaining: 4m 53s\n",
      "760:\tlearn: 0.4001804\ttotal: 15m 31s\tremaining: 4m 52s\n",
      "761:\tlearn: 0.4001205\ttotal: 15m 32s\tremaining: 4m 51s\n",
      "762:\tlearn: 0.4000557\ttotal: 15m 33s\tremaining: 4m 49s\n",
      "763:\tlearn: 0.3999806\ttotal: 15m 34s\tremaining: 4m 48s\n",
      "764:\tlearn: 0.3999378\ttotal: 15m 35s\tremaining: 4m 47s\n",
      "765:\tlearn: 0.3998553\ttotal: 15m 37s\tremaining: 4m 46s\n",
      "766:\tlearn: 0.3997756\ttotal: 15m 38s\tremaining: 4m 45s\n",
      "767:\tlearn: 0.3996796\ttotal: 15m 39s\tremaining: 4m 43s\n",
      "768:\tlearn: 0.3996224\ttotal: 15m 41s\tremaining: 4m 42s\n",
      "769:\tlearn: 0.3995351\ttotal: 15m 42s\tremaining: 4m 41s\n",
      "770:\tlearn: 0.3994562\ttotal: 15m 43s\tremaining: 4m 40s\n",
      "771:\tlearn: 0.3993719\ttotal: 15m 44s\tremaining: 4m 39s\n",
      "772:\tlearn: 0.3993244\ttotal: 15m 45s\tremaining: 4m 37s\n",
      "773:\tlearn: 0.3992538\ttotal: 15m 47s\tremaining: 4m 36s\n",
      "774:\tlearn: 0.3991552\ttotal: 15m 48s\tremaining: 4m 35s\n",
      "775:\tlearn: 0.3991082\ttotal: 15m 49s\tremaining: 4m 34s\n",
      "776:\tlearn: 0.3990372\ttotal: 15m 51s\tremaining: 4m 32s\n",
      "777:\tlearn: 0.3989897\ttotal: 15m 52s\tremaining: 4m 31s\n",
      "778:\tlearn: 0.3989048\ttotal: 15m 53s\tremaining: 4m 30s\n",
      "779:\tlearn: 0.3988436\ttotal: 15m 54s\tremaining: 4m 29s\n",
      "780:\tlearn: 0.3987479\ttotal: 15m 56s\tremaining: 4m 28s\n",
      "781:\tlearn: 0.3986880\ttotal: 15m 57s\tremaining: 4m 26s\n",
      "782:\tlearn: 0.3986302\ttotal: 15m 58s\tremaining: 4m 25s\n",
      "783:\tlearn: 0.3985605\ttotal: 16m\tremaining: 4m 24s\n",
      "784:\tlearn: 0.3984993\ttotal: 16m 1s\tremaining: 4m 23s\n",
      "785:\tlearn: 0.3984060\ttotal: 16m 2s\tremaining: 4m 22s\n",
      "786:\tlearn: 0.3983304\ttotal: 16m 4s\tremaining: 4m 20s\n",
      "787:\tlearn: 0.3982711\ttotal: 16m 5s\tremaining: 4m 19s\n",
      "788:\tlearn: 0.3981974\ttotal: 16m 6s\tremaining: 4m 18s\n",
      "789:\tlearn: 0.3981654\ttotal: 16m 7s\tremaining: 4m 17s\n",
      "790:\tlearn: 0.3980875\ttotal: 16m 9s\tremaining: 4m 16s\n",
      "791:\tlearn: 0.3980265\ttotal: 16m 10s\tremaining: 4m 14s\n",
      "792:\tlearn: 0.3979559\ttotal: 16m 11s\tremaining: 4m 13s\n",
      "793:\tlearn: 0.3978854\ttotal: 16m 13s\tremaining: 4m 12s\n",
      "794:\tlearn: 0.3978280\ttotal: 16m 14s\tremaining: 4m 11s\n",
      "795:\tlearn: 0.3977459\ttotal: 16m 15s\tremaining: 4m 10s\n",
      "796:\tlearn: 0.3977082\ttotal: 16m 16s\tremaining: 4m 8s\n",
      "797:\tlearn: 0.3976488\ttotal: 16m 17s\tremaining: 4m 7s\n",
      "798:\tlearn: 0.3975911\ttotal: 16m 19s\tremaining: 4m 6s\n",
      "799:\tlearn: 0.3975229\ttotal: 16m 20s\tremaining: 4m 5s\n",
      "800:\tlearn: 0.3974259\ttotal: 16m 21s\tremaining: 4m 3s\n",
      "801:\tlearn: 0.3973986\ttotal: 16m 22s\tremaining: 4m 2s\n",
      "802:\tlearn: 0.3973193\ttotal: 16m 24s\tremaining: 4m 1s\n",
      "803:\tlearn: 0.3972558\ttotal: 16m 25s\tremaining: 4m\n",
      "804:\tlearn: 0.3971983\ttotal: 16m 26s\tremaining: 3m 59s\n",
      "805:\tlearn: 0.3971263\ttotal: 16m 28s\tremaining: 3m 57s\n",
      "806:\tlearn: 0.3970864\ttotal: 16m 29s\tremaining: 3m 56s\n",
      "807:\tlearn: 0.3970248\ttotal: 16m 30s\tremaining: 3m 55s\n",
      "808:\tlearn: 0.3969603\ttotal: 16m 31s\tremaining: 3m 54s\n",
      "809:\tlearn: 0.3968959\ttotal: 16m 32s\tremaining: 3m 52s\n",
      "810:\tlearn: 0.3968295\ttotal: 16m 34s\tremaining: 3m 51s\n",
      "811:\tlearn: 0.3967533\ttotal: 16m 35s\tremaining: 3m 50s\n",
      "812:\tlearn: 0.3966727\ttotal: 16m 36s\tremaining: 3m 49s\n",
      "813:\tlearn: 0.3965798\ttotal: 16m 37s\tremaining: 3m 47s\n",
      "814:\tlearn: 0.3965032\ttotal: 16m 38s\tremaining: 3m 46s\n",
      "815:\tlearn: 0.3964409\ttotal: 16m 40s\tremaining: 3m 45s\n",
      "816:\tlearn: 0.3963655\ttotal: 16m 41s\tremaining: 3m 44s\n",
      "817:\tlearn: 0.3963138\ttotal: 16m 42s\tremaining: 3m 43s\n",
      "818:\tlearn: 0.3962536\ttotal: 16m 43s\tremaining: 3m 41s\n",
      "819:\tlearn: 0.3960116\ttotal: 16m 44s\tremaining: 3m 40s\n",
      "820:\tlearn: 0.3959303\ttotal: 16m 46s\tremaining: 3m 39s\n",
      "821:\tlearn: 0.3958474\ttotal: 16m 47s\tremaining: 3m 38s\n",
      "822:\tlearn: 0.3958071\ttotal: 16m 48s\tremaining: 3m 36s\n",
      "823:\tlearn: 0.3957546\ttotal: 16m 49s\tremaining: 3m 35s\n",
      "824:\tlearn: 0.3956888\ttotal: 16m 50s\tremaining: 3m 34s\n",
      "825:\tlearn: 0.3956008\ttotal: 16m 51s\tremaining: 3m 33s\n",
      "826:\tlearn: 0.3955358\ttotal: 16m 53s\tremaining: 3m 31s\n",
      "827:\tlearn: 0.3954663\ttotal: 16m 54s\tremaining: 3m 30s\n",
      "828:\tlearn: 0.3953913\ttotal: 16m 55s\tremaining: 3m 29s\n",
      "829:\tlearn: 0.3951252\ttotal: 16m 56s\tremaining: 3m 28s\n",
      "830:\tlearn: 0.3950615\ttotal: 16m 57s\tremaining: 3m 27s\n",
      "831:\tlearn: 0.3950210\ttotal: 16m 58s\tremaining: 3m 25s\n",
      "832:\tlearn: 0.3949586\ttotal: 17m\tremaining: 3m 24s\n",
      "833:\tlearn: 0.3948936\ttotal: 17m 1s\tremaining: 3m 23s\n",
      "834:\tlearn: 0.3948018\ttotal: 17m 2s\tremaining: 3m 22s\n",
      "835:\tlearn: 0.3947402\ttotal: 17m 4s\tremaining: 3m 20s\n",
      "836:\tlearn: 0.3946459\ttotal: 17m 5s\tremaining: 3m 19s\n",
      "837:\tlearn: 0.3945683\ttotal: 17m 6s\tremaining: 3m 18s\n",
      "838:\tlearn: 0.3944988\ttotal: 17m 8s\tremaining: 3m 17s\n",
      "839:\tlearn: 0.3944128\ttotal: 17m 9s\tremaining: 3m 16s\n",
      "840:\tlearn: 0.3943352\ttotal: 17m 11s\tremaining: 3m 14s\n",
      "841:\tlearn: 0.3942827\ttotal: 17m 12s\tremaining: 3m 13s\n",
      "842:\tlearn: 0.3942196\ttotal: 17m 13s\tremaining: 3m 12s\n",
      "843:\tlearn: 0.3941595\ttotal: 17m 14s\tremaining: 3m 11s\n",
      "844:\tlearn: 0.3940890\ttotal: 17m 16s\tremaining: 3m 10s\n",
      "845:\tlearn: 0.3940589\ttotal: 17m 17s\tremaining: 3m 8s\n",
      "846:\tlearn: 0.3939987\ttotal: 17m 18s\tremaining: 3m 7s\n",
      "847:\tlearn: 0.3939373\ttotal: 17m 20s\tremaining: 3m 6s\n",
      "848:\tlearn: 0.3938408\ttotal: 17m 21s\tremaining: 3m 5s\n",
      "849:\tlearn: 0.3937814\ttotal: 17m 23s\tremaining: 3m 4s\n",
      "850:\tlearn: 0.3937442\ttotal: 17m 24s\tremaining: 3m 2s\n",
      "851:\tlearn: 0.3936917\ttotal: 17m 25s\tremaining: 3m 1s\n",
      "852:\tlearn: 0.3936606\ttotal: 17m 26s\tremaining: 3m\n",
      "853:\tlearn: 0.3935912\ttotal: 17m 28s\tremaining: 2m 59s\n",
      "854:\tlearn: 0.3935129\ttotal: 17m 29s\tremaining: 2m 57s\n",
      "855:\tlearn: 0.3934468\ttotal: 17m 30s\tremaining: 2m 56s\n",
      "856:\tlearn: 0.3933774\ttotal: 17m 32s\tremaining: 2m 55s\n",
      "857:\tlearn: 0.3933199\ttotal: 17m 33s\tremaining: 2m 54s\n",
      "858:\tlearn: 0.3932609\ttotal: 17m 34s\tremaining: 2m 53s\n",
      "859:\tlearn: 0.3932157\ttotal: 17m 35s\tremaining: 2m 51s\n",
      "860:\tlearn: 0.3931361\ttotal: 17m 37s\tremaining: 2m 50s\n",
      "861:\tlearn: 0.3930608\ttotal: 17m 38s\tremaining: 2m 49s\n",
      "862:\tlearn: 0.3929975\ttotal: 17m 40s\tremaining: 2m 48s\n",
      "863:\tlearn: 0.3929179\ttotal: 17m 41s\tremaining: 2m 47s\n",
      "864:\tlearn: 0.3928857\ttotal: 17m 42s\tremaining: 2m 45s\n",
      "865:\tlearn: 0.3928062\ttotal: 17m 44s\tremaining: 2m 44s\n",
      "866:\tlearn: 0.3927352\ttotal: 17m 45s\tremaining: 2m 43s\n",
      "867:\tlearn: 0.3926770\ttotal: 17m 46s\tremaining: 2m 42s\n",
      "868:\tlearn: 0.3926141\ttotal: 17m 47s\tremaining: 2m 40s\n",
      "869:\tlearn: 0.3925390\ttotal: 17m 48s\tremaining: 2m 39s\n",
      "870:\tlearn: 0.3924856\ttotal: 17m 50s\tremaining: 2m 38s\n",
      "871:\tlearn: 0.3924175\ttotal: 17m 51s\tremaining: 2m 37s\n",
      "872:\tlearn: 0.3923359\ttotal: 17m 52s\tremaining: 2m 36s\n",
      "873:\tlearn: 0.3922663\ttotal: 17m 53s\tremaining: 2m 34s\n",
      "874:\tlearn: 0.3921903\ttotal: 17m 54s\tremaining: 2m 33s\n",
      "875:\tlearn: 0.3921171\ttotal: 17m 56s\tremaining: 2m 32s\n",
      "876:\tlearn: 0.3920901\ttotal: 17m 57s\tremaining: 2m 31s\n",
      "877:\tlearn: 0.3920302\ttotal: 17m 58s\tremaining: 2m 29s\n",
      "878:\tlearn: 0.3919712\ttotal: 17m 59s\tremaining: 2m 28s\n",
      "879:\tlearn: 0.3918976\ttotal: 18m\tremaining: 2m 27s\n",
      "880:\tlearn: 0.3918369\ttotal: 18m 1s\tremaining: 2m 26s\n",
      "881:\tlearn: 0.3917889\ttotal: 18m 2s\tremaining: 2m 24s\n",
      "882:\tlearn: 0.3917284\ttotal: 18m 3s\tremaining: 2m 23s\n",
      "883:\tlearn: 0.3916650\ttotal: 18m 4s\tremaining: 2m 22s\n",
      "884:\tlearn: 0.3915976\ttotal: 18m 5s\tremaining: 2m 21s\n",
      "885:\tlearn: 0.3915362\ttotal: 18m 6s\tremaining: 2m 19s\n",
      "886:\tlearn: 0.3914484\ttotal: 18m 7s\tremaining: 2m 18s\n",
      "887:\tlearn: 0.3913555\ttotal: 18m 8s\tremaining: 2m 17s\n",
      "888:\tlearn: 0.3912861\ttotal: 18m 10s\tremaining: 2m 16s\n",
      "889:\tlearn: 0.3912152\ttotal: 18m 11s\tremaining: 2m 14s\n",
      "890:\tlearn: 0.3911521\ttotal: 18m 12s\tremaining: 2m 13s\n",
      "891:\tlearn: 0.3910796\ttotal: 18m 13s\tremaining: 2m 12s\n",
      "892:\tlearn: 0.3910252\ttotal: 18m 14s\tremaining: 2m 11s\n",
      "893:\tlearn: 0.3909537\ttotal: 18m 15s\tremaining: 2m 9s\n",
      "894:\tlearn: 0.3908782\ttotal: 18m 16s\tremaining: 2m 8s\n",
      "895:\tlearn: 0.3908140\ttotal: 18m 17s\tremaining: 2m 7s\n",
      "896:\tlearn: 0.3907472\ttotal: 18m 18s\tremaining: 2m 6s\n",
      "897:\tlearn: 0.3906750\ttotal: 18m 19s\tremaining: 2m 4s\n",
      "898:\tlearn: 0.3905907\ttotal: 18m 20s\tremaining: 2m 3s\n",
      "899:\tlearn: 0.3905214\ttotal: 18m 21s\tremaining: 2m 2s\n",
      "900:\tlearn: 0.3904464\ttotal: 18m 22s\tremaining: 2m 1s\n",
      "901:\tlearn: 0.3904171\ttotal: 18m 23s\tremaining: 1m 59s\n",
      "902:\tlearn: 0.3903471\ttotal: 18m 24s\tremaining: 1m 58s\n",
      "903:\tlearn: 0.3902709\ttotal: 18m 26s\tremaining: 1m 57s\n",
      "904:\tlearn: 0.3901867\ttotal: 18m 27s\tremaining: 1m 56s\n",
      "905:\tlearn: 0.3901183\ttotal: 18m 28s\tremaining: 1m 55s\n",
      "906:\tlearn: 0.3900418\ttotal: 18m 29s\tremaining: 1m 53s\n",
      "907:\tlearn: 0.3899666\ttotal: 18m 30s\tremaining: 1m 52s\n",
      "908:\tlearn: 0.3898793\ttotal: 18m 32s\tremaining: 1m 51s\n",
      "909:\tlearn: 0.3898120\ttotal: 18m 33s\tremaining: 1m 50s\n",
      "910:\tlearn: 0.3897635\ttotal: 18m 34s\tremaining: 1m 48s\n",
      "911:\tlearn: 0.3896922\ttotal: 18m 35s\tremaining: 1m 47s\n",
      "912:\tlearn: 0.3896567\ttotal: 18m 37s\tremaining: 1m 46s\n",
      "913:\tlearn: 0.3895936\ttotal: 18m 38s\tremaining: 1m 45s\n",
      "914:\tlearn: 0.3895127\ttotal: 18m 39s\tremaining: 1m 43s\n",
      "915:\tlearn: 0.3894551\ttotal: 18m 40s\tremaining: 1m 42s\n",
      "916:\tlearn: 0.3894290\ttotal: 18m 42s\tremaining: 1m 41s\n",
      "917:\tlearn: 0.3893563\ttotal: 18m 43s\tremaining: 1m 40s\n",
      "918:\tlearn: 0.3892807\ttotal: 18m 44s\tremaining: 1m 39s\n",
      "919:\tlearn: 0.3892157\ttotal: 18m 45s\tremaining: 1m 37s\n",
      "920:\tlearn: 0.3891665\ttotal: 18m 47s\tremaining: 1m 36s\n",
      "921:\tlearn: 0.3891214\ttotal: 18m 48s\tremaining: 1m 35s\n",
      "922:\tlearn: 0.3890469\ttotal: 18m 49s\tremaining: 1m 34s\n",
      "923:\tlearn: 0.3889894\ttotal: 18m 51s\tremaining: 1m 33s\n",
      "924:\tlearn: 0.3888771\ttotal: 18m 52s\tremaining: 1m 31s\n",
      "925:\tlearn: 0.3888212\ttotal: 18m 54s\tremaining: 1m 30s\n",
      "926:\tlearn: 0.3887933\ttotal: 18m 55s\tremaining: 1m 29s\n",
      "927:\tlearn: 0.3887448\ttotal: 18m 56s\tremaining: 1m 28s\n",
      "928:\tlearn: 0.3886778\ttotal: 18m 58s\tremaining: 1m 26s\n",
      "929:\tlearn: 0.3886218\ttotal: 18m 59s\tremaining: 1m 25s\n",
      "930:\tlearn: 0.3885490\ttotal: 19m 1s\tremaining: 1m 24s\n",
      "931:\tlearn: 0.3884904\ttotal: 19m 2s\tremaining: 1m 23s\n",
      "932:\tlearn: 0.3884340\ttotal: 19m 3s\tremaining: 1m 22s\n",
      "933:\tlearn: 0.3883672\ttotal: 19m 5s\tremaining: 1m 20s\n",
      "934:\tlearn: 0.3882864\ttotal: 19m 6s\tremaining: 1m 19s\n",
      "935:\tlearn: 0.3882065\ttotal: 19m 7s\tremaining: 1m 18s\n",
      "936:\tlearn: 0.3881514\ttotal: 19m 9s\tremaining: 1m 17s\n",
      "937:\tlearn: 0.3880553\ttotal: 19m 10s\tremaining: 1m 16s\n",
      "938:\tlearn: 0.3879935\ttotal: 19m 12s\tremaining: 1m 14s\n",
      "939:\tlearn: 0.3879183\ttotal: 19m 13s\tremaining: 1m 13s\n",
      "940:\tlearn: 0.3878820\ttotal: 19m 15s\tremaining: 1m 12s\n",
      "941:\tlearn: 0.3878131\ttotal: 19m 16s\tremaining: 1m 11s\n",
      "942:\tlearn: 0.3877354\ttotal: 19m 17s\tremaining: 1m 9s\n",
      "943:\tlearn: 0.3876618\ttotal: 19m 19s\tremaining: 1m 8s\n",
      "944:\tlearn: 0.3876131\ttotal: 19m 20s\tremaining: 1m 7s\n",
      "945:\tlearn: 0.3875568\ttotal: 19m 21s\tremaining: 1m 6s\n",
      "946:\tlearn: 0.3874966\ttotal: 19m 22s\tremaining: 1m 5s\n",
      "947:\tlearn: 0.3874176\ttotal: 19m 24s\tremaining: 1m 3s\n",
      "948:\tlearn: 0.3873929\ttotal: 19m 25s\tremaining: 1m 2s\n",
      "949:\tlearn: 0.3873111\ttotal: 19m 26s\tremaining: 1m 1s\n",
      "950:\tlearn: 0.3872680\ttotal: 19m 27s\tremaining: 1m\n",
      "951:\tlearn: 0.3871990\ttotal: 19m 29s\tremaining: 58.9s\n",
      "952:\tlearn: 0.3871386\ttotal: 19m 30s\tremaining: 57.7s\n",
      "953:\tlearn: 0.3870725\ttotal: 19m 31s\tremaining: 56.5s\n",
      "954:\tlearn: 0.3870459\ttotal: 19m 32s\tremaining: 55.3s\n",
      "955:\tlearn: 0.3869773\ttotal: 19m 34s\tremaining: 54s\n",
      "956:\tlearn: 0.3869283\ttotal: 19m 35s\tremaining: 52.8s\n",
      "957:\tlearn: 0.3868244\ttotal: 19m 36s\tremaining: 51.6s\n",
      "958:\tlearn: 0.3867547\ttotal: 19m 37s\tremaining: 50.4s\n",
      "959:\tlearn: 0.3867069\ttotal: 19m 38s\tremaining: 49.1s\n",
      "960:\tlearn: 0.3866534\ttotal: 19m 40s\tremaining: 47.9s\n",
      "961:\tlearn: 0.3865538\ttotal: 19m 41s\tremaining: 46.7s\n",
      "962:\tlearn: 0.3865006\ttotal: 19m 42s\tremaining: 45.4s\n",
      "963:\tlearn: 0.3864496\ttotal: 19m 43s\tremaining: 44.2s\n",
      "964:\tlearn: 0.3863818\ttotal: 19m 44s\tremaining: 43s\n",
      "965:\tlearn: 0.3863470\ttotal: 19m 45s\tremaining: 41.7s\n",
      "966:\tlearn: 0.3862996\ttotal: 19m 46s\tremaining: 40.5s\n",
      "967:\tlearn: 0.3862291\ttotal: 19m 47s\tremaining: 39.3s\n",
      "968:\tlearn: 0.3861664\ttotal: 19m 49s\tremaining: 38s\n",
      "969:\tlearn: 0.3860871\ttotal: 19m 50s\tremaining: 36.8s\n",
      "970:\tlearn: 0.3860171\ttotal: 19m 51s\tremaining: 35.6s\n",
      "971:\tlearn: 0.3859904\ttotal: 19m 52s\tremaining: 34.4s\n",
      "972:\tlearn: 0.3859437\ttotal: 19m 53s\tremaining: 33.1s\n",
      "973:\tlearn: 0.3858751\ttotal: 19m 54s\tremaining: 31.9s\n",
      "974:\tlearn: 0.3858019\ttotal: 19m 56s\tremaining: 30.7s\n",
      "975:\tlearn: 0.3857501\ttotal: 19m 57s\tremaining: 29.4s\n",
      "976:\tlearn: 0.3856915\ttotal: 19m 58s\tremaining: 28.2s\n",
      "977:\tlearn: 0.3856642\ttotal: 19m 59s\tremaining: 27s\n",
      "978:\tlearn: 0.3856021\ttotal: 20m\tremaining: 25.7s\n",
      "979:\tlearn: 0.3855289\ttotal: 20m 1s\tremaining: 24.5s\n",
      "980:\tlearn: 0.3854698\ttotal: 20m 2s\tremaining: 23.3s\n",
      "981:\tlearn: 0.3854458\ttotal: 20m 3s\tremaining: 22.1s\n",
      "982:\tlearn: 0.3853993\ttotal: 20m 4s\tremaining: 20.8s\n",
      "983:\tlearn: 0.3853170\ttotal: 20m 5s\tremaining: 19.6s\n",
      "984:\tlearn: 0.3852620\ttotal: 20m 6s\tremaining: 18.4s\n",
      "985:\tlearn: 0.3852145\ttotal: 20m 7s\tremaining: 17.1s\n",
      "986:\tlearn: 0.3851415\ttotal: 20m 8s\tremaining: 15.9s\n",
      "987:\tlearn: 0.3850551\ttotal: 20m 9s\tremaining: 14.7s\n",
      "988:\tlearn: 0.3849704\ttotal: 20m 11s\tremaining: 13.5s\n",
      "989:\tlearn: 0.3849200\ttotal: 20m 12s\tremaining: 12.2s\n",
      "990:\tlearn: 0.3848664\ttotal: 20m 13s\tremaining: 11s\n",
      "991:\tlearn: 0.3848136\ttotal: 20m 14s\tremaining: 9.79s\n",
      "992:\tlearn: 0.3847536\ttotal: 20m 15s\tremaining: 8.57s\n",
      "993:\tlearn: 0.3846791\ttotal: 20m 16s\tremaining: 7.34s\n",
      "994:\tlearn: 0.3846541\ttotal: 20m 17s\tremaining: 6.12s\n",
      "995:\tlearn: 0.3845827\ttotal: 20m 18s\tremaining: 4.89s\n",
      "996:\tlearn: 0.3845144\ttotal: 20m 19s\tremaining: 3.67s\n",
      "997:\tlearn: 0.3844418\ttotal: 20m 20s\tremaining: 2.45s\n",
      "998:\tlearn: 0.3843729\ttotal: 20m 21s\tremaining: 1.22s\n",
      "999:\tlearn: 0.3843197\ttotal: 20m 22s\tremaining: 0us\n",
      "accuracy score: 0.814801817082919\n",
      "f1 score: 0.7953198733004374\n"
     ]
    }
   ],
   "source": [
    "# train with catboost\n",
    "model_catboost = train_catboost(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBMClassifier tfidf 0.777 (very fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 102582, number of negative: 102581\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 16.314267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1192906\n",
      "[LightGBM] [Info] Number of data points in the train set: 205163, number of used features: 14304\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000010\n",
      "[LightGBM] [Info] Start training from score 0.000010\n",
      "f1 score: 0.7745418135910496\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train with lgbm\n",
    "model_lgbm = train_lgbm(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier tfidf 0.681"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.6805433245269928\n"
     ]
    }
   ],
   "source": [
    "# knnclassifier with tfidf data, wrap in a function\n",
    "\n",
    "def train_knn(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # instantiate knn model\n",
    "    knn = KNeighborsClassifier(random_state=42)\n",
    "\n",
    "    # fit model\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    # print f1 score\n",
    "    print(f\"f1 score: {f1_score(y_test, y_pred)}\")\n",
    "\n",
    "    return knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train with knn\n",
    "model_knn = train_knn(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question (full narrative data)\n",
    "\n",
    "bench test: lr and nb model f1\n",
    "\n",
    "- bow only (below)\n",
    "\n",
    "- tfidf only (done. above section)\n",
    "- tfidf only. feature importantces, key terms (done)\n",
    "\n",
    "- w2v only (done)\n",
    "\n",
    "- tfidf weighted w2v, which better? (done. non-weighted w2v is better than tfidf weighed w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to test. w2v (or tfidf-w2v) mixture with other categorical columns? (done, tfidf-w2v mixture categorical perform best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of count_vectorizer_matrix: (164034, 94466)\n"
     ]
    }
   ],
   "source": [
    "# BOW on narrative_processed column\n",
    "\n",
    "# initialize count vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# fit and transform count vectorizer on narrative_processed column\n",
    "count_vectorizer.fit(df2.narrative_processed)\n",
    "count_vectorizer_matrix = count_vectorizer.transform(df2.narrative_processed)\n",
    "\n",
    "# print shape of count_vectorizer_matrix\n",
    "print(f\"shape of count_vectorizer_matrix: {count_vectorizer_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256454, 94466) (256454,)\n"
     ]
    }
   ],
   "source": [
    "# smote to oversample minority class for count vectorizer data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote_bow, y_smote_bow = smote.fit_resample(count_vectorizer_matrix, df2.disputed)\n",
    "\n",
    "# print shape of X_smote_bow, y_smote_bow\n",
    "print(X_smote_bow.shape, y_smote_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205163, 94466) (51291, 94466) (205163,) (51291,)\n"
     ]
    }
   ],
   "source": [
    "# train test split, 20% test, random_state=42, stratify=y_smote_bow\n",
    "X_smote_bow_train, X_smote_bow_test, y_smote_bow_train, y_smote_bow_test = train_test_split(X_smote_bow, y_smote_bow, stratify=y_smote_bow, random_state=42, test_size=0.2)\n",
    "\n",
    "# print shape of X_smote_bow_train, X_smote_bow_test, y_smote_bow_train, y_smote_bow_test\n",
    "print(X_smote_bow_train.shape, X_smote_bow_test.shape, y_smote_bow_train.shape, y_smote_bow_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression: 0.763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.7628121736262529\n"
     ]
    }
   ],
   "source": [
    "# train BOW with logistic regression\n",
    "model_lr_bow = train_lr(X_smote_bow_train, y_smote_bow_train, X_smote_bow_test, y_smote_bow_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "naive_bayes: 0.721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.7211319203020448\n"
     ]
    }
   ],
   "source": [
    "# train BOW with naive bayes\n",
    "\n",
    "model_nb_bow = train_naive_bayes(X_smote_bow_train, y_smote_bow_train, X_smote_bow_test, y_smote_bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 102582, number of negative: 102581\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.422180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 50299\n",
      "[LightGBM] [Info] Number of data points in the train set: 205163, number of used features: 10428\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000010\n",
      "[LightGBM] [Info] Start training from score 0.000010\n",
      "f1 score: 0.7518208413194796\n"
     ]
    }
   ],
   "source": [
    "# train lightgbm with BOW data\n",
    "\n",
    "def train_lgbm_bow(X_train, y_train, X_test, y_test):\n",
    "    # Convert input features to float32\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    \n",
    "    # Instantiate LightGBM model\n",
    "    lgbm = LGBMClassifier(random_state=42)\n",
    "    \n",
    "    # Fit model\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = lgbm.predict(X_test)\n",
    "    \n",
    "    # Calculate and print f1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"f1 score: {f1}\")\n",
    "    \n",
    "    return lgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_lgbm_bow = train_lgbm_bow(X_smote_bow_train, y_smote_bow_train, X_smote_bow_test, y_smote_bow_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W2V only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164034/164034 [00:43<00:00, 3758.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# get w2v vector for each narrative, reuse function from above\n",
    "\n",
    "df2['narrative_w2v'] = df2['narrative_processed'].progress_apply(get_mean_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164034, 2)\n",
      "narrative_w2v    0\n",
      "disputed         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# subset narrative_w2v column and disupted column to df_w2v\n",
    "df_w2v = df2[['narrative_w2v', 'disputed']]\n",
    "\n",
    "# print shape of df_w2v\n",
    "print(df_w2v.shape)\n",
    "\n",
    "# print number of null values in df_w2v\n",
    "print(df_w2v.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df_w2v['narrative_w2v']  to numpy array X_w2v\n",
    "\n",
    "X_w2v = np.array(df_w2v.narrative_w2v.tolist())\n",
    "\n",
    "y = df_w2v.disputed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256454, 300) (256454,)\n"
     ]
    }
   ],
   "source": [
    "# apply smote to oversample minority class\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_w2v_smote, y_w2v_smote = smote.fit_resample(X_w2v, y)\n",
    "\n",
    "# print shape of X_w2v_smote, y_w2v_smote\n",
    "print(X_w2v_smote.shape, y_w2v_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split, 20% test, random_state=42, stratify=y_w2v_smote\n",
    "X_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_train, y_w2v_smote_test = train_test_split(X_w2v_smote, y_w2v_smote, stratify=y_w2v_smote, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression: 0.594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.5937518157695958\n"
     ]
    }
   ],
   "source": [
    "# train lr\n",
    "\n",
    "model_lr_w2v = train_lr(X_w2v_smote_train, y_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "naive bayes: 0.595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.5949083030256131\n"
     ]
    }
   ],
   "source": [
    "# train naive bayes\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "model_lr_w2v = train_lr(scaler.fit_transform(X_w2v_smote_train), y_w2v_smote_train, scaler.transform(X_w2v_smote_test), y_w2v_smote_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lightgbm: 0.689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 102582, number of negative: 102581\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76500\n",
      "[LightGBM] [Info] Number of data points in the train set: 205163, number of used features: 300\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000010\n",
      "[LightGBM] [Info] Start training from score 0.000010\n",
      "f1 score: 0.6891626208722866\n"
     ]
    }
   ],
   "source": [
    "# train lightgbm with w2v data\n",
    "\n",
    "model_lgbm_w2v = train_lgbm(X_w2v_smote_train, y_w2v_smote_train, X_w2v_smote_test, y_w2v_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF-W2V (tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>narrative</th>\n",
       "      <th>disputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31338</th>\n",
       "      <td>XXXX XXXX, 2016 To Whom It May Concern : I am ...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31664</th>\n",
       "      <td>History of account : I am a co-signer on XXXX ...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35599</th>\n",
       "      <td>I have excellent credit and the only times I m...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49963</th>\n",
       "      <td>There are XXXX hard pulls in my credit reports...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70552</th>\n",
       "      <td>Experian has fail to delete wrong personal inf...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024109</th>\n",
       "      <td>I opened a checking account in XX/XX/XXXX with...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024112</th>\n",
       "      <td>There was an overpayment on my credit card of ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024119</th>\n",
       "      <td>I have filed dispute to Credit bureau but they...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024126</th>\n",
       "      <td>This complaint follows practices performed by ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028159</th>\n",
       "      <td>First Pay Pal Credit has no idea what they are...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164034 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 narrative disputed\n",
       "31338    XXXX XXXX, 2016 To Whom It May Concern : I am ...      Yes\n",
       "31664    History of account : I am a co-signer on XXXX ...      Yes\n",
       "35599    I have excellent credit and the only times I m...      Yes\n",
       "49963    There are XXXX hard pulls in my credit reports...       No\n",
       "70552    Experian has fail to delete wrong personal inf...       No\n",
       "...                                                    ...      ...\n",
       "4024109  I opened a checking account in XX/XX/XXXX with...      Yes\n",
       "4024112  There was an overpayment on my credit card of ...       No\n",
       "4024119  I have filed dispute to Credit bureau but they...       No\n",
       "4024126  This complaint follows practices performed by ...       No\n",
       "4028159  First Pay Pal Credit has no idea what they are...      Yes\n",
       "\n",
       "[164034 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftw = data.copy()\n",
    "\n",
    "# rename columns\n",
    "\n",
    "dftw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164034/164034 [00:15<00:00, 10385.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# encode disputed column and preprocess narrative column\n",
    "\n",
    "dftw.disputed = dftw.disputed.map({'No': 0, 'Yes': 1})\n",
    "\n",
    "dftw['narrative_processed'] = dftw.narrative.progress_apply(preprocess_narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164034/164034 [1:07:47<00:00, 40.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# get tfidf weighted word2vec vector for each narrative, reuse function from above\n",
    "\n",
    "dftw['narrative_tw'] = dftw['narrative_processed'].progress_apply(get_mean_tfidf_weighted_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256454, 300) (256454,)\n"
     ]
    }
   ],
   "source": [
    "# convert df_tw['narrative_tw']  to numpy array X_tw, assign y to disputed column\n",
    "\n",
    "X_tw = np.array(dftw.narrative_tw.tolist())\n",
    "y = dftw.disputed.copy()\n",
    "\n",
    "# smote to oversample minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_tw_smote, y_tw_smote = smote.fit_resample(X_tw, y)\n",
    "print(X_tw_smote.shape, y_tw_smote.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split, 20% test, random_state=42, stratify=y_tw_smote\n",
    "X_tw_smote_train, X_tw_smote_test, y_tw_smote_train, y_tw_smote_test = train_test_split(X_tw_smote, y_tw_smote, stratify=y_tw_smote, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression: 0.589\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes: 0.589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.5894106280193236\n",
      "f1 score: 0.5892674254574968\n"
     ]
    }
   ],
   "source": [
    "# train lr and naive bayes\n",
    "model_lr_tw = train_lr(X_tw_smote_train, y_tw_smote_train, X_tw_smote_test, y_tw_smote_test)\n",
    "\n",
    "# since naive bayes doesn't work with negative values, we scale the values\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "model_lr_tw = train_lr(scaler.fit_transform(X_tw_smote_train), y_tw_smote_train, scaler.transform(X_tw_smote_test), y_tw_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lightgbm: 0.684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 102582, number of negative: 102581\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76500\n",
      "[LightGBM] [Info] Number of data points in the train set: 205163, number of used features: 300\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000010\n",
      "[LightGBM] [Info] Start training from score 0.000010\n",
      "f1 score: 0.6840841574085378\n"
     ]
    }
   ],
   "source": [
    "# train lightgbm with tfidf weighted word2vec data\n",
    "model_lgbm_tw = train_lgbm(X_tw_smote_train, y_tw_smote_train, X_tw_smote_test, y_tw_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment. Mixture of categorical columns and w2v matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>Payday loan, title loan, or personal loan</td>\n",
       "      <td>Installment loan</td>\n",
       "      <td>Problem when making payments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OneMain Finance Corporation</td>\n",
       "      <td>NC</td>\n",
       "      <td>27407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7448359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Account information incorrect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>Experian Information Solutions Inc.</td>\n",
       "      <td>GA</td>\n",
       "      <td>30318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent not provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7442747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date received                                            Product  \\\n",
       "0    2023-08-23          Payday loan, title loan, or personal loan   \n",
       "1    2023-08-23  Credit reporting, credit repair services, or o...   \n",
       "\n",
       "        Sub-product                                 Issue  \\\n",
       "0  Installment loan          Problem when making payments   \n",
       "1  Credit reporting  Incorrect information on your report   \n",
       "\n",
       "                       Sub-issue Consumer complaint narrative  \\\n",
       "0                            NaN                          NaN   \n",
       "1  Account information incorrect                          NaN   \n",
       "\n",
       "                             Company public response  \\\n",
       "0                                                NaN   \n",
       "1  Company has responded to the consumer and the ...   \n",
       "\n",
       "                               Company State ZIP code Tags  \\\n",
       "0          OneMain Finance Corporation    NC    27407  NaN   \n",
       "1  Experian Information Solutions Inc.    GA    30318  NaN   \n",
       "\n",
       "  Consumer consent provided? Submitted via Date sent to company  \\\n",
       "0                        NaN           Web           2023-08-23   \n",
       "1       Consent not provided           Web           2023-08-23   \n",
       "\n",
       "      Company response to consumer Timely response? Consumer disputed?  \\\n",
       "0          Closed with explanation              Yes                NaN   \n",
       "1  Closed with non-monetary relief              Yes                NaN   \n",
       "\n",
       "   Complaint ID  \n",
       "0       7448359  \n",
       "1       7442747  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DATA.copy()\n",
    "\n",
    "# rename columns\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values in disputed column and narrative column\n",
    "df.dropna(subset=['Consumer disputed?', 'Consumer complaint narrative'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop ZIP code column and Complaint ID column, Consumer consent provided? column, Submitted via Column\n",
    "df.drop(['ZIP code', 'Complaint ID', 'Consumer consent provided?', 'Submitted via'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute date sent - date received, convery them to datetime format for substraction. store in column 'days' as int\n",
    "\n",
    "df['Date received'] = pd.to_datetime(df['Date received'])\n",
    "df['Date sent to company'] = pd.to_datetime(df['Date sent to company'])\n",
    "\n",
    "df['days'] = (df['Date sent to company'] - df['Date received']).dt.days\n",
    "\n",
    "# drop Date received and Date sent to company columns\n",
    "df.drop(['Date received', 'Date sent to company'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    157932\n",
       "No       6102\n",
       "Name: Timely response?, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Timely response?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    0.977602\n",
       "No     0.022398\n",
       "Name: Timely response?, dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disputed yes, how many timely response? in ratio\n",
    "\n",
    "df[df['Consumer disputed?'] == 'Yes']['Timely response?'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    0.958667\n",
       "No     0.041333\n",
       "Name: Timely response?, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disputed no, how many timely response? in ratio\n",
    "\n",
    "df[df['Consumer disputed?'] == 'No']['Timely response?'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Timely response? column\n",
    "\n",
    "df.drop('Timely response?', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product                              0\n",
       "Sub-product                      52171\n",
       "Issue                                0\n",
       "Sub-issue                        80979\n",
       "Consumer complaint narrative         0\n",
       "Company public response          85912\n",
       "Company                              0\n",
       "State                              459\n",
       "Tags                            137040\n",
       "Company response to consumer         0\n",
       "Consumer disputed?                   0\n",
       "days                                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values across columns\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product                         0\n",
       "Sub-product                     0\n",
       "Issue                           0\n",
       "Sub-issue                       0\n",
       "Consumer complaint narrative    0\n",
       "Company public response         0\n",
       "Company                         0\n",
       "State                           0\n",
       "Tags                            0\n",
       "Company response to consumer    0\n",
       "Consumer disputed?              0\n",
       "days                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill missing values with 'Unknown' in all columns\n",
    "df.fillna('Unknown', inplace=True)\n",
    "\n",
    "# check missing values across columns\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164034/164034 [00:16<00:00, 10054.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocess narrative column and store in narrative_processed column\n",
    "\n",
    "df['narrative_processed'] = df['Consumer complaint narrative'].progress_apply(preprocess_narrative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164034/164034 [00:35<00:00, 4612.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply word2vec on narrative_processed column, save to narrative_w2v column\n",
    "df['narrative_w2v'] = df['narrative_processed'].progress_apply(get_mean_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Product', 'Sub-product', 'Issue', 'Sub-issue',\n",
       "       'Consumer complaint narrative', 'Company public response', 'Company',\n",
       "       'State', 'Tags', 'Company response to consumer', 'Consumer disputed?',\n",
       "       'days', 'narrative_processed', 'narrative_w2v'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['narraive_processed'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb Cell 165\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y536sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m narrative_original_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(df[\u001b[39m'\u001b[39m\u001b[39mConsumer complaint narrative\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y536sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# drop Consumer disputed? and Consumer complaint narrative columns\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/apple/DataspellProjects/mads_dataspell/696/ml8.ipynb#Y536sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mConsumer disputed?\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mConsumer complaint narrative\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnarraive_processed\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnarrative_w2v\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/pandas/core/frame.py:5391\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5243\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   5244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5245\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5252\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5253\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5254\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5255\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5256\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5389\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5390\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   5392\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   5393\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5394\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   5395\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   5396\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   5397\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   5398\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   5399\u001b[0m     )\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/pandas/core/generic.py:4510\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4508\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4509\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4510\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4512\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   4513\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/pandas/core/generic.py:4551\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4549\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m   4550\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4551\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4552\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4554\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4555\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/DataspellProjects/mads_dataspell/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:6972\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6970\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m   6971\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 6972\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6973\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m   6974\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['narraive_processed'] not found in axis\""
     ]
    }
   ],
   "source": [
    "y = df['Consumer disputed?'].copy()\n",
    "\n",
    "narrative_w2v_matrix = np.array(df.narrative_w2v.tolist())\n",
    "\n",
    "narrative_processed_matrix = np.array(df.narrative_processed.tolist())\n",
    "\n",
    "narrative_original_matrix = np.array(df['Consumer complaint narrative'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['may concern writing regard barrett financial llc dealing company feel important pertinent agencies notified shady business practices every hard money lender az said illegal owner occupied home hoping letter writing get someone look company ability follow understand law really hurt someone information terrified capable hoping put fraud alert credit agencies everyone involved deal unhappy company worked particular man show respect told blatant lies another party process purchasing home took personal financial info including bank account info social security numbers sent us prequalification letter found home put offer following guidelines thought good wrong man disappeared buyers sellers agent agent title company inspection company hold talked receptionist know going would circumstances allow us speak another lender phone message machines voice mails full front main line went days lost house asked simply pay us back wire transfer costs title company refused still refuses day knew paperwork took advantage lied person lack communication clients subpar best honestly believe man thinks law better people pure sociopath humble opinion sincerely ca az',\n",
       "       'history account cosigner auto loan husband immigrant trying establish credit loan years never reported late appx actually corporate wide credit reporting issue months pull reporting bureau fixing resulted us losing established credit account files significantly dropping credit scores time forcing us sub prime lending histories added back bureau increased score renumeration given higher rates interest pay new accounts established time using auto pay option though current issue sent alert company called bridgecrest stage never heard placed late credit file called find company told bought earlier year escalated team supervisor named pulled file determined likely fault transferring auto draft gave number call escalated supervisor informed issue lay solely bridgecrest refused issue email stating acted error directed back bridgecrest continue day disavow responsibility party provider provide auto pay internally immediately caught late pay plus current bill set new auto pay bridgecrest great deal pushed back forth gave awhile credit report came saw dropped scores points closing new home put us much higher interest rate new home currently buying cost us added month called bridgecrest back plea bargain courtesy removal late reporting justification shared responsibility scripted response politely told rep would forced involve cfpb ignored claimed calls made told never happened asked recording calls supposedly tried make letters showing bridgecrest ever sent us letter introduction bridgecrest stage became politely frustrated transferred supervisor named listened sympathetically looked phone log informed one numbers calling bad number number mine said ability pull record see call log asked would identified calling said would likely say bridgecrest would like speak recollection ever receiving call call numbers saved phone however still working numbers go directly bridgecrest point unrecognizable company phone number nt know bridgecrest would assumed sales people blocked also nt seem know cfpb gave extension suggested call try get send email stating fault brings degrees back started',\n",
       "       'excellent credit times miss payment revolving credit account vendor fails send bill aka car care synchrony bank formerly ge capital one several similar cards company also large money market account one specialty cards purchase interest free qualifying purchase period time presuming account kept current get slapped interest rate either paid regularly full deadline know always pay full time never received bill emergency car repair purchase unsure would show bill since busy way keep track companies refuse notify owe money way knowing missed payment bill showed immediately called company tell missed payment nt received bill nice customer service agent took late charge told would problem made double payment assured notated account three days later started getting regular calls number synchrony bank telling call putting hold minutes since busy calls came meetings dinner hung first tried call back time called back got someone nt even identify company department took minutes precious time figure collections determined nt security tell accounts breached told already called tell mistake told pay payment due later month asking check records leave alone intervening two weeks received daily calls letter synchrony often dinner time times day supposed call yesterday finally called number letter spent minutes transferred finally get escalation unit us asked supervisor told could refer supervisor voice mail none people answered phone identified company answered phone supervisor never called back despite clearly stating would report cfpb nt hear back evening scheduled payment months suspect going slapped huge fees interest rates synchrony ca nt seem send bills regular basis keep records straight btw first time received bill specialty cards',\n",
       "       ...,\n",
       "       'filed dispute credit bureau nt give proper notice investigation',\n",
       "       'complaint follows practices performed jp morgan chase co bank pertaining checking account deposit item amount check processed inadvertently using bank mobile banking app allowed redeposit exact check twice readily admit unintended human error nt chase realized mistake overdrafting account deposited account acknowledging error nt understand one largest banks allowed mobile banking platform insecurely flawed allow successful reoccurring deposits check bank accepted check twice left confidence check never deposited surely bank would recognized thus rejected reoccurring deposit money gone months later essentially charged account overdrawing held responsible strongly contest held accountable surmounting overdraft fees bank informed would follow account remain overdrawn practice cuts beyond compensatory rationale would justify recompensing instead unduly penalizes consumer associated overdraft fees fault bank could happen yes could happen millions others yes calling serious flaw mobile banking would allow anyone exploit system allows multiple deposits one check unsavory character could recash checks without bank halting check clear electronic banking nt recognize done instead original person issued check disputed bringing bank jp morgan chase attention bank asked money back granted said effect need money back way going conveniently penalize fees capitalize mistake participated clearing check also known despite opting overdraft protection prevents account holders overdrawing accounts make purchase without sufficient funds actually cover charges told would nonetheless subject overdraft fees recurrent insufficient fund fees exchange documented following case number issuing jp morgan chase bank inhouse complaint management team additionally chase defended position consumer row arguing nt catch check issuer financial institution filed appeal subsequently reviewed chase ultimately honored however chase notified realtime could acted swiftly return money redistributing elsewhere believe great magnitude immediate importance investigate ensure consumer protection',\n",
       "       'first pay pal credit idea refunds hit account sent disputes one corrected errors removing refund promotions due comes disputing charge disputed hotwire charge transaction date letters pay pal credit received email today attached finally got around opening dispute charge mo prior kidding currently charges account never purchased charge say date give charge amount monthly statements really scary function make purchase purchase appearing statement could say owe amount indicate addition never disclose merchant charge made become secret always paid time plus record however told months back forth correcting multitude errors correct account pay put payments towards charges mine attaching email received pay pal credit finally faxes finally opened dispute requested merchant hotwire unbelievable also attached final fax dispute detailing remaining charges mine charges mine corrected give correct balance due incorrectly state reported credit bureaus completed fraud affidavits charges sent documentation paypal credit still refuse remove even though charges never appeared statements lastly remaining charges mine attached statement showing charge indicated amount merchant never dealt nt know defraud public defraud please help review documentation let know need anything thank much happy new year attached attachment final dispute sent detailing charges mine charges placed onto promotional purchase list due although never made purchase amounts attachment statement includes charge dates state charges defy find charge amounts say refuse disclose merchant nt matter never made charges amounts state brainer thank heart help verge nervous breakdown summarize attachment charges merchant say owe mysteriously placed account even though charge purchase ever posted account amounts follows attachment will show statements include dates show charges never appeared ever account mine thank attachment contains statements purchases adjustments see purchases none mine hence additional many errors left refuse remove'],\n",
       "      dtype='<U2898')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_processed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"XXXX XXXX, 2016 To Whom It May Concern : I am writing in regard to Barrett Financial L.L.C. After dealing with this company, I feel it is very important that all pertinent agencies are notified about their shady business practices. Every other hard money lender in AZ said what he was doing was 100 % illegal for an owner occupied home. I am hoping my letter writing will get someone to look into this company and their ability to follow and understand the law, before they really hurt someone. With all the information they have on me I am terrified of what they are capable of doing to me. I am hoping to put a fraud alert on with all the credit agencies for everyone involved in the deal. \\nI was very unhappy with this company! I worked with XXXX XXXX. This particular man did not show me respect. He told blatant lies. Another party and I were in the process of purchasing a home with XXXX. He took all our personal financial info ( including bank account info and social security numbers ) and sent us a pre-qualification letter. We found a home and put in an offer, following his guidelines. We thought all was good. We were wrong. The man disappeared. The buyers ( me ), the sellers, our agent, their agent, the title company and the inspection company were all on hold. We talked to the receptionist and she did not know what was going and would under no circumstances allow us to speak to another lender. Then all the phone message machines ( voice mails ) were full, XXXX 's and the front main line. This went on for days until we lost the house. I asked XXXX to simply pay us back for the wire transfer costs to the title company of {$35.00}. He refused and still refuses to this day. I am XXXX and he knew it from all the paperwork. He took advantage and lied to a XXXX person. His lack of communication with his clients was below sub-par at best. \\nI honestly believe this man thinks he is above the law and better than other people. Pure sociopath in my humble opinion. \\n\\nSincerely, XXXX XXXX XXXX XXXX XXXX. # XXXX XXXX, CA XXXX XXXX XXXX XXXX. \\nXXXX XXXX XXXX XXXX XXXX. # XXXX XXXX, AZ XXXX XXXX\",\n",
       "       'History of account : I am a co-signer on XXXX XXXX XXXX auto loan for my husband, XXXX who is an immigrant from XXXX trying to establish his credit. We have had this loan for over 4 years and NEVER had a reported late. In appx XXXX XXXX actually had a corporate wide credit reporting issue and for 7 months had to pull ALL their reporting from the bureau while fixing it. This resulted in us losing an established credit account from our files ; significantly dropping our credit scores during that time and forcing us into sub prime lending. The histories were added back to the bureau, it increased our score but no renumeration was given for the higher rates of interest we had to pay on new accounts established during that time. We had been using their only auto pay option though XXXX XXXX. \\n\\nCurrent issue : In XXXX, XXXX XXXX sent me an alert that a company called Bridgecrest ( whom at this stage I had never heard of ) placed a XXXX late on our credit file. I called to find out who the company was and was told they bought out XXXX earlier in the year. I escalated to a team supervisor named XXXX who pulled my file and it was determined that XXXX XXXX was \" likely \\'\\' at fault for not transferring the auto draft. He gave me the number to call XXXX. I escalated to their supervisor who informed me that the issue lay solely with Bridgecrest and refused issue an email stating that XXXX had acted in error. I was only directed back to Bridgecrest who continue to this day to disavow any responsibility with their XXXX party provider ( they now provide their own auto pay internally ). We immediately caught up the late pay plus the current bill and set a new auto pay with Bridgecrest themselves. After a great deal of being pushed back and forth, I gave up for awhile. \\n\\nThat is until the XXXX credit report came out and I saw it dropped our scores by XXXX points. We are closing a new home on XXXX XXXX and this has put us into a much higher interest rate for the new home we are currently buying. It will cost us an added {$270.00} a month! I called Bridgecrest back again to plea bargain for a courtesy removal of the late reporting under the justification that it was a shared responsibility. Same scripted response. I politely told the rep that I would be forced to involve the CFPB. It was ignored. She claimed there had been XXXX calls made. I told her it never happened and asked to have a recording of any calls they supposedly tried to make to me or any letters showing that XXXX or Bridgecrest had ever sent us a letter of introduction to Bridgecrest. At this stage she became politely frustrated and transferred me to a supervisor named XXXX. \\n\\nXXXX listened sympathetically, looked at the phone log and informed me that one of the numbers they had been calling was a bad number. The other number they had was mine. She said they have no ability to pull a record only to see a call log. I asked if they would have identified who they were when calling. She said they would likely only say : \\'\\' This is Bridgecrest and we would like to speak to XXXX XXXX \\'\\'. I have no recollection of ever receiving a call nor did they call from any of the XXXX XXXX numbers saved in my phone. However, they are still working numbers that go directly to Bridgecrest. My point being it was an unrecognizable company and phone number. If I did n\\'t know who Bridgecrest was, I would have assumed they were sales people and blocked them. She also did n\\'t seem to know who CFPB was. She gave me her extension and suggested I call XXXX XXXX again and try to get them to send an email stating it was their fault ... which brings me XXXX degrees back to where I started.',\n",
       "       \"I have excellent credit and the only times I miss a payment on a revolving credit account is when the vendor fails to send me a bill. XXXX ( aka car care XXXX from Synchrony bank ( formerly GE capital ) is one of several similar cards I have with this company. I also have a very large money market account with them. \\n\\nThis is one of those specialty cards where you can purchase interest free for a qualifying purchase for a period of time and, presuming the account is kept current, and you get slapped with a 26 % interest rate if its either not paid regularly or in full before the deadline. They know I always pay in full and in time. \\n\\nI never received a bill in XXXX for an emergency car repair purchase that I was unsure would show up on the XXXX bill. And since I am busy and have no way to keep track of companies who refuse to notify me that I owe them money, I had no way of knowing I had missed a payment until the XXXX bill showed up. I immediately called the company to tell them that I had missed the payment because I had n't received the bill. The very nice customer service agent took off the late charge and told me that it would be no problem if I just made a double payment in XXXX. She assured me that she had notated the account. \\n\\nThree days later, I started getting regular calls from some number from Synchrony bank telling me that they had a call for me, then putting me on hold for XXXX minutes. Since I am busy and these calls came in during meetings or dinner, I hung up on the first XXXX and tried to call them back when I had time. When I called back, I got to someone who did n't even identify the company they were with or department. It took XXXX minutes of my precious time just to figure out this was collections. Once I determined this was n't security to tell me that my accounts had been breached, I told them that I had already called in to tell them that this was their mistake and been told to pay it when the payment was due later that month, asking them to check their records and leave me alone. \\n\\nIn the intervening two weeks, I have received daily calls and a letter from synchrony, often at dinner time or other times during the day when they are not supposed to call. Yesterday I finally called the number in the letter, then spent XXXX minutes being transferred through XXXX and XXXX to finally get to an escalation unit in the U.S., but when I asked for her supervisor, was told that all she could do is refer me to her supervisor 's voice mail. NONE of the people that answered the phone identified who they were or what company they were with when they answered the phone. The supervisor never called back, despite me clearly stating that I would report this to CFPB if I did n't hear back from him that evening. \\n\\nI have scheduled payment for XXXX months, but suspect I am going to be slapped with huge fees and interest rates because synchrony ca n't seem to send out bills on a regular basis and keep their records straight. BTW, this is not the first time I have not received a bill from them for XXXX of the XXXX specialty cards I have with them.\",\n",
       "       ...,\n",
       "       \"I have filed dispute to Credit bureau but they do n't give me proper notice of the investigation\",\n",
       "       'This complaint follows practices performed by JP Morgan Chase & Co. ( XXXX ) bank pertaining to a checking \\'s account deposit. An item in the amount of {$1000.00}, for a check was processed on XXXX/XXXX/XXXX, and again on XXXX/XXXX/XXXX. Inadvertently using the bank \\'s Mobile Banking app, I was allowed to redeposit the same exact check twice -- which I readily admit was an unintended human error. It was n\\'t until XXXX/XXXX/XXXX when Chase realized this mistake by over-drafting my account for the {$1000.00} of which it deposited into my account. \\n\\nAcknowledging my error, I do n\\'t understand how one of the largest banks allowed for it \\'s mobile banking platform to be so insecurely flawed as to allow successful reoccurring deposits for the same check. Because the bank accepted the same check twice, I was left in confidence that this check had never been deposited -- -because if so, surely the bank would have recognized this and thus rejected it \\'s reoccurring deposit. \\n\\nNow that the money is gone, months later, it essentially charged my account by overdrawing it. As I \\'m held responsible for the {$1000.00}, I strongly contest being held accountable for the surmounting overdraft fees of which the bank informed me would follow should my account remain overdrawn. This practice cuts beyond the compensatory rationale ( which would justify recompensing for the {$1000.00} itself ), but instead unduly penalizes the consumer for associated overdraft fees that are of the fault of the bank. \\n\\nCould this happen again? Yes. Could this happen to millions of others? Yes. What I \\'m calling out is a serious flaw in mobile banking that would allow anyone to exploit a system, which allows multiple deposits of one check. An unsavory character could re-cash checks over and over again, without out the bank halting the check to clear. What \\'s more, is that electronic banking did n\\'t not recognize it had done this. It was instead, the original person who issued the check that disputed this bringing this to the bank \\'s ( JP Morgan Chase ) attention, for which the bank asked for the money back it had granted me, and then said in effect, \" We need that money back. And, by the way, we are going to conveniently penalize you with fees and capitalize from a mistake of which we participated in by clearing you for a check that we should not have. \\'\\' It should also be known despite opting out of overdraft protection ( which prevents account holders from overdrawing their accounts should they make a purchase without having sufficient funds to actually cover their charges ) I was told that I would nonetheless be subject to both \" overdraft fees \\'\\' and \" recurrent insufficient fund fees. \\'\\' This exchange was documented by the following case number issuing from the JP Morgan Chase Bank \\'s in-house complaint management team : \" XXXX. \\'\\' Additionally, Chase defended their position in this consumer row by arguing that they did n\\'t catch this until the check issuer \\'s financial institution filed an appeal, which was subsequently reviewed by Chase and ultimately honored. However, had Chase notified me of this in real-time, I could have acted to swiftly return the money, before redistributing it elsewhere. \\n\\nI believe this is of great magnitude and immediate importance to investigate to ensure consumer protection.',\n",
       "       \"First Pay Pal Credit has no idea what they are doing when refunds hit the account. I have sent in XXXX disputes which each one they corrected their errors of not removing the refund from my promotions due. When it comes to disputing a charge I disputed a {$110.00} hotwire charge transaction date XXXX/XXXX/XXXX on XXXX/XXXX/XXXX. After XXXX letters to pay pal credit I just received an email today [ and attached ] that they finally got around to opening my dispute for the {$110.00} XXXX charge of 4 mo prior ARE YOU KIDDING ME??? There are currently XXXX charges on my account that I never purchased. The charge they say and date they give there is no charge of that amount on my monthly statements. It is really scary how they function. If I did not make the purchase and the purchase is not appearing on my statement how could they say I owe the amount they indicate. In addition they never disclose the Merchant the charge was made with? When did that become a secret. I have always paid on time XXXX A plus record however I told them after 6 months back and forth them correcting a multitude of their errors that until they correct my account I will not pay any more as they will put the payments towards charges that ARE NOT MINE. I am attaching the email received by Pay pal credit XXXX/XXXX/XXXX XXXX finally after XXXX faxes to them they finally opened up my dispute i requested on XXXX/XXXX/XXXX for {$110.00} merchant hotwire [ unbelievable ] I have also attached the final fax dispute to them detailing the XXXX remaining charges that are not mine and why. and the XXXX charges that are mine. which when corrected will give me a correct balance due of {$2700.00} not {$4900.00} as they incorrectly state. I have reported this to the credit bureaus completed fraud affidavits on these charges and sent all this documentation to PayPal credit and they still refuse to remove even though these charges never appeared on my statements. Lastly for each of the remaining XXXX charges that are not mine I have attached my statement showing NO WHERE is there a charge for the indicated amount for any merchant at all. I have never dealt with this I do n't know how they defraud the public but will not defraud me. PLEASE help and after YOU REVIEW THE DOCUMENTATION LET ME KNOW IF YOU NEED ANYTHING FURTHER THANK YOU SO MUCH Happy New Year! I have attached XXXX. Attachment XXXX of XXXX is my final dispute sent to them detailing the charges that are mine and the XXXX charges they placed onto my promotional purchase list due them although i never made any purchase for these amounts so attachment # XXXX is each statement that includes the charge dates the state for the XXXX charges and I defy you to find any charge at all for the amounts they say. Again they refuse to disclose the merchant which does n't matter as I never made and there are no charges in the amounts they state. Its a no brainer. Thank you from the my heart for your help. I am on the verge of a nervous breakdown. To summarize attachment # 1 the XXXX charges with no merchant they say i owe and they mysteriously placed on my account even though no charge purchase ever posted to my account for these amounts are as follows : XXXX/XXXX/XXXX for {$710.00} and XXXX/XXXX/XXXX for {$860.00} and XXXX/XXXX/XXXX for {$430.00} and XXXX/XXXX/XXXX for {$240.00} and XXXX/XXXX/XXXX for {$270.00} and XXXX/XXXX/XXXX for {$180.00} and XXXX/XXXX/XXXX for {$710.00} and XXXX/XXXX/XXXX for {$410.00}. So attachment # 2will show you my statements that include these above XXXX dates to show you these charges never appeared ever on my account so they can not be mine. Thank you Attachment XXXX contains XXXX statements under purchases and adjustments to see my purchases none of them are the XXXX above that are not mine hence they are additional of many errors but these XXXX left they refuse to remove.\"],\n",
       "      dtype='<U5151')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_original_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.03310955e-02,  2.66198311e-02,  1.48412865e-02, ...,\n",
       "        -5.60802110e-02,  7.92303402e-03, -4.32446934e-02],\n",
       "       [ 1.53348176e-02,  4.56538722e-02,  2.79140938e-02, ...,\n",
       "        -2.97490135e-02,  1.41884778e-02, -3.53953280e-02],\n",
       "       [ 1.57127827e-02,  3.29174884e-02,  1.58946514e-02, ...,\n",
       "        -5.23409769e-02,  6.34420989e-03, -4.40079980e-02],\n",
       "       ...,\n",
       "       [-7.43408203e-02,  4.70648892e-03,  5.74408658e-03, ...,\n",
       "        -9.64355469e-02, -1.33680552e-01,  3.20095494e-02],\n",
       "       [-3.67872952e-03,  4.22935337e-02,  2.65742876e-02, ...,\n",
       "        -6.74242377e-02, -1.14557631e-02, -3.84237207e-02],\n",
       "       [ 3.22739870e-05,  2.30683796e-02,  2.35056151e-02, ...,\n",
       "        -7.92681053e-02, -2.23291526e-03, -6.01284802e-02]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_w2v_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Product', 'Sub-product', 'Issue', 'Sub-issue',\n",
       "       'Company public response', 'Company', 'State', 'Tags',\n",
       "       'Company response to consumer', 'days'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop Consumer complaint narrative, narrative_processed, narrative_w2v, Consumer disputed? columns\n",
    "df.drop(['Consumer complaint narrative', 'narrative_processed', 'narrative_w2v', 'Consumer disputed?'], axis=1, inplace=True)\n",
    "\n",
    "# check shape of df\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product                           12\n",
       "Sub-product                       47\n",
       "Issue                             91\n",
       "Sub-issue                         58\n",
       "Company public response           11\n",
       "Company                         3148\n",
       "State                             63\n",
       "Tags                               4\n",
       "Company response to consumer       5\n",
       "days                             269\n",
       "dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical columns Product                         \n",
    "\n",
    "cat_cols = [\n",
    "    \"Product\",\n",
    "    \"Sub-product\",\n",
    "    \"Issue\",\n",
    "    \"Sub-issue\",\n",
    "    \"Company public response\",\n",
    "    \"Company\",\n",
    "    \"State\",\n",
    "    \"Tags\",\n",
    "    \"Company response to consumer\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164034, 3440)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode categorical columns\n",
    "df = pd.get_dummies(df, columns=cat_cols)\n",
    "\n",
    "# check shape of df\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['days', 'Product_Bank account or service', 'Product_Consumer Loan',\n",
       "       'Product_Credit card', 'Product_Credit reporting',\n",
       "       'Product_Debt collection', 'Product_Money transfers',\n",
       "       'Product_Mortgage', 'Product_Other financial service',\n",
       "       'Product_Payday loan',\n",
       "       ...\n",
       "       'State_WY', 'Tags_Older American', 'Tags_Older American, Servicemember',\n",
       "       'Tags_Servicemember', 'Tags_Unknown',\n",
       "       'Company response to consumer_Closed',\n",
       "       'Company response to consumer_Closed with explanation',\n",
       "       'Company response to consumer_Closed with monetary relief',\n",
       "       'Company response to consumer_Closed with non-monetary relief',\n",
       "       'Company response to consumer_Untimely response'],\n",
       "      dtype='object', length=3440)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164034, 3740)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate narrative_w2v_matrix with df\n",
    "X = np.concatenate((narrative_w2v_matrix, df), axis=1)\n",
    "\n",
    "# check shape of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164034, 3740)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert back to dataframe\n",
    "df = pd.DataFrame(X, columns=list(range(X.shape[1])))\n",
    "\n",
    "# check shape of df\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3730</th>\n",
       "      <th>3731</th>\n",
       "      <th>3732</th>\n",
       "      <th>3733</th>\n",
       "      <th>3734</th>\n",
       "      <th>3735</th>\n",
       "      <th>3736</th>\n",
       "      <th>3737</th>\n",
       "      <th>3738</th>\n",
       "      <th>3739</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040331</td>\n",
       "      <td>0.026620</td>\n",
       "      <td>0.014841</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>-0.043637</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>-0.029973</td>\n",
       "      <td>0.068532</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015335</td>\n",
       "      <td>0.045654</td>\n",
       "      <td>0.027914</td>\n",
       "      <td>0.039537</td>\n",
       "      <td>-0.039236</td>\n",
       "      <td>-0.039606</td>\n",
       "      <td>0.051731</td>\n",
       "      <td>-0.051289</td>\n",
       "      <td>0.112583</td>\n",
       "      <td>0.051383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.032917</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.039849</td>\n",
       "      <td>-0.040777</td>\n",
       "      <td>-0.038908</td>\n",
       "      <td>0.052282</td>\n",
       "      <td>-0.054552</td>\n",
       "      <td>0.101980</td>\n",
       "      <td>0.038749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.009928</td>\n",
       "      <td>0.047897</td>\n",
       "      <td>-0.017245</td>\n",
       "      <td>0.025085</td>\n",
       "      <td>0.007024</td>\n",
       "      <td>0.019206</td>\n",
       "      <td>0.031511</td>\n",
       "      <td>-0.071070</td>\n",
       "      <td>0.180567</td>\n",
       "      <td>-0.099528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.005290</td>\n",
       "      <td>-0.038241</td>\n",
       "      <td>0.093601</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>-0.105635</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>0.062344</td>\n",
       "      <td>-0.002190</td>\n",
       "      <td>0.142828</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164029</th>\n",
       "      <td>-0.020167</td>\n",
       "      <td>-0.007511</td>\n",
       "      <td>0.017210</td>\n",
       "      <td>-0.032641</td>\n",
       "      <td>-0.034213</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.050488</td>\n",
       "      <td>-0.022190</td>\n",
       "      <td>0.127320</td>\n",
       "      <td>0.025251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164030</th>\n",
       "      <td>-0.079239</td>\n",
       "      <td>0.061995</td>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.066190</td>\n",
       "      <td>0.020204</td>\n",
       "      <td>-0.029269</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>-0.048678</td>\n",
       "      <td>0.078075</td>\n",
       "      <td>-0.017703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164031</th>\n",
       "      <td>-0.074341</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>-0.151923</td>\n",
       "      <td>0.049004</td>\n",
       "      <td>-0.062514</td>\n",
       "      <td>0.132853</td>\n",
       "      <td>-0.049582</td>\n",
       "      <td>0.063992</td>\n",
       "      <td>-0.006917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164032</th>\n",
       "      <td>-0.003679</td>\n",
       "      <td>0.042294</td>\n",
       "      <td>0.026574</td>\n",
       "      <td>0.042956</td>\n",
       "      <td>-0.058771</td>\n",
       "      <td>-0.022558</td>\n",
       "      <td>0.064927</td>\n",
       "      <td>-0.032685</td>\n",
       "      <td>0.113615</td>\n",
       "      <td>0.030233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164033</th>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.023068</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>0.018988</td>\n",
       "      <td>-0.018625</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>0.073933</td>\n",
       "      <td>-0.039302</td>\n",
       "      <td>0.111953</td>\n",
       "      <td>0.049116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164034 rows × 3740 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6     \\\n",
       "0       0.040331  0.026620  0.014841  0.014846 -0.043637  0.000967  0.057010   \n",
       "1       0.015335  0.045654  0.027914  0.039537 -0.039236 -0.039606  0.051731   \n",
       "2       0.015713  0.032917  0.015895  0.039849 -0.040777 -0.038908  0.052282   \n",
       "3      -0.009928  0.047897 -0.017245  0.025085  0.007024  0.019206  0.031511   \n",
       "4      -0.005290 -0.038241  0.093601  0.012044 -0.105635  0.013963  0.062344   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "164029 -0.020167 -0.007511  0.017210 -0.032641 -0.034213  0.006221  0.050488   \n",
       "164030 -0.079239  0.061995  0.037255  0.066190  0.020204 -0.029269  0.014772   \n",
       "164031 -0.074341  0.004706  0.005744 -0.151923  0.049004 -0.062514  0.132853   \n",
       "164032 -0.003679  0.042294  0.026574  0.042956 -0.058771 -0.022558  0.064927   \n",
       "164033  0.000032  0.023068  0.023506  0.018988 -0.018625  0.016055  0.073933   \n",
       "\n",
       "            7         8         9     ...  3730  3731  3732  3733  3734  3735  \\\n",
       "0      -0.029973  0.068532  0.010967  ...   0.0   0.0   0.0   0.0   1.0   0.0   \n",
       "1      -0.051289  0.112583  0.051383  ...   0.0   0.0   0.0   0.0   1.0   0.0   \n",
       "2      -0.054552  0.101980  0.038749  ...   0.0   0.0   0.0   0.0   1.0   0.0   \n",
       "3      -0.071070  0.180567 -0.099528  ...   0.0   0.0   0.0   0.0   1.0   0.0   \n",
       "4      -0.002190  0.142828  0.001794  ...   0.0   0.0   0.0   0.0   1.0   0.0   \n",
       "...          ...       ...       ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "164029 -0.022190  0.127320  0.025251  ...   0.0   0.0   0.0   0.0   1.0   0.0   \n",
       "164030 -0.048678  0.078075 -0.017703  ...   0.0   0.0   0.0   0.0   1.0   0.0   \n",
       "164031 -0.049582  0.063992 -0.006917  ...   0.0   0.0   0.0   0.0   1.0   0.0   \n",
       "164032 -0.032685  0.113615  0.030233  ...   0.0   0.0   0.0   0.0   1.0   0.0   \n",
       "164033 -0.039302  0.111953  0.049116  ...   0.0   0.0   0.0   0.0   1.0   0.0   \n",
       "\n",
       "        3736  3737  3738  3739  \n",
       "0        0.0   1.0   0.0   0.0  \n",
       "1        1.0   0.0   0.0   0.0  \n",
       "2        1.0   0.0   0.0   0.0  \n",
       "3        1.0   0.0   0.0   0.0  \n",
       "4        1.0   0.0   0.0   0.0  \n",
       "...      ...   ...   ...   ...  \n",
       "164029   0.0   1.0   0.0   0.0  \n",
       "164030   0.0   1.0   0.0   0.0  \n",
       "164031   1.0   0.0   0.0   0.0  \n",
       "164032   1.0   0.0   0.0   0.0  \n",
       "164033   0.0   1.0   0.0   0.0  \n",
       "\n",
       "[164034 rows x 3740 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         61.0\n",
       "1          2.0\n",
       "2          0.0\n",
       "3          0.0\n",
       "4          0.0\n",
       "          ... \n",
       "164029     0.0\n",
       "164030     0.0\n",
       "164031     0.0\n",
       "164032     0.0\n",
       "164033     0.0\n",
       "Name: 300, Length: 164034, dtype: float64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check 301th column\n",
    "\n",
    "df[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256454, 3740) (256454,)\n",
      "Yes    128227\n",
      "No     128227\n",
      "Name: Consumer disputed?, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# smote to oversample minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(df, y)\n",
    "\n",
    "# print shape of X_smote, y_smote\n",
    "print(X_smote.shape, y_smote.shape)\n",
    "\n",
    "# print value counts of y_smote\n",
    "print(pd.Series(y_smote).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode y_smote to 0 and 1\n",
    "y_smote = y_smote.map({'No': 0, 'Yes': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205163, 3740) (51291, 3740) (205163,) (51291,)\n",
      "1    102582\n",
      "0    102581\n",
      "Name: Consumer disputed?, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# train test split, 20% test, random_state=42, stratify=y_smote\n",
    "X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, stratify=y_smote, random_state=42, test_size=0.2)\n",
    "\n",
    "# print shape\n",
    "print(X_smote_train.shape, X_smote_test.shape, y_smote_train.shape, y_smote_test.shape)\n",
    "\n",
    "# print value counts of y_smote_train\n",
    "print(pd.Series(y_smote_train).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost: 0.792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:26:43] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1 score: 0.7924859264093987\n"
     ]
    }
   ],
   "source": [
    "# train with xgboost\n",
    "model_xgb = train_xgb(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightgbm: 0.781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 102582, number of negative: 102581\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.889060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 129863\n",
      "[LightGBM] [Info] Number of data points in the train set: 205163, number of used features: 1376\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000010\n",
      "[LightGBM] [Info] Start training from score 0.000010\n",
      "f1 score: 0.7811456298443505\n"
     ]
    }
   ],
   "source": [
    "# train with lightgbm\n",
    "model_lgbm_w2v_onehot = train_lgbm(X_smote_train, y_smote_train, X_smote_test, y_smote_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost: 0.814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.100046\n",
      "0:\tlearn: 0.6812282\ttotal: 151ms\tremaining: 2m 30s\n",
      "1:\tlearn: 0.6745768\ttotal: 231ms\tremaining: 1m 55s\n",
      "2:\tlearn: 0.6684506\ttotal: 306ms\tremaining: 1m 41s\n",
      "3:\tlearn: 0.6634391\ttotal: 384ms\tremaining: 1m 35s\n",
      "4:\tlearn: 0.6587479\ttotal: 466ms\tremaining: 1m 32s\n",
      "5:\tlearn: 0.6523111\ttotal: 542ms\tremaining: 1m 29s\n",
      "6:\tlearn: 0.6487270\ttotal: 618ms\tremaining: 1m 27s\n",
      "7:\tlearn: 0.6454409\ttotal: 698ms\tremaining: 1m 26s\n",
      "8:\tlearn: 0.6425357\ttotal: 773ms\tremaining: 1m 25s\n",
      "9:\tlearn: 0.6398729\ttotal: 855ms\tremaining: 1m 24s\n",
      "10:\tlearn: 0.6312922\ttotal: 935ms\tremaining: 1m 24s\n",
      "11:\tlearn: 0.6203893\ttotal: 1.01s\tremaining: 1m 23s\n",
      "12:\tlearn: 0.6180025\ttotal: 1.09s\tremaining: 1m 23s\n",
      "13:\tlearn: 0.6139492\ttotal: 1.18s\tremaining: 1m 23s\n",
      "14:\tlearn: 0.6121039\ttotal: 1.26s\tremaining: 1m 22s\n",
      "15:\tlearn: 0.6102919\ttotal: 1.34s\tremaining: 1m 22s\n",
      "16:\tlearn: 0.6087215\ttotal: 1.43s\tremaining: 1m 22s\n",
      "17:\tlearn: 0.6070896\ttotal: 1.5s\tremaining: 1m 22s\n",
      "18:\tlearn: 0.6028153\ttotal: 1.58s\tremaining: 1m 21s\n",
      "19:\tlearn: 0.6011790\ttotal: 1.67s\tremaining: 1m 21s\n",
      "20:\tlearn: 0.5940211\ttotal: 1.76s\tremaining: 1m 21s\n",
      "21:\tlearn: 0.5915135\ttotal: 1.85s\tremaining: 1m 22s\n",
      "22:\tlearn: 0.5903319\ttotal: 1.93s\tremaining: 1m 22s\n",
      "23:\tlearn: 0.5866922\ttotal: 2.02s\tremaining: 1m 22s\n",
      "24:\tlearn: 0.5853893\ttotal: 2.11s\tremaining: 1m 22s\n",
      "25:\tlearn: 0.5842279\ttotal: 2.19s\tremaining: 1m 21s\n",
      "26:\tlearn: 0.5831401\ttotal: 2.27s\tremaining: 1m 21s\n",
      "27:\tlearn: 0.5821724\ttotal: 2.36s\tremaining: 1m 21s\n",
      "28:\tlearn: 0.5812133\ttotal: 2.44s\tremaining: 1m 21s\n",
      "29:\tlearn: 0.5779641\ttotal: 2.52s\tremaining: 1m 21s\n",
      "30:\tlearn: 0.5716493\ttotal: 2.6s\tremaining: 1m 21s\n",
      "31:\tlearn: 0.5692544\ttotal: 2.68s\tremaining: 1m 21s\n",
      "32:\tlearn: 0.5668946\ttotal: 2.76s\tremaining: 1m 20s\n",
      "33:\tlearn: 0.5661180\ttotal: 2.84s\tremaining: 1m 20s\n",
      "34:\tlearn: 0.5637367\ttotal: 2.91s\tremaining: 1m 20s\n",
      "35:\tlearn: 0.5621954\ttotal: 2.99s\tremaining: 1m 20s\n",
      "36:\tlearn: 0.5581992\ttotal: 3.08s\tremaining: 1m 20s\n",
      "37:\tlearn: 0.5537014\ttotal: 3.16s\tremaining: 1m 20s\n",
      "38:\tlearn: 0.5530428\ttotal: 3.25s\tremaining: 1m 20s\n",
      "39:\tlearn: 0.5472474\ttotal: 3.34s\tremaining: 1m 20s\n",
      "40:\tlearn: 0.5465152\ttotal: 3.42s\tremaining: 1m 19s\n",
      "41:\tlearn: 0.5458126\ttotal: 3.5s\tremaining: 1m 19s\n",
      "42:\tlearn: 0.5401185\ttotal: 3.61s\tremaining: 1m 20s\n",
      "43:\tlearn: 0.5385627\ttotal: 3.71s\tremaining: 1m 20s\n",
      "44:\tlearn: 0.5378183\ttotal: 3.79s\tremaining: 1m 20s\n",
      "45:\tlearn: 0.5371816\ttotal: 3.88s\tremaining: 1m 20s\n",
      "46:\tlearn: 0.5346907\ttotal: 3.96s\tremaining: 1m 20s\n",
      "47:\tlearn: 0.5330596\ttotal: 4.06s\tremaining: 1m 20s\n",
      "48:\tlearn: 0.5322385\ttotal: 4.15s\tremaining: 1m 20s\n",
      "49:\tlearn: 0.5288891\ttotal: 4.24s\tremaining: 1m 20s\n",
      "50:\tlearn: 0.5266956\ttotal: 4.33s\tremaining: 1m 20s\n",
      "51:\tlearn: 0.5262563\ttotal: 4.42s\tremaining: 1m 20s\n",
      "52:\tlearn: 0.5257811\ttotal: 4.52s\tremaining: 1m 20s\n",
      "53:\tlearn: 0.5243568\ttotal: 4.6s\tremaining: 1m 20s\n",
      "54:\tlearn: 0.5235047\ttotal: 4.68s\tremaining: 1m 20s\n",
      "55:\tlearn: 0.5204936\ttotal: 4.76s\tremaining: 1m 20s\n",
      "56:\tlearn: 0.5181246\ttotal: 4.84s\tremaining: 1m 20s\n",
      "57:\tlearn: 0.5177338\ttotal: 4.92s\tremaining: 1m 19s\n",
      "58:\tlearn: 0.5157254\ttotal: 5s\tremaining: 1m 19s\n",
      "59:\tlearn: 0.5138587\ttotal: 5.08s\tremaining: 1m 19s\n",
      "60:\tlearn: 0.5131694\ttotal: 5.16s\tremaining: 1m 19s\n",
      "61:\tlearn: 0.5122732\ttotal: 5.23s\tremaining: 1m 19s\n",
      "62:\tlearn: 0.5109294\ttotal: 5.31s\tremaining: 1m 19s\n",
      "63:\tlearn: 0.5105831\ttotal: 5.38s\tremaining: 1m 18s\n",
      "64:\tlearn: 0.5082332\ttotal: 5.46s\tremaining: 1m 18s\n",
      "65:\tlearn: 0.5070609\ttotal: 5.53s\tremaining: 1m 18s\n",
      "66:\tlearn: 0.5067323\ttotal: 5.61s\tremaining: 1m 18s\n",
      "67:\tlearn: 0.5058801\ttotal: 5.69s\tremaining: 1m 18s\n",
      "68:\tlearn: 0.5052954\ttotal: 5.77s\tremaining: 1m 17s\n",
      "69:\tlearn: 0.5049612\ttotal: 5.85s\tremaining: 1m 17s\n",
      "70:\tlearn: 0.5029311\ttotal: 5.92s\tremaining: 1m 17s\n",
      "71:\tlearn: 0.5023042\ttotal: 6s\tremaining: 1m 17s\n",
      "72:\tlearn: 0.5018494\ttotal: 6.08s\tremaining: 1m 17s\n",
      "73:\tlearn: 0.4989590\ttotal: 6.16s\tremaining: 1m 17s\n",
      "74:\tlearn: 0.4980599\ttotal: 6.24s\tremaining: 1m 16s\n",
      "75:\tlearn: 0.4977818\ttotal: 6.32s\tremaining: 1m 16s\n",
      "76:\tlearn: 0.4967140\ttotal: 6.39s\tremaining: 1m 16s\n",
      "77:\tlearn: 0.4963922\ttotal: 6.47s\tremaining: 1m 16s\n",
      "78:\tlearn: 0.4961104\ttotal: 6.54s\tremaining: 1m 16s\n",
      "79:\tlearn: 0.4952497\ttotal: 6.62s\tremaining: 1m 16s\n",
      "80:\tlearn: 0.4941908\ttotal: 6.69s\tremaining: 1m 15s\n",
      "81:\tlearn: 0.4939180\ttotal: 6.77s\tremaining: 1m 15s\n",
      "82:\tlearn: 0.4936779\ttotal: 6.84s\tremaining: 1m 15s\n",
      "83:\tlearn: 0.4925281\ttotal: 6.91s\tremaining: 1m 15s\n",
      "84:\tlearn: 0.4917809\ttotal: 6.99s\tremaining: 1m 15s\n",
      "85:\tlearn: 0.4910139\ttotal: 7.06s\tremaining: 1m 15s\n",
      "86:\tlearn: 0.4906396\ttotal: 7.14s\tremaining: 1m 14s\n",
      "87:\tlearn: 0.4903568\ttotal: 7.21s\tremaining: 1m 14s\n",
      "88:\tlearn: 0.4899585\ttotal: 7.29s\tremaining: 1m 14s\n",
      "89:\tlearn: 0.4895502\ttotal: 7.36s\tremaining: 1m 14s\n",
      "90:\tlearn: 0.4885041\ttotal: 7.44s\tremaining: 1m 14s\n",
      "91:\tlearn: 0.4878278\ttotal: 7.51s\tremaining: 1m 14s\n",
      "92:\tlearn: 0.4861737\ttotal: 7.58s\tremaining: 1m 13s\n",
      "93:\tlearn: 0.4851403\ttotal: 7.66s\tremaining: 1m 13s\n",
      "94:\tlearn: 0.4836746\ttotal: 7.74s\tremaining: 1m 13s\n",
      "95:\tlearn: 0.4824596\ttotal: 7.81s\tremaining: 1m 13s\n",
      "96:\tlearn: 0.4815413\ttotal: 7.88s\tremaining: 1m 13s\n",
      "97:\tlearn: 0.4812672\ttotal: 7.96s\tremaining: 1m 13s\n",
      "98:\tlearn: 0.4803087\ttotal: 8.04s\tremaining: 1m 13s\n",
      "99:\tlearn: 0.4794132\ttotal: 8.11s\tremaining: 1m 12s\n",
      "100:\tlearn: 0.4788393\ttotal: 8.18s\tremaining: 1m 12s\n",
      "101:\tlearn: 0.4777293\ttotal: 8.26s\tremaining: 1m 12s\n",
      "102:\tlearn: 0.4770638\ttotal: 8.34s\tremaining: 1m 12s\n",
      "103:\tlearn: 0.4757223\ttotal: 8.42s\tremaining: 1m 12s\n",
      "104:\tlearn: 0.4753074\ttotal: 8.49s\tremaining: 1m 12s\n",
      "105:\tlearn: 0.4737060\ttotal: 8.57s\tremaining: 1m 12s\n",
      "106:\tlearn: 0.4727559\ttotal: 8.65s\tremaining: 1m 12s\n",
      "107:\tlearn: 0.4725279\ttotal: 8.73s\tremaining: 1m 12s\n",
      "108:\tlearn: 0.4718791\ttotal: 8.8s\tremaining: 1m 11s\n",
      "109:\tlearn: 0.4708394\ttotal: 8.88s\tremaining: 1m 11s\n",
      "110:\tlearn: 0.4703145\ttotal: 8.96s\tremaining: 1m 11s\n",
      "111:\tlearn: 0.4695732\ttotal: 9.03s\tremaining: 1m 11s\n",
      "112:\tlearn: 0.4688014\ttotal: 9.11s\tremaining: 1m 11s\n",
      "113:\tlearn: 0.4685622\ttotal: 9.2s\tremaining: 1m 11s\n",
      "114:\tlearn: 0.4682151\ttotal: 9.28s\tremaining: 1m 11s\n",
      "115:\tlearn: 0.4680021\ttotal: 9.36s\tremaining: 1m 11s\n",
      "116:\tlearn: 0.4675492\ttotal: 9.44s\tremaining: 1m 11s\n",
      "117:\tlearn: 0.4671254\ttotal: 9.57s\tremaining: 1m 11s\n",
      "118:\tlearn: 0.4669278\ttotal: 9.67s\tremaining: 1m 11s\n",
      "119:\tlearn: 0.4664190\ttotal: 9.74s\tremaining: 1m 11s\n",
      "120:\tlearn: 0.4655621\ttotal: 9.81s\tremaining: 1m 11s\n",
      "121:\tlearn: 0.4651293\ttotal: 9.87s\tremaining: 1m 11s\n",
      "122:\tlearn: 0.4638570\ttotal: 9.94s\tremaining: 1m 10s\n",
      "123:\tlearn: 0.4633764\ttotal: 10s\tremaining: 1m 10s\n",
      "124:\tlearn: 0.4628793\ttotal: 10.1s\tremaining: 1m 10s\n",
      "125:\tlearn: 0.4626279\ttotal: 10.2s\tremaining: 1m 10s\n",
      "126:\tlearn: 0.4615196\ttotal: 10.2s\tremaining: 1m 10s\n",
      "127:\tlearn: 0.4610961\ttotal: 10.3s\tremaining: 1m 10s\n",
      "128:\tlearn: 0.4605069\ttotal: 10.4s\tremaining: 1m 10s\n",
      "129:\tlearn: 0.4601923\ttotal: 10.4s\tremaining: 1m 9s\n",
      "130:\tlearn: 0.4593956\ttotal: 10.5s\tremaining: 1m 9s\n",
      "131:\tlearn: 0.4587564\ttotal: 10.6s\tremaining: 1m 9s\n",
      "132:\tlearn: 0.4580103\ttotal: 10.7s\tremaining: 1m 9s\n",
      "133:\tlearn: 0.4574592\ttotal: 10.7s\tremaining: 1m 9s\n",
      "134:\tlearn: 0.4571844\ttotal: 10.8s\tremaining: 1m 9s\n",
      "135:\tlearn: 0.4559020\ttotal: 10.9s\tremaining: 1m 9s\n",
      "136:\tlearn: 0.4552344\ttotal: 11s\tremaining: 1m 9s\n",
      "137:\tlearn: 0.4550141\ttotal: 11s\tremaining: 1m 8s\n",
      "138:\tlearn: 0.4548013\ttotal: 11.1s\tremaining: 1m 8s\n",
      "139:\tlearn: 0.4542907\ttotal: 11.2s\tremaining: 1m 8s\n",
      "140:\tlearn: 0.4539734\ttotal: 11.3s\tremaining: 1m 8s\n",
      "141:\tlearn: 0.4537482\ttotal: 11.4s\tremaining: 1m 8s\n",
      "142:\tlearn: 0.4533664\ttotal: 11.4s\tremaining: 1m 8s\n",
      "143:\tlearn: 0.4528781\ttotal: 11.5s\tremaining: 1m 8s\n",
      "144:\tlearn: 0.4522455\ttotal: 11.6s\tremaining: 1m 8s\n",
      "145:\tlearn: 0.4519556\ttotal: 11.7s\tremaining: 1m 8s\n",
      "146:\tlearn: 0.4516207\ttotal: 11.7s\tremaining: 1m 8s\n",
      "147:\tlearn: 0.4513081\ttotal: 11.8s\tremaining: 1m 7s\n",
      "148:\tlearn: 0.4496111\ttotal: 11.9s\tremaining: 1m 7s\n",
      "149:\tlearn: 0.4491092\ttotal: 12s\tremaining: 1m 7s\n",
      "150:\tlearn: 0.4488967\ttotal: 12s\tremaining: 1m 7s\n",
      "151:\tlearn: 0.4483738\ttotal: 12.1s\tremaining: 1m 7s\n",
      "152:\tlearn: 0.4479573\ttotal: 12.2s\tremaining: 1m 7s\n",
      "153:\tlearn: 0.4477302\ttotal: 12.3s\tremaining: 1m 7s\n",
      "154:\tlearn: 0.4473439\ttotal: 12.3s\tremaining: 1m 7s\n",
      "155:\tlearn: 0.4466282\ttotal: 12.4s\tremaining: 1m 7s\n",
      "156:\tlearn: 0.4464661\ttotal: 12.5s\tremaining: 1m 7s\n",
      "157:\tlearn: 0.4462696\ttotal: 12.6s\tremaining: 1m 6s\n",
      "158:\tlearn: 0.4461024\ttotal: 12.6s\tremaining: 1m 6s\n",
      "159:\tlearn: 0.4455111\ttotal: 12.7s\tremaining: 1m 6s\n",
      "160:\tlearn: 0.4453079\ttotal: 12.8s\tremaining: 1m 6s\n",
      "161:\tlearn: 0.4449033\ttotal: 12.9s\tremaining: 1m 6s\n",
      "162:\tlearn: 0.4446926\ttotal: 12.9s\tremaining: 1m 6s\n",
      "163:\tlearn: 0.4444212\ttotal: 13s\tremaining: 1m 6s\n",
      "164:\tlearn: 0.4440987\ttotal: 13.1s\tremaining: 1m 6s\n",
      "165:\tlearn: 0.4438561\ttotal: 13.2s\tremaining: 1m 6s\n",
      "166:\tlearn: 0.4434854\ttotal: 13.2s\tremaining: 1m 5s\n",
      "167:\tlearn: 0.4428134\ttotal: 13.3s\tremaining: 1m 5s\n",
      "168:\tlearn: 0.4426329\ttotal: 13.4s\tremaining: 1m 5s\n",
      "169:\tlearn: 0.4424647\ttotal: 13.5s\tremaining: 1m 5s\n",
      "170:\tlearn: 0.4421460\ttotal: 13.5s\tremaining: 1m 5s\n",
      "171:\tlearn: 0.4417949\ttotal: 13.6s\tremaining: 1m 5s\n",
      "172:\tlearn: 0.4413785\ttotal: 13.7s\tremaining: 1m 5s\n",
      "173:\tlearn: 0.4406895\ttotal: 13.8s\tremaining: 1m 5s\n",
      "174:\tlearn: 0.4400823\ttotal: 13.9s\tremaining: 1m 5s\n",
      "175:\tlearn: 0.4396870\ttotal: 13.9s\tremaining: 1m 5s\n",
      "176:\tlearn: 0.4395006\ttotal: 14s\tremaining: 1m 5s\n",
      "177:\tlearn: 0.4393470\ttotal: 14.1s\tremaining: 1m 5s\n",
      "178:\tlearn: 0.4387474\ttotal: 14.2s\tremaining: 1m 4s\n",
      "179:\tlearn: 0.4380925\ttotal: 14.2s\tremaining: 1m 4s\n",
      "180:\tlearn: 0.4379029\ttotal: 14.3s\tremaining: 1m 4s\n",
      "181:\tlearn: 0.4377273\ttotal: 14.4s\tremaining: 1m 4s\n",
      "182:\tlearn: 0.4368569\ttotal: 14.5s\tremaining: 1m 4s\n",
      "183:\tlearn: 0.4364421\ttotal: 14.5s\tremaining: 1m 4s\n",
      "184:\tlearn: 0.4361789\ttotal: 14.6s\tremaining: 1m 4s\n",
      "185:\tlearn: 0.4360475\ttotal: 14.7s\tremaining: 1m 4s\n",
      "186:\tlearn: 0.4359095\ttotal: 14.8s\tremaining: 1m 4s\n",
      "187:\tlearn: 0.4356274\ttotal: 14.9s\tremaining: 1m 4s\n",
      "188:\tlearn: 0.4351694\ttotal: 14.9s\tremaining: 1m 4s\n",
      "189:\tlearn: 0.4347984\ttotal: 15s\tremaining: 1m 3s\n",
      "190:\tlearn: 0.4346244\ttotal: 15.1s\tremaining: 1m 3s\n",
      "191:\tlearn: 0.4340823\ttotal: 15.2s\tremaining: 1m 3s\n",
      "192:\tlearn: 0.4336804\ttotal: 15.2s\tremaining: 1m 3s\n",
      "193:\tlearn: 0.4334853\ttotal: 15.3s\tremaining: 1m 3s\n",
      "194:\tlearn: 0.4333362\ttotal: 15.4s\tremaining: 1m 3s\n",
      "195:\tlearn: 0.4329099\ttotal: 15.5s\tremaining: 1m 3s\n",
      "196:\tlearn: 0.4324864\ttotal: 15.5s\tremaining: 1m 3s\n",
      "197:\tlearn: 0.4320272\ttotal: 15.6s\tremaining: 1m 3s\n",
      "198:\tlearn: 0.4318267\ttotal: 15.7s\tremaining: 1m 3s\n",
      "199:\tlearn: 0.4315334\ttotal: 15.8s\tremaining: 1m 3s\n",
      "200:\tlearn: 0.4313774\ttotal: 15.9s\tremaining: 1m 3s\n",
      "201:\tlearn: 0.4312322\ttotal: 15.9s\tremaining: 1m 2s\n",
      "202:\tlearn: 0.4310428\ttotal: 16s\tremaining: 1m 2s\n",
      "203:\tlearn: 0.4308917\ttotal: 16.1s\tremaining: 1m 2s\n",
      "204:\tlearn: 0.4306772\ttotal: 16.2s\tremaining: 1m 2s\n",
      "205:\tlearn: 0.4299421\ttotal: 16.3s\tremaining: 1m 2s\n",
      "206:\tlearn: 0.4296878\ttotal: 16.3s\tremaining: 1m 2s\n",
      "207:\tlearn: 0.4295463\ttotal: 16.4s\tremaining: 1m 2s\n",
      "208:\tlearn: 0.4293322\ttotal: 16.5s\tremaining: 1m 2s\n",
      "209:\tlearn: 0.4291979\ttotal: 16.6s\tremaining: 1m 2s\n",
      "210:\tlearn: 0.4290846\ttotal: 16.7s\tremaining: 1m 2s\n",
      "211:\tlearn: 0.4289505\ttotal: 16.8s\tremaining: 1m 2s\n",
      "212:\tlearn: 0.4288169\ttotal: 16.8s\tremaining: 1m 2s\n",
      "213:\tlearn: 0.4284052\ttotal: 16.9s\tremaining: 1m 2s\n",
      "214:\tlearn: 0.4282322\ttotal: 17s\tremaining: 1m 1s\n",
      "215:\tlearn: 0.4280844\ttotal: 17.1s\tremaining: 1m 1s\n",
      "216:\tlearn: 0.4279462\ttotal: 17.1s\tremaining: 1m 1s\n",
      "217:\tlearn: 0.4273518\ttotal: 17.2s\tremaining: 1m 1s\n",
      "218:\tlearn: 0.4272024\ttotal: 17.3s\tremaining: 1m 1s\n",
      "219:\tlearn: 0.4268859\ttotal: 17.4s\tremaining: 1m 1s\n",
      "220:\tlearn: 0.4264398\ttotal: 17.5s\tremaining: 1m 1s\n",
      "221:\tlearn: 0.4262931\ttotal: 17.5s\tremaining: 1m 1s\n",
      "222:\tlearn: 0.4261632\ttotal: 17.6s\tremaining: 1m 1s\n",
      "223:\tlearn: 0.4259985\ttotal: 17.7s\tremaining: 1m 1s\n",
      "224:\tlearn: 0.4258842\ttotal: 17.8s\tremaining: 1m 1s\n",
      "225:\tlearn: 0.4252351\ttotal: 17.9s\tremaining: 1m 1s\n",
      "226:\tlearn: 0.4242379\ttotal: 17.9s\tremaining: 1m 1s\n",
      "227:\tlearn: 0.4241188\ttotal: 18s\tremaining: 1m\n",
      "228:\tlearn: 0.4239785\ttotal: 18.1s\tremaining: 1m\n",
      "229:\tlearn: 0.4238306\ttotal: 18.2s\tremaining: 1m\n",
      "230:\tlearn: 0.4237002\ttotal: 18.2s\tremaining: 1m\n",
      "231:\tlearn: 0.4235507\ttotal: 18.3s\tremaining: 1m\n",
      "232:\tlearn: 0.4233831\ttotal: 18.4s\tremaining: 1m\n",
      "233:\tlearn: 0.4232532\ttotal: 18.5s\tremaining: 1m\n",
      "234:\tlearn: 0.4227139\ttotal: 18.6s\tremaining: 1m\n",
      "235:\tlearn: 0.4220260\ttotal: 18.7s\tremaining: 1m\n",
      "236:\tlearn: 0.4218721\ttotal: 18.7s\tremaining: 1m\n",
      "237:\tlearn: 0.4214241\ttotal: 18.8s\tremaining: 1m\n",
      "238:\tlearn: 0.4212487\ttotal: 18.9s\tremaining: 1m\n",
      "239:\tlearn: 0.4210923\ttotal: 19s\tremaining: 1m\n",
      "240:\tlearn: 0.4209328\ttotal: 19.1s\tremaining: 1m\n",
      "241:\tlearn: 0.4206263\ttotal: 19.2s\tremaining: 1m\n",
      "242:\tlearn: 0.4202521\ttotal: 19.2s\tremaining: 59.9s\n",
      "243:\tlearn: 0.4201572\ttotal: 19.3s\tremaining: 59.8s\n",
      "244:\tlearn: 0.4200503\ttotal: 19.4s\tremaining: 59.8s\n",
      "245:\tlearn: 0.4197376\ttotal: 19.5s\tremaining: 59.7s\n",
      "246:\tlearn: 0.4192514\ttotal: 19.6s\tremaining: 59.6s\n",
      "247:\tlearn: 0.4190103\ttotal: 19.6s\tremaining: 59.5s\n",
      "248:\tlearn: 0.4189034\ttotal: 19.7s\tremaining: 59.5s\n",
      "249:\tlearn: 0.4187997\ttotal: 19.8s\tremaining: 59.4s\n",
      "250:\tlearn: 0.4186172\ttotal: 19.9s\tremaining: 59.3s\n",
      "251:\tlearn: 0.4185040\ttotal: 20s\tremaining: 59.3s\n",
      "252:\tlearn: 0.4183740\ttotal: 20.1s\tremaining: 59.2s\n",
      "253:\tlearn: 0.4181489\ttotal: 20.1s\tremaining: 59.1s\n",
      "254:\tlearn: 0.4180212\ttotal: 20.2s\tremaining: 59.1s\n",
      "255:\tlearn: 0.4174398\ttotal: 20.3s\tremaining: 59.1s\n",
      "256:\tlearn: 0.4172193\ttotal: 20.4s\tremaining: 59s\n",
      "257:\tlearn: 0.4170131\ttotal: 20.5s\tremaining: 58.9s\n",
      "258:\tlearn: 0.4168676\ttotal: 20.6s\tremaining: 58.9s\n",
      "259:\tlearn: 0.4167300\ttotal: 20.7s\tremaining: 58.8s\n",
      "260:\tlearn: 0.4165421\ttotal: 20.7s\tremaining: 58.7s\n",
      "261:\tlearn: 0.4163016\ttotal: 20.8s\tremaining: 58.7s\n",
      "262:\tlearn: 0.4161816\ttotal: 20.9s\tremaining: 58.6s\n",
      "263:\tlearn: 0.4156847\ttotal: 21s\tremaining: 58.6s\n",
      "264:\tlearn: 0.4152244\ttotal: 21.1s\tremaining: 58.5s\n",
      "265:\tlearn: 0.4151130\ttotal: 21.2s\tremaining: 58.4s\n",
      "266:\tlearn: 0.4149266\ttotal: 21.2s\tremaining: 58.3s\n",
      "267:\tlearn: 0.4148016\ttotal: 21.3s\tremaining: 58.3s\n",
      "268:\tlearn: 0.4146831\ttotal: 21.4s\tremaining: 58.2s\n",
      "269:\tlearn: 0.4145516\ttotal: 21.5s\tremaining: 58.2s\n",
      "270:\tlearn: 0.4144586\ttotal: 21.6s\tremaining: 58.1s\n",
      "271:\tlearn: 0.4143345\ttotal: 21.7s\tremaining: 58.1s\n",
      "272:\tlearn: 0.4142368\ttotal: 21.8s\tremaining: 58s\n",
      "273:\tlearn: 0.4141063\ttotal: 21.9s\tremaining: 58s\n",
      "274:\tlearn: 0.4139988\ttotal: 22s\tremaining: 57.9s\n",
      "275:\tlearn: 0.4138852\ttotal: 22s\tremaining: 57.8s\n",
      "276:\tlearn: 0.4136932\ttotal: 22.1s\tremaining: 57.8s\n",
      "277:\tlearn: 0.4135917\ttotal: 22.2s\tremaining: 57.7s\n",
      "278:\tlearn: 0.4134898\ttotal: 22.3s\tremaining: 57.7s\n",
      "279:\tlearn: 0.4132918\ttotal: 22.4s\tremaining: 57.6s\n",
      "280:\tlearn: 0.4130780\ttotal: 22.5s\tremaining: 57.5s\n",
      "281:\tlearn: 0.4126820\ttotal: 22.6s\tremaining: 57.5s\n",
      "282:\tlearn: 0.4125458\ttotal: 22.6s\tremaining: 57.4s\n",
      "283:\tlearn: 0.4123674\ttotal: 22.7s\tremaining: 57.3s\n",
      "284:\tlearn: 0.4122273\ttotal: 22.8s\tremaining: 57.3s\n",
      "285:\tlearn: 0.4119054\ttotal: 22.9s\tremaining: 57.2s\n",
      "286:\tlearn: 0.4117594\ttotal: 23s\tremaining: 57.1s\n",
      "287:\tlearn: 0.4116079\ttotal: 23.1s\tremaining: 57s\n",
      "288:\tlearn: 0.4115167\ttotal: 23.2s\tremaining: 57s\n",
      "289:\tlearn: 0.4114027\ttotal: 23.2s\tremaining: 56.9s\n",
      "290:\tlearn: 0.4111508\ttotal: 23.3s\tremaining: 56.8s\n",
      "291:\tlearn: 0.4110415\ttotal: 23.4s\tremaining: 56.8s\n",
      "292:\tlearn: 0.4109298\ttotal: 23.5s\tremaining: 56.7s\n",
      "293:\tlearn: 0.4108087\ttotal: 23.6s\tremaining: 56.7s\n",
      "294:\tlearn: 0.4106849\ttotal: 23.7s\tremaining: 56.6s\n",
      "295:\tlearn: 0.4105557\ttotal: 23.8s\tremaining: 56.6s\n",
      "296:\tlearn: 0.4102359\ttotal: 23.9s\tremaining: 56.5s\n",
      "297:\tlearn: 0.4101260\ttotal: 24s\tremaining: 56.4s\n",
      "298:\tlearn: 0.4098934\ttotal: 24s\tremaining: 56.4s\n",
      "299:\tlearn: 0.4097119\ttotal: 24.1s\tremaining: 56.3s\n",
      "300:\tlearn: 0.4096157\ttotal: 24.2s\tremaining: 56.2s\n",
      "301:\tlearn: 0.4094997\ttotal: 24.3s\tremaining: 56.2s\n",
      "302:\tlearn: 0.4093895\ttotal: 24.4s\tremaining: 56.2s\n",
      "303:\tlearn: 0.4093159\ttotal: 24.5s\tremaining: 56.1s\n",
      "304:\tlearn: 0.4091936\ttotal: 24.6s\tremaining: 56.1s\n",
      "305:\tlearn: 0.4090546\ttotal: 24.7s\tremaining: 56s\n",
      "306:\tlearn: 0.4089396\ttotal: 24.9s\tremaining: 56.1s\n",
      "307:\tlearn: 0.4088489\ttotal: 24.9s\tremaining: 56.1s\n",
      "308:\tlearn: 0.4087076\ttotal: 25s\tremaining: 56s\n",
      "309:\tlearn: 0.4085953\ttotal: 25.1s\tremaining: 55.9s\n",
      "310:\tlearn: 0.4084986\ttotal: 25.2s\tremaining: 55.8s\n",
      "311:\tlearn: 0.4079365\ttotal: 25.3s\tremaining: 55.8s\n",
      "312:\tlearn: 0.4077256\ttotal: 25.4s\tremaining: 55.7s\n",
      "313:\tlearn: 0.4076437\ttotal: 25.5s\tremaining: 55.6s\n",
      "314:\tlearn: 0.4074615\ttotal: 25.5s\tremaining: 55.5s\n",
      "315:\tlearn: 0.4072600\ttotal: 25.6s\tremaining: 55.5s\n",
      "316:\tlearn: 0.4071430\ttotal: 25.7s\tremaining: 55.4s\n",
      "317:\tlearn: 0.4069165\ttotal: 25.8s\tremaining: 55.4s\n",
      "318:\tlearn: 0.4068274\ttotal: 25.9s\tremaining: 55.3s\n",
      "319:\tlearn: 0.4067459\ttotal: 26s\tremaining: 55.2s\n",
      "320:\tlearn: 0.4066518\ttotal: 26.1s\tremaining: 55.1s\n",
      "321:\tlearn: 0.4064432\ttotal: 26.1s\tremaining: 55s\n",
      "322:\tlearn: 0.4063440\ttotal: 26.2s\tremaining: 55s\n",
      "323:\tlearn: 0.4061098\ttotal: 26.3s\tremaining: 54.9s\n",
      "324:\tlearn: 0.4060249\ttotal: 26.4s\tremaining: 54.8s\n",
      "325:\tlearn: 0.4059245\ttotal: 26.5s\tremaining: 54.8s\n",
      "326:\tlearn: 0.4058108\ttotal: 26.6s\tremaining: 54.7s\n",
      "327:\tlearn: 0.4055692\ttotal: 26.6s\tremaining: 54.6s\n",
      "328:\tlearn: 0.4054241\ttotal: 26.7s\tremaining: 54.5s\n",
      "329:\tlearn: 0.4053152\ttotal: 26.8s\tremaining: 54.5s\n",
      "330:\tlearn: 0.4050278\ttotal: 26.9s\tremaining: 54.4s\n",
      "331:\tlearn: 0.4049166\ttotal: 27s\tremaining: 54.3s\n",
      "332:\tlearn: 0.4048020\ttotal: 27.1s\tremaining: 54.2s\n",
      "333:\tlearn: 0.4047062\ttotal: 27.2s\tremaining: 54.1s\n",
      "334:\tlearn: 0.4045906\ttotal: 27.2s\tremaining: 54.1s\n",
      "335:\tlearn: 0.4044801\ttotal: 27.3s\tremaining: 54s\n",
      "336:\tlearn: 0.4043754\ttotal: 27.4s\tremaining: 53.9s\n",
      "337:\tlearn: 0.4042499\ttotal: 27.5s\tremaining: 53.9s\n",
      "338:\tlearn: 0.4041456\ttotal: 27.6s\tremaining: 53.8s\n",
      "339:\tlearn: 0.4040592\ttotal: 27.7s\tremaining: 53.7s\n",
      "340:\tlearn: 0.4038680\ttotal: 27.7s\tremaining: 53.6s\n",
      "341:\tlearn: 0.4037623\ttotal: 27.8s\tremaining: 53.6s\n",
      "342:\tlearn: 0.4036234\ttotal: 27.9s\tremaining: 53.5s\n",
      "343:\tlearn: 0.4035055\ttotal: 28s\tremaining: 53.4s\n",
      "344:\tlearn: 0.4033979\ttotal: 28.1s\tremaining: 53.3s\n",
      "345:\tlearn: 0.4033004\ttotal: 28.2s\tremaining: 53.3s\n",
      "346:\tlearn: 0.4031914\ttotal: 28.3s\tremaining: 53.2s\n",
      "347:\tlearn: 0.4030286\ttotal: 28.4s\tremaining: 53.1s\n",
      "348:\tlearn: 0.4029185\ttotal: 28.4s\tremaining: 53s\n",
      "349:\tlearn: 0.4028316\ttotal: 28.5s\tremaining: 53s\n",
      "350:\tlearn: 0.4026973\ttotal: 28.6s\tremaining: 52.9s\n",
      "351:\tlearn: 0.4025816\ttotal: 28.7s\tremaining: 52.8s\n",
      "352:\tlearn: 0.4024829\ttotal: 28.8s\tremaining: 52.8s\n",
      "353:\tlearn: 0.4022379\ttotal: 28.9s\tremaining: 52.7s\n",
      "354:\tlearn: 0.4020685\ttotal: 29s\tremaining: 52.6s\n",
      "355:\tlearn: 0.4014315\ttotal: 29.1s\tremaining: 52.6s\n",
      "356:\tlearn: 0.4013628\ttotal: 29.1s\tremaining: 52.5s\n",
      "357:\tlearn: 0.4012561\ttotal: 29.2s\tremaining: 52.4s\n",
      "358:\tlearn: 0.4011401\ttotal: 29.3s\tremaining: 52.4s\n",
      "359:\tlearn: 0.4010495\ttotal: 29.4s\tremaining: 52.3s\n",
      "360:\tlearn: 0.4009568\ttotal: 29.5s\tremaining: 52.2s\n",
      "361:\tlearn: 0.4008078\ttotal: 29.6s\tremaining: 52.2s\n",
      "362:\tlearn: 0.4007082\ttotal: 29.7s\tremaining: 52.1s\n",
      "363:\tlearn: 0.4005281\ttotal: 29.8s\tremaining: 52s\n",
      "364:\tlearn: 0.4003778\ttotal: 29.8s\tremaining: 51.9s\n",
      "365:\tlearn: 0.4002656\ttotal: 29.9s\tremaining: 51.8s\n",
      "366:\tlearn: 0.4001638\ttotal: 30s\tremaining: 51.8s\n",
      "367:\tlearn: 0.4000618\ttotal: 30.1s\tremaining: 51.7s\n",
      "368:\tlearn: 0.3999713\ttotal: 30.2s\tremaining: 51.6s\n",
      "369:\tlearn: 0.3997588\ttotal: 30.3s\tremaining: 51.5s\n",
      "370:\tlearn: 0.3995568\ttotal: 30.3s\tremaining: 51.4s\n",
      "371:\tlearn: 0.3994589\ttotal: 30.4s\tremaining: 51.4s\n",
      "372:\tlearn: 0.3993596\ttotal: 30.5s\tremaining: 51.3s\n",
      "373:\tlearn: 0.3992812\ttotal: 30.6s\tremaining: 51.2s\n",
      "374:\tlearn: 0.3992281\ttotal: 30.7s\tremaining: 51.1s\n",
      "375:\tlearn: 0.3991487\ttotal: 30.8s\tremaining: 51s\n",
      "376:\tlearn: 0.3990580\ttotal: 30.8s\tremaining: 51s\n",
      "377:\tlearn: 0.3989512\ttotal: 30.9s\tremaining: 50.9s\n",
      "378:\tlearn: 0.3988645\ttotal: 31s\tremaining: 50.8s\n",
      "379:\tlearn: 0.3987569\ttotal: 31.1s\tremaining: 50.8s\n",
      "380:\tlearn: 0.3982615\ttotal: 31.2s\tremaining: 50.7s\n",
      "381:\tlearn: 0.3981491\ttotal: 31.3s\tremaining: 50.6s\n",
      "382:\tlearn: 0.3980400\ttotal: 31.4s\tremaining: 50.6s\n",
      "383:\tlearn: 0.3979440\ttotal: 31.5s\tremaining: 50.5s\n",
      "384:\tlearn: 0.3978233\ttotal: 31.5s\tremaining: 50.4s\n",
      "385:\tlearn: 0.3977223\ttotal: 31.6s\tremaining: 50.3s\n",
      "386:\tlearn: 0.3976231\ttotal: 31.7s\tremaining: 50.3s\n",
      "387:\tlearn: 0.3974980\ttotal: 31.8s\tremaining: 50.2s\n",
      "388:\tlearn: 0.3973986\ttotal: 31.9s\tremaining: 50.1s\n",
      "389:\tlearn: 0.3972965\ttotal: 32s\tremaining: 50s\n",
      "390:\tlearn: 0.3971956\ttotal: 32.1s\tremaining: 50s\n",
      "391:\tlearn: 0.3970926\ttotal: 32.2s\tremaining: 49.9s\n",
      "392:\tlearn: 0.3970409\ttotal: 32.2s\tremaining: 49.8s\n",
      "393:\tlearn: 0.3969191\ttotal: 32.3s\tremaining: 49.7s\n",
      "394:\tlearn: 0.3968087\ttotal: 32.4s\tremaining: 49.7s\n",
      "395:\tlearn: 0.3966990\ttotal: 32.5s\tremaining: 49.6s\n",
      "396:\tlearn: 0.3966264\ttotal: 32.6s\tremaining: 49.5s\n",
      "397:\tlearn: 0.3965203\ttotal: 32.7s\tremaining: 49.5s\n",
      "398:\tlearn: 0.3964387\ttotal: 32.8s\tremaining: 49.4s\n",
      "399:\tlearn: 0.3963362\ttotal: 32.9s\tremaining: 49.3s\n",
      "400:\tlearn: 0.3962086\ttotal: 32.9s\tremaining: 49.2s\n",
      "401:\tlearn: 0.3961266\ttotal: 33s\tremaining: 49.1s\n",
      "402:\tlearn: 0.3960313\ttotal: 33.1s\tremaining: 49s\n",
      "403:\tlearn: 0.3958809\ttotal: 33.2s\tremaining: 49s\n",
      "404:\tlearn: 0.3957817\ttotal: 33.3s\tremaining: 48.9s\n",
      "405:\tlearn: 0.3956929\ttotal: 33.4s\tremaining: 48.8s\n",
      "406:\tlearn: 0.3955957\ttotal: 33.5s\tremaining: 48.8s\n",
      "407:\tlearn: 0.3954836\ttotal: 33.6s\tremaining: 48.7s\n",
      "408:\tlearn: 0.3953417\ttotal: 33.7s\tremaining: 48.6s\n",
      "409:\tlearn: 0.3951894\ttotal: 33.7s\tremaining: 48.6s\n",
      "410:\tlearn: 0.3951020\ttotal: 33.8s\tremaining: 48.5s\n",
      "411:\tlearn: 0.3950063\ttotal: 33.9s\tremaining: 48.4s\n",
      "412:\tlearn: 0.3949200\ttotal: 34s\tremaining: 48.3s\n",
      "413:\tlearn: 0.3947815\ttotal: 34.1s\tremaining: 48.3s\n",
      "414:\tlearn: 0.3946964\ttotal: 34.2s\tremaining: 48.2s\n",
      "415:\tlearn: 0.3945709\ttotal: 34.3s\tremaining: 48.1s\n",
      "416:\tlearn: 0.3944794\ttotal: 34.4s\tremaining: 48s\n",
      "417:\tlearn: 0.3943257\ttotal: 34.4s\tremaining: 48s\n",
      "418:\tlearn: 0.3940909\ttotal: 34.5s\tremaining: 47.9s\n",
      "419:\tlearn: 0.3939480\ttotal: 34.6s\tremaining: 47.8s\n",
      "420:\tlearn: 0.3938696\ttotal: 34.7s\tremaining: 47.7s\n",
      "421:\tlearn: 0.3937480\ttotal: 34.8s\tremaining: 47.7s\n",
      "422:\tlearn: 0.3936201\ttotal: 34.9s\tremaining: 47.6s\n",
      "423:\tlearn: 0.3935018\ttotal: 35s\tremaining: 47.5s\n",
      "424:\tlearn: 0.3933874\ttotal: 35.1s\tremaining: 47.4s\n",
      "425:\tlearn: 0.3932339\ttotal: 35.2s\tremaining: 47.4s\n",
      "426:\tlearn: 0.3931390\ttotal: 35.3s\tremaining: 47.3s\n",
      "427:\tlearn: 0.3930488\ttotal: 35.3s\tremaining: 47.2s\n",
      "428:\tlearn: 0.3929704\ttotal: 35.4s\tremaining: 47.2s\n",
      "429:\tlearn: 0.3928670\ttotal: 35.5s\tremaining: 47.1s\n",
      "430:\tlearn: 0.3927404\ttotal: 35.6s\tremaining: 47s\n",
      "431:\tlearn: 0.3926488\ttotal: 35.7s\tremaining: 46.9s\n",
      "432:\tlearn: 0.3925546\ttotal: 35.8s\tremaining: 46.9s\n",
      "433:\tlearn: 0.3924492\ttotal: 35.9s\tremaining: 46.8s\n",
      "434:\tlearn: 0.3923631\ttotal: 36s\tremaining: 46.7s\n",
      "435:\tlearn: 0.3922899\ttotal: 36.1s\tremaining: 46.6s\n",
      "436:\tlearn: 0.3922082\ttotal: 36.1s\tremaining: 46.6s\n",
      "437:\tlearn: 0.3921180\ttotal: 36.2s\tremaining: 46.5s\n",
      "438:\tlearn: 0.3920295\ttotal: 36.3s\tremaining: 46.4s\n",
      "439:\tlearn: 0.3919526\ttotal: 36.4s\tremaining: 46.3s\n",
      "440:\tlearn: 0.3916133\ttotal: 36.5s\tremaining: 46.3s\n",
      "441:\tlearn: 0.3915154\ttotal: 36.6s\tremaining: 46.2s\n",
      "442:\tlearn: 0.3914073\ttotal: 36.7s\tremaining: 46.2s\n",
      "443:\tlearn: 0.3913189\ttotal: 36.8s\tremaining: 46.1s\n",
      "444:\tlearn: 0.3912073\ttotal: 36.9s\tremaining: 46s\n",
      "445:\tlearn: 0.3910775\ttotal: 37s\tremaining: 45.9s\n",
      "446:\tlearn: 0.3910012\ttotal: 37.1s\tremaining: 45.9s\n",
      "447:\tlearn: 0.3909180\ttotal: 37.2s\tremaining: 45.8s\n",
      "448:\tlearn: 0.3908176\ttotal: 37.3s\tremaining: 45.7s\n",
      "449:\tlearn: 0.3907116\ttotal: 37.3s\tremaining: 45.6s\n",
      "450:\tlearn: 0.3906438\ttotal: 37.4s\tremaining: 45.6s\n",
      "451:\tlearn: 0.3905406\ttotal: 37.5s\tremaining: 45.5s\n",
      "452:\tlearn: 0.3904592\ttotal: 37.6s\tremaining: 45.4s\n",
      "453:\tlearn: 0.3902149\ttotal: 37.7s\tremaining: 45.3s\n",
      "454:\tlearn: 0.3899022\ttotal: 37.8s\tremaining: 45.3s\n",
      "455:\tlearn: 0.3898033\ttotal: 37.9s\tremaining: 45.2s\n",
      "456:\tlearn: 0.3896968\ttotal: 38s\tremaining: 45.1s\n",
      "457:\tlearn: 0.3895900\ttotal: 38.1s\tremaining: 45s\n",
      "458:\tlearn: 0.3895010\ttotal: 38.2s\tremaining: 45s\n",
      "459:\tlearn: 0.3894090\ttotal: 38.3s\tremaining: 44.9s\n",
      "460:\tlearn: 0.3893367\ttotal: 38.3s\tremaining: 44.8s\n",
      "461:\tlearn: 0.3892140\ttotal: 38.4s\tremaining: 44.7s\n",
      "462:\tlearn: 0.3891185\ttotal: 38.5s\tremaining: 44.7s\n",
      "463:\tlearn: 0.3890250\ttotal: 38.6s\tremaining: 44.6s\n",
      "464:\tlearn: 0.3887125\ttotal: 38.7s\tremaining: 44.5s\n",
      "465:\tlearn: 0.3886350\ttotal: 38.8s\tremaining: 44.5s\n",
      "466:\tlearn: 0.3885436\ttotal: 38.9s\tremaining: 44.4s\n",
      "467:\tlearn: 0.3884672\ttotal: 39s\tremaining: 44.3s\n",
      "468:\tlearn: 0.3883812\ttotal: 39.1s\tremaining: 44.2s\n",
      "469:\tlearn: 0.3882903\ttotal: 39.2s\tremaining: 44.2s\n",
      "470:\tlearn: 0.3881943\ttotal: 39.3s\tremaining: 44.1s\n",
      "471:\tlearn: 0.3881100\ttotal: 39.3s\tremaining: 44s\n",
      "472:\tlearn: 0.3880355\ttotal: 39.4s\tremaining: 43.9s\n",
      "473:\tlearn: 0.3879468\ttotal: 39.5s\tremaining: 43.8s\n",
      "474:\tlearn: 0.3877759\ttotal: 39.6s\tremaining: 43.8s\n",
      "475:\tlearn: 0.3875147\ttotal: 39.7s\tremaining: 43.7s\n",
      "476:\tlearn: 0.3874308\ttotal: 39.8s\tremaining: 43.6s\n",
      "477:\tlearn: 0.3873404\ttotal: 39.9s\tremaining: 43.5s\n",
      "478:\tlearn: 0.3872454\ttotal: 40s\tremaining: 43.5s\n",
      "479:\tlearn: 0.3871459\ttotal: 40.1s\tremaining: 43.4s\n",
      "480:\tlearn: 0.3870688\ttotal: 40.1s\tremaining: 43.3s\n",
      "481:\tlearn: 0.3869830\ttotal: 40.2s\tremaining: 43.2s\n",
      "482:\tlearn: 0.3868764\ttotal: 40.3s\tremaining: 43.2s\n",
      "483:\tlearn: 0.3866352\ttotal: 40.4s\tremaining: 43.1s\n",
      "484:\tlearn: 0.3865495\ttotal: 40.5s\tremaining: 43s\n",
      "485:\tlearn: 0.3864836\ttotal: 40.6s\tremaining: 42.9s\n",
      "486:\tlearn: 0.3860762\ttotal: 40.7s\tremaining: 42.8s\n",
      "487:\tlearn: 0.3859801\ttotal: 40.8s\tremaining: 42.8s\n",
      "488:\tlearn: 0.3857073\ttotal: 40.9s\tremaining: 42.7s\n",
      "489:\tlearn: 0.3856172\ttotal: 41s\tremaining: 42.6s\n",
      "490:\tlearn: 0.3854655\ttotal: 41.1s\tremaining: 42.6s\n",
      "491:\tlearn: 0.3853851\ttotal: 41.2s\tremaining: 42.5s\n",
      "492:\tlearn: 0.3852914\ttotal: 41.2s\tremaining: 42.4s\n",
      "493:\tlearn: 0.3851936\ttotal: 41.3s\tremaining: 42.4s\n",
      "494:\tlearn: 0.3851019\ttotal: 41.4s\tremaining: 42.3s\n",
      "495:\tlearn: 0.3849710\ttotal: 41.5s\tremaining: 42.2s\n",
      "496:\tlearn: 0.3848869\ttotal: 41.6s\tremaining: 42.1s\n",
      "497:\tlearn: 0.3847820\ttotal: 41.7s\tremaining: 42s\n",
      "498:\tlearn: 0.3847011\ttotal: 41.8s\tremaining: 41.9s\n",
      "499:\tlearn: 0.3846222\ttotal: 41.9s\tremaining: 41.9s\n",
      "500:\tlearn: 0.3845107\ttotal: 42s\tremaining: 41.8s\n",
      "501:\tlearn: 0.3844393\ttotal: 42.1s\tremaining: 41.7s\n",
      "502:\tlearn: 0.3843567\ttotal: 42.1s\tremaining: 41.6s\n",
      "503:\tlearn: 0.3842582\ttotal: 42.2s\tremaining: 41.6s\n",
      "504:\tlearn: 0.3841648\ttotal: 42.3s\tremaining: 41.5s\n",
      "505:\tlearn: 0.3840840\ttotal: 42.4s\tremaining: 41.4s\n",
      "506:\tlearn: 0.3839995\ttotal: 42.5s\tremaining: 41.3s\n",
      "507:\tlearn: 0.3839148\ttotal: 42.6s\tremaining: 41.2s\n",
      "508:\tlearn: 0.3838288\ttotal: 42.7s\tremaining: 41.2s\n",
      "509:\tlearn: 0.3837981\ttotal: 42.8s\tremaining: 41.1s\n",
      "510:\tlearn: 0.3836425\ttotal: 42.8s\tremaining: 41s\n",
      "511:\tlearn: 0.3835596\ttotal: 42.9s\tremaining: 40.9s\n",
      "512:\tlearn: 0.3834733\ttotal: 43s\tremaining: 40.8s\n",
      "513:\tlearn: 0.3833717\ttotal: 43.1s\tremaining: 40.8s\n",
      "514:\tlearn: 0.3832900\ttotal: 43.2s\tremaining: 40.7s\n",
      "515:\tlearn: 0.3831629\ttotal: 43.3s\tremaining: 40.6s\n",
      "516:\tlearn: 0.3831022\ttotal: 43.4s\tremaining: 40.5s\n",
      "517:\tlearn: 0.3830122\ttotal: 43.5s\tremaining: 40.4s\n",
      "518:\tlearn: 0.3829075\ttotal: 43.6s\tremaining: 40.4s\n",
      "519:\tlearn: 0.3828634\ttotal: 43.6s\tremaining: 40.3s\n",
      "520:\tlearn: 0.3827679\ttotal: 43.8s\tremaining: 40.2s\n",
      "521:\tlearn: 0.3826791\ttotal: 43.8s\tremaining: 40.1s\n",
      "522:\tlearn: 0.3825965\ttotal: 43.9s\tremaining: 40.1s\n",
      "523:\tlearn: 0.3825258\ttotal: 44s\tremaining: 40s\n",
      "524:\tlearn: 0.3824337\ttotal: 44.1s\tremaining: 39.9s\n",
      "525:\tlearn: 0.3823475\ttotal: 44.2s\tremaining: 39.8s\n",
      "526:\tlearn: 0.3822468\ttotal: 44.3s\tremaining: 39.8s\n",
      "527:\tlearn: 0.3821795\ttotal: 44.4s\tremaining: 39.7s\n",
      "528:\tlearn: 0.3820920\ttotal: 44.5s\tremaining: 39.6s\n",
      "529:\tlearn: 0.3819904\ttotal: 44.6s\tremaining: 39.5s\n",
      "530:\tlearn: 0.3818980\ttotal: 44.7s\tremaining: 39.5s\n",
      "531:\tlearn: 0.3818169\ttotal: 44.8s\tremaining: 39.4s\n",
      "532:\tlearn: 0.3817513\ttotal: 44.9s\tremaining: 39.4s\n",
      "533:\tlearn: 0.3816542\ttotal: 45s\tremaining: 39.3s\n",
      "534:\tlearn: 0.3815739\ttotal: 45.1s\tremaining: 39.2s\n",
      "535:\tlearn: 0.3815144\ttotal: 45.2s\tremaining: 39.1s\n",
      "536:\tlearn: 0.3814442\ttotal: 45.3s\tremaining: 39s\n",
      "537:\tlearn: 0.3813624\ttotal: 45.4s\tremaining: 39s\n",
      "538:\tlearn: 0.3812972\ttotal: 45.5s\tremaining: 38.9s\n",
      "539:\tlearn: 0.3812195\ttotal: 45.5s\tremaining: 38.8s\n",
      "540:\tlearn: 0.3811499\ttotal: 45.6s\tremaining: 38.7s\n",
      "541:\tlearn: 0.3810504\ttotal: 45.7s\tremaining: 38.6s\n",
      "542:\tlearn: 0.3809680\ttotal: 45.8s\tremaining: 38.6s\n",
      "543:\tlearn: 0.3807143\ttotal: 45.9s\tremaining: 38.5s\n",
      "544:\tlearn: 0.3805834\ttotal: 46s\tremaining: 38.4s\n",
      "545:\tlearn: 0.3805129\ttotal: 46.1s\tremaining: 38.3s\n",
      "546:\tlearn: 0.3804280\ttotal: 46.2s\tremaining: 38.2s\n",
      "547:\tlearn: 0.3802576\ttotal: 46.3s\tremaining: 38.2s\n",
      "548:\tlearn: 0.3801637\ttotal: 46.4s\tremaining: 38.1s\n",
      "549:\tlearn: 0.3800720\ttotal: 46.4s\tremaining: 38s\n",
      "550:\tlearn: 0.3799976\ttotal: 46.5s\tremaining: 37.9s\n",
      "551:\tlearn: 0.3799121\ttotal: 46.6s\tremaining: 37.8s\n",
      "552:\tlearn: 0.3798213\ttotal: 46.7s\tremaining: 37.8s\n",
      "553:\tlearn: 0.3796938\ttotal: 46.8s\tremaining: 37.7s\n",
      "554:\tlearn: 0.3796112\ttotal: 46.9s\tremaining: 37.6s\n",
      "555:\tlearn: 0.3795303\ttotal: 47s\tremaining: 37.5s\n",
      "556:\tlearn: 0.3794445\ttotal: 47.1s\tremaining: 37.5s\n",
      "557:\tlearn: 0.3793534\ttotal: 47.2s\tremaining: 37.4s\n",
      "558:\tlearn: 0.3792786\ttotal: 47.3s\tremaining: 37.3s\n",
      "559:\tlearn: 0.3792106\ttotal: 47.4s\tremaining: 37.2s\n",
      "560:\tlearn: 0.3791393\ttotal: 47.4s\tremaining: 37.1s\n",
      "561:\tlearn: 0.3790186\ttotal: 47.5s\tremaining: 37s\n",
      "562:\tlearn: 0.3789213\ttotal: 47.6s\tremaining: 37s\n",
      "563:\tlearn: 0.3788059\ttotal: 47.7s\tremaining: 36.9s\n",
      "564:\tlearn: 0.3787535\ttotal: 47.8s\tremaining: 36.8s\n",
      "565:\tlearn: 0.3786693\ttotal: 47.9s\tremaining: 36.7s\n",
      "566:\tlearn: 0.3786036\ttotal: 48s\tremaining: 36.6s\n",
      "567:\tlearn: 0.3783924\ttotal: 48.1s\tremaining: 36.5s\n",
      "568:\tlearn: 0.3783208\ttotal: 48.1s\tremaining: 36.5s\n",
      "569:\tlearn: 0.3783001\ttotal: 48.2s\tremaining: 36.4s\n",
      "570:\tlearn: 0.3782230\ttotal: 48.3s\tremaining: 36.3s\n",
      "571:\tlearn: 0.3781299\ttotal: 48.4s\tremaining: 36.2s\n",
      "572:\tlearn: 0.3780499\ttotal: 48.5s\tremaining: 36.1s\n",
      "573:\tlearn: 0.3779810\ttotal: 48.6s\tremaining: 36s\n",
      "574:\tlearn: 0.3778044\ttotal: 48.7s\tremaining: 36s\n",
      "575:\tlearn: 0.3776541\ttotal: 48.7s\tremaining: 35.9s\n",
      "576:\tlearn: 0.3775754\ttotal: 48.8s\tremaining: 35.8s\n",
      "577:\tlearn: 0.3775119\ttotal: 48.9s\tremaining: 35.7s\n",
      "578:\tlearn: 0.3774372\ttotal: 49s\tremaining: 35.6s\n",
      "579:\tlearn: 0.3773790\ttotal: 49.1s\tremaining: 35.5s\n",
      "580:\tlearn: 0.3773115\ttotal: 49.2s\tremaining: 35.5s\n",
      "581:\tlearn: 0.3772341\ttotal: 49.3s\tremaining: 35.4s\n",
      "582:\tlearn: 0.3770897\ttotal: 49.4s\tremaining: 35.3s\n",
      "583:\tlearn: 0.3770021\ttotal: 49.5s\tremaining: 35.2s\n",
      "584:\tlearn: 0.3767790\ttotal: 49.5s\tremaining: 35.1s\n",
      "585:\tlearn: 0.3767079\ttotal: 49.6s\tremaining: 35.1s\n",
      "586:\tlearn: 0.3766176\ttotal: 49.7s\tremaining: 35s\n",
      "587:\tlearn: 0.3765556\ttotal: 49.8s\tremaining: 34.9s\n",
      "588:\tlearn: 0.3764802\ttotal: 49.9s\tremaining: 34.8s\n",
      "589:\tlearn: 0.3763889\ttotal: 50s\tremaining: 34.8s\n",
      "590:\tlearn: 0.3762806\ttotal: 50.1s\tremaining: 34.7s\n",
      "591:\tlearn: 0.3762079\ttotal: 50.2s\tremaining: 34.6s\n",
      "592:\tlearn: 0.3757897\ttotal: 50.3s\tremaining: 34.5s\n",
      "593:\tlearn: 0.3757254\ttotal: 50.4s\tremaining: 34.4s\n",
      "594:\tlearn: 0.3756527\ttotal: 50.5s\tremaining: 34.3s\n",
      "595:\tlearn: 0.3755935\ttotal: 50.5s\tremaining: 34.3s\n",
      "596:\tlearn: 0.3754668\ttotal: 50.6s\tremaining: 34.2s\n",
      "597:\tlearn: 0.3753800\ttotal: 50.7s\tremaining: 34.1s\n",
      "598:\tlearn: 0.3752483\ttotal: 50.8s\tremaining: 34s\n",
      "599:\tlearn: 0.3751573\ttotal: 50.9s\tremaining: 33.9s\n",
      "600:\tlearn: 0.3750722\ttotal: 51s\tremaining: 33.9s\n",
      "601:\tlearn: 0.3749291\ttotal: 51.1s\tremaining: 33.8s\n",
      "602:\tlearn: 0.3748700\ttotal: 51.2s\tremaining: 33.7s\n",
      "603:\tlearn: 0.3747765\ttotal: 51.3s\tremaining: 33.6s\n",
      "604:\tlearn: 0.3747190\ttotal: 51.4s\tremaining: 33.5s\n",
      "605:\tlearn: 0.3746471\ttotal: 51.4s\tremaining: 33.4s\n",
      "606:\tlearn: 0.3745705\ttotal: 51.5s\tremaining: 33.4s\n",
      "607:\tlearn: 0.3743416\ttotal: 51.6s\tremaining: 33.3s\n",
      "608:\tlearn: 0.3741654\ttotal: 51.7s\tremaining: 33.2s\n",
      "609:\tlearn: 0.3740986\ttotal: 51.8s\tremaining: 33.1s\n",
      "610:\tlearn: 0.3740420\ttotal: 51.9s\tremaining: 33s\n",
      "611:\tlearn: 0.3739566\ttotal: 51.9s\tremaining: 32.9s\n",
      "612:\tlearn: 0.3738835\ttotal: 52s\tremaining: 32.8s\n",
      "613:\tlearn: 0.3737969\ttotal: 52.1s\tremaining: 32.8s\n",
      "614:\tlearn: 0.3737370\ttotal: 52.2s\tremaining: 32.7s\n",
      "615:\tlearn: 0.3736898\ttotal: 52.3s\tremaining: 32.6s\n",
      "616:\tlearn: 0.3736406\ttotal: 52.4s\tremaining: 32.5s\n",
      "617:\tlearn: 0.3735524\ttotal: 52.5s\tremaining: 32.4s\n",
      "618:\tlearn: 0.3734928\ttotal: 52.5s\tremaining: 32.3s\n",
      "619:\tlearn: 0.3734186\ttotal: 52.6s\tremaining: 32.3s\n",
      "620:\tlearn: 0.3733039\ttotal: 52.7s\tremaining: 32.2s\n",
      "621:\tlearn: 0.3732038\ttotal: 52.8s\tremaining: 32.1s\n",
      "622:\tlearn: 0.3731322\ttotal: 52.9s\tremaining: 32s\n",
      "623:\tlearn: 0.3730481\ttotal: 53s\tremaining: 31.9s\n",
      "624:\tlearn: 0.3729052\ttotal: 53.1s\tremaining: 31.9s\n",
      "625:\tlearn: 0.3728292\ttotal: 53.2s\tremaining: 31.8s\n",
      "626:\tlearn: 0.3727458\ttotal: 53.3s\tremaining: 31.7s\n",
      "627:\tlearn: 0.3726670\ttotal: 53.3s\tremaining: 31.6s\n",
      "628:\tlearn: 0.3725676\ttotal: 53.4s\tremaining: 31.5s\n",
      "629:\tlearn: 0.3724982\ttotal: 53.5s\tremaining: 31.4s\n",
      "630:\tlearn: 0.3724205\ttotal: 53.6s\tremaining: 31.3s\n",
      "631:\tlearn: 0.3723399\ttotal: 53.7s\tremaining: 31.3s\n",
      "632:\tlearn: 0.3722827\ttotal: 53.8s\tremaining: 31.2s\n",
      "633:\tlearn: 0.3721940\ttotal: 53.9s\tremaining: 31.1s\n",
      "634:\tlearn: 0.3721115\ttotal: 53.9s\tremaining: 31s\n",
      "635:\tlearn: 0.3719003\ttotal: 54s\tremaining: 30.9s\n",
      "636:\tlearn: 0.3718138\ttotal: 54.1s\tremaining: 30.8s\n",
      "637:\tlearn: 0.3717446\ttotal: 54.2s\tremaining: 30.7s\n",
      "638:\tlearn: 0.3716489\ttotal: 54.3s\tremaining: 30.7s\n",
      "639:\tlearn: 0.3716373\ttotal: 54.3s\tremaining: 30.6s\n",
      "640:\tlearn: 0.3715465\ttotal: 54.4s\tremaining: 30.5s\n",
      "641:\tlearn: 0.3714964\ttotal: 54.5s\tremaining: 30.4s\n",
      "642:\tlearn: 0.3714245\ttotal: 54.6s\tremaining: 30.3s\n",
      "643:\tlearn: 0.3713471\ttotal: 54.7s\tremaining: 30.2s\n",
      "644:\tlearn: 0.3712964\ttotal: 54.8s\tremaining: 30.2s\n",
      "645:\tlearn: 0.3712077\ttotal: 54.9s\tremaining: 30.1s\n",
      "646:\tlearn: 0.3711329\ttotal: 55s\tremaining: 30s\n",
      "647:\tlearn: 0.3710379\ttotal: 55.1s\tremaining: 29.9s\n",
      "648:\tlearn: 0.3710019\ttotal: 55.1s\tremaining: 29.8s\n",
      "649:\tlearn: 0.3709295\ttotal: 55.2s\tremaining: 29.7s\n",
      "650:\tlearn: 0.3708463\ttotal: 55.3s\tremaining: 29.7s\n",
      "651:\tlearn: 0.3707703\ttotal: 55.4s\tremaining: 29.6s\n",
      "652:\tlearn: 0.3706876\ttotal: 55.5s\tremaining: 29.5s\n",
      "653:\tlearn: 0.3705140\ttotal: 55.6s\tremaining: 29.4s\n",
      "654:\tlearn: 0.3704487\ttotal: 55.7s\tremaining: 29.3s\n",
      "655:\tlearn: 0.3703630\ttotal: 55.8s\tremaining: 29.2s\n",
      "656:\tlearn: 0.3702542\ttotal: 55.9s\tremaining: 29.2s\n",
      "657:\tlearn: 0.3701581\ttotal: 56s\tremaining: 29.1s\n",
      "658:\tlearn: 0.3700875\ttotal: 56s\tremaining: 29s\n",
      "659:\tlearn: 0.3700242\ttotal: 56.1s\tremaining: 28.9s\n",
      "660:\tlearn: 0.3699567\ttotal: 56.2s\tremaining: 28.8s\n",
      "661:\tlearn: 0.3698781\ttotal: 56.3s\tremaining: 28.7s\n",
      "662:\tlearn: 0.3697989\ttotal: 56.4s\tremaining: 28.7s\n",
      "663:\tlearn: 0.3697705\ttotal: 56.5s\tremaining: 28.6s\n",
      "664:\tlearn: 0.3696865\ttotal: 56.5s\tremaining: 28.5s\n",
      "665:\tlearn: 0.3696082\ttotal: 56.6s\tremaining: 28.4s\n",
      "666:\tlearn: 0.3695347\ttotal: 56.7s\tremaining: 28.3s\n",
      "667:\tlearn: 0.3694637\ttotal: 56.8s\tremaining: 28.2s\n",
      "668:\tlearn: 0.3693849\ttotal: 56.9s\tremaining: 28.1s\n",
      "669:\tlearn: 0.3692916\ttotal: 57s\tremaining: 28.1s\n",
      "670:\tlearn: 0.3692125\ttotal: 57.1s\tremaining: 28s\n",
      "671:\tlearn: 0.3690514\ttotal: 57.1s\tremaining: 27.9s\n",
      "672:\tlearn: 0.3689619\ttotal: 57.2s\tremaining: 27.8s\n",
      "673:\tlearn: 0.3689028\ttotal: 57.3s\tremaining: 27.7s\n",
      "674:\tlearn: 0.3688200\ttotal: 57.4s\tremaining: 27.6s\n",
      "675:\tlearn: 0.3687380\ttotal: 57.5s\tremaining: 27.6s\n",
      "676:\tlearn: 0.3686403\ttotal: 57.6s\tremaining: 27.5s\n",
      "677:\tlearn: 0.3685582\ttotal: 57.7s\tremaining: 27.4s\n",
      "678:\tlearn: 0.3684811\ttotal: 57.8s\tremaining: 27.3s\n",
      "679:\tlearn: 0.3683725\ttotal: 57.8s\tremaining: 27.2s\n",
      "680:\tlearn: 0.3682283\ttotal: 57.9s\tremaining: 27.1s\n",
      "681:\tlearn: 0.3681802\ttotal: 58s\tremaining: 27.1s\n",
      "682:\tlearn: 0.3681050\ttotal: 58.1s\tremaining: 27s\n",
      "683:\tlearn: 0.3680420\ttotal: 58.2s\tremaining: 26.9s\n",
      "684:\tlearn: 0.3679163\ttotal: 58.3s\tremaining: 26.8s\n",
      "685:\tlearn: 0.3678378\ttotal: 58.4s\tremaining: 26.7s\n",
      "686:\tlearn: 0.3677349\ttotal: 58.5s\tremaining: 26.6s\n",
      "687:\tlearn: 0.3676722\ttotal: 58.5s\tremaining: 26.5s\n",
      "688:\tlearn: 0.3675949\ttotal: 58.6s\tremaining: 26.5s\n",
      "689:\tlearn: 0.3674932\ttotal: 58.7s\tremaining: 26.4s\n",
      "690:\tlearn: 0.3674196\ttotal: 58.8s\tremaining: 26.3s\n",
      "691:\tlearn: 0.3672492\ttotal: 58.9s\tremaining: 26.2s\n",
      "692:\tlearn: 0.3671804\ttotal: 59s\tremaining: 26.1s\n",
      "693:\tlearn: 0.3670973\ttotal: 59.1s\tremaining: 26.1s\n",
      "694:\tlearn: 0.3670607\ttotal: 59.2s\tremaining: 26s\n",
      "695:\tlearn: 0.3669823\ttotal: 59.3s\tremaining: 25.9s\n",
      "696:\tlearn: 0.3668984\ttotal: 59.3s\tremaining: 25.8s\n",
      "697:\tlearn: 0.3668210\ttotal: 59.4s\tremaining: 25.7s\n",
      "698:\tlearn: 0.3667444\ttotal: 59.5s\tremaining: 25.6s\n",
      "699:\tlearn: 0.3666650\ttotal: 59.6s\tremaining: 25.6s\n",
      "700:\tlearn: 0.3665568\ttotal: 59.7s\tremaining: 25.5s\n",
      "701:\tlearn: 0.3664980\ttotal: 59.8s\tremaining: 25.4s\n",
      "702:\tlearn: 0.3664041\ttotal: 59.9s\tremaining: 25.3s\n",
      "703:\tlearn: 0.3663379\ttotal: 60s\tremaining: 25.2s\n",
      "704:\tlearn: 0.3662584\ttotal: 1m\tremaining: 25.1s\n",
      "705:\tlearn: 0.3660528\ttotal: 1m\tremaining: 25.1s\n",
      "706:\tlearn: 0.3659849\ttotal: 1m\tremaining: 25s\n",
      "707:\tlearn: 0.3658934\ttotal: 1m\tremaining: 24.9s\n",
      "708:\tlearn: 0.3658237\ttotal: 1m\tremaining: 24.8s\n",
      "709:\tlearn: 0.3658109\ttotal: 1m\tremaining: 24.7s\n",
      "710:\tlearn: 0.3657437\ttotal: 1m\tremaining: 24.6s\n",
      "711:\tlearn: 0.3656711\ttotal: 1m\tremaining: 24.5s\n",
      "712:\tlearn: 0.3656031\ttotal: 1m\tremaining: 24.5s\n",
      "713:\tlearn: 0.3655670\ttotal: 1m\tremaining: 24.4s\n",
      "714:\tlearn: 0.3654286\ttotal: 1m\tremaining: 24.3s\n",
      "715:\tlearn: 0.3653685\ttotal: 1m 1s\tremaining: 24.2s\n",
      "716:\tlearn: 0.3652896\ttotal: 1m 1s\tremaining: 24.1s\n",
      "717:\tlearn: 0.3652153\ttotal: 1m 1s\tremaining: 24s\n",
      "718:\tlearn: 0.3651330\ttotal: 1m 1s\tremaining: 24s\n",
      "719:\tlearn: 0.3650611\ttotal: 1m 1s\tremaining: 23.9s\n",
      "720:\tlearn: 0.3649717\ttotal: 1m 1s\tremaining: 23.8s\n",
      "721:\tlearn: 0.3649065\ttotal: 1m 1s\tremaining: 23.7s\n",
      "722:\tlearn: 0.3648273\ttotal: 1m 1s\tremaining: 23.6s\n",
      "723:\tlearn: 0.3647605\ttotal: 1m 1s\tremaining: 23.5s\n",
      "724:\tlearn: 0.3647313\ttotal: 1m 1s\tremaining: 23.5s\n",
      "725:\tlearn: 0.3646986\ttotal: 1m 1s\tremaining: 23.4s\n",
      "726:\tlearn: 0.3646258\ttotal: 1m 1s\tremaining: 23.3s\n",
      "727:\tlearn: 0.3645346\ttotal: 1m 2s\tremaining: 23.2s\n",
      "728:\tlearn: 0.3644495\ttotal: 1m 2s\tremaining: 23.1s\n",
      "729:\tlearn: 0.3643799\ttotal: 1m 2s\tremaining: 23s\n",
      "730:\tlearn: 0.3643164\ttotal: 1m 2s\tremaining: 22.9s\n",
      "731:\tlearn: 0.3643064\ttotal: 1m 2s\tremaining: 22.8s\n",
      "732:\tlearn: 0.3642760\ttotal: 1m 2s\tremaining: 22.8s\n",
      "733:\tlearn: 0.3642096\ttotal: 1m 2s\tremaining: 22.7s\n",
      "734:\tlearn: 0.3641361\ttotal: 1m 2s\tremaining: 22.6s\n",
      "735:\tlearn: 0.3640644\ttotal: 1m 2s\tremaining: 22.5s\n",
      "736:\tlearn: 0.3639933\ttotal: 1m 2s\tremaining: 22.4s\n",
      "737:\tlearn: 0.3639204\ttotal: 1m 2s\tremaining: 22.3s\n",
      "738:\tlearn: 0.3638296\ttotal: 1m 2s\tremaining: 22.2s\n",
      "739:\tlearn: 0.3637698\ttotal: 1m 3s\tremaining: 22.2s\n",
      "740:\tlearn: 0.3637074\ttotal: 1m 3s\tremaining: 22.1s\n",
      "741:\tlearn: 0.3636334\ttotal: 1m 3s\tremaining: 22s\n",
      "742:\tlearn: 0.3635815\ttotal: 1m 3s\tremaining: 21.9s\n",
      "743:\tlearn: 0.3635173\ttotal: 1m 3s\tremaining: 21.8s\n",
      "744:\tlearn: 0.3634491\ttotal: 1m 3s\tremaining: 21.7s\n",
      "745:\tlearn: 0.3633761\ttotal: 1m 3s\tremaining: 21.7s\n",
      "746:\tlearn: 0.3633243\ttotal: 1m 3s\tremaining: 21.6s\n",
      "747:\tlearn: 0.3632164\ttotal: 1m 3s\tremaining: 21.5s\n",
      "748:\tlearn: 0.3631343\ttotal: 1m 3s\tremaining: 21.4s\n",
      "749:\tlearn: 0.3630579\ttotal: 1m 3s\tremaining: 21.3s\n",
      "750:\tlearn: 0.3629743\ttotal: 1m 4s\tremaining: 21.2s\n",
      "751:\tlearn: 0.3629064\ttotal: 1m 4s\tremaining: 21.2s\n",
      "752:\tlearn: 0.3628969\ttotal: 1m 4s\tremaining: 21.1s\n",
      "753:\tlearn: 0.3628262\ttotal: 1m 4s\tremaining: 21s\n",
      "754:\tlearn: 0.3627514\ttotal: 1m 4s\tremaining: 20.9s\n",
      "755:\tlearn: 0.3626651\ttotal: 1m 4s\tremaining: 20.8s\n",
      "756:\tlearn: 0.3625896\ttotal: 1m 4s\tremaining: 20.7s\n",
      "757:\tlearn: 0.3625103\ttotal: 1m 4s\tremaining: 20.7s\n",
      "758:\tlearn: 0.3624409\ttotal: 1m 4s\tremaining: 20.6s\n",
      "759:\tlearn: 0.3623447\ttotal: 1m 4s\tremaining: 20.5s\n",
      "760:\tlearn: 0.3622628\ttotal: 1m 5s\tremaining: 20.4s\n",
      "761:\tlearn: 0.3621943\ttotal: 1m 5s\tremaining: 20.3s\n",
      "762:\tlearn: 0.3621202\ttotal: 1m 5s\tremaining: 20.2s\n",
      "763:\tlearn: 0.3620592\ttotal: 1m 5s\tremaining: 20.2s\n",
      "764:\tlearn: 0.3619875\ttotal: 1m 5s\tremaining: 20.1s\n",
      "765:\tlearn: 0.3619250\ttotal: 1m 5s\tremaining: 20s\n",
      "766:\tlearn: 0.3618819\ttotal: 1m 5s\tremaining: 19.9s\n",
      "767:\tlearn: 0.3618314\ttotal: 1m 5s\tremaining: 19.8s\n",
      "768:\tlearn: 0.3617816\ttotal: 1m 5s\tremaining: 19.7s\n",
      "769:\tlearn: 0.3617236\ttotal: 1m 5s\tremaining: 19.6s\n",
      "770:\tlearn: 0.3616435\ttotal: 1m 5s\tremaining: 19.6s\n",
      "771:\tlearn: 0.3615712\ttotal: 1m 5s\tremaining: 19.5s\n",
      "772:\tlearn: 0.3615114\ttotal: 1m 6s\tremaining: 19.4s\n",
      "773:\tlearn: 0.3615034\ttotal: 1m 6s\tremaining: 19.3s\n",
      "774:\tlearn: 0.3614286\ttotal: 1m 6s\tremaining: 19.2s\n",
      "775:\tlearn: 0.3613628\ttotal: 1m 6s\tremaining: 19.1s\n",
      "776:\tlearn: 0.3612995\ttotal: 1m 6s\tremaining: 19s\n",
      "777:\tlearn: 0.3612255\ttotal: 1m 6s\tremaining: 19s\n",
      "778:\tlearn: 0.3611785\ttotal: 1m 6s\tremaining: 18.9s\n",
      "779:\tlearn: 0.3611294\ttotal: 1m 6s\tremaining: 18.8s\n",
      "780:\tlearn: 0.3610376\ttotal: 1m 6s\tremaining: 18.7s\n",
      "781:\tlearn: 0.3609594\ttotal: 1m 6s\tremaining: 18.6s\n",
      "782:\tlearn: 0.3608742\ttotal: 1m 6s\tremaining: 18.5s\n",
      "783:\tlearn: 0.3608082\ttotal: 1m 6s\tremaining: 18.4s\n",
      "784:\tlearn: 0.3607449\ttotal: 1m 7s\tremaining: 18.4s\n",
      "785:\tlearn: 0.3606779\ttotal: 1m 7s\tremaining: 18.3s\n",
      "786:\tlearn: 0.3606089\ttotal: 1m 7s\tremaining: 18.2s\n",
      "787:\tlearn: 0.3605421\ttotal: 1m 7s\tremaining: 18.1s\n",
      "788:\tlearn: 0.3604749\ttotal: 1m 7s\tremaining: 18s\n",
      "789:\tlearn: 0.3603967\ttotal: 1m 7s\tremaining: 17.9s\n",
      "790:\tlearn: 0.3603184\ttotal: 1m 7s\tremaining: 17.9s\n",
      "791:\tlearn: 0.3601384\ttotal: 1m 7s\tremaining: 17.8s\n",
      "792:\tlearn: 0.3600662\ttotal: 1m 7s\tremaining: 17.7s\n",
      "793:\tlearn: 0.3600323\ttotal: 1m 7s\tremaining: 17.6s\n",
      "794:\tlearn: 0.3599542\ttotal: 1m 7s\tremaining: 17.5s\n",
      "795:\tlearn: 0.3598837\ttotal: 1m 8s\tremaining: 17.4s\n",
      "796:\tlearn: 0.3597058\ttotal: 1m 8s\tremaining: 17.3s\n",
      "797:\tlearn: 0.3594596\ttotal: 1m 8s\tremaining: 17.3s\n",
      "798:\tlearn: 0.3593086\ttotal: 1m 8s\tremaining: 17.2s\n",
      "799:\tlearn: 0.3592193\ttotal: 1m 8s\tremaining: 17.1s\n",
      "800:\tlearn: 0.3591172\ttotal: 1m 8s\tremaining: 17s\n",
      "801:\tlearn: 0.3590425\ttotal: 1m 8s\tremaining: 16.9s\n",
      "802:\tlearn: 0.3589814\ttotal: 1m 8s\tremaining: 16.8s\n",
      "803:\tlearn: 0.3589052\ttotal: 1m 8s\tremaining: 16.8s\n",
      "804:\tlearn: 0.3588325\ttotal: 1m 8s\tremaining: 16.7s\n",
      "805:\tlearn: 0.3587372\ttotal: 1m 8s\tremaining: 16.6s\n",
      "806:\tlearn: 0.3584686\ttotal: 1m 9s\tremaining: 16.5s\n",
      "807:\tlearn: 0.3582134\ttotal: 1m 9s\tremaining: 16.4s\n",
      "808:\tlearn: 0.3581426\ttotal: 1m 9s\tremaining: 16.3s\n",
      "809:\tlearn: 0.3580782\ttotal: 1m 9s\tremaining: 16.2s\n",
      "810:\tlearn: 0.3579679\ttotal: 1m 9s\tremaining: 16.2s\n",
      "811:\tlearn: 0.3578824\ttotal: 1m 9s\tremaining: 16.1s\n",
      "812:\tlearn: 0.3578161\ttotal: 1m 9s\tremaining: 16s\n",
      "813:\tlearn: 0.3577502\ttotal: 1m 9s\tremaining: 15.9s\n",
      "814:\tlearn: 0.3576820\ttotal: 1m 9s\tremaining: 15.8s\n",
      "815:\tlearn: 0.3576082\ttotal: 1m 9s\tremaining: 15.7s\n",
      "816:\tlearn: 0.3575439\ttotal: 1m 9s\tremaining: 15.7s\n",
      "817:\tlearn: 0.3574791\ttotal: 1m 9s\tremaining: 15.6s\n",
      "818:\tlearn: 0.3574338\ttotal: 1m 10s\tremaining: 15.5s\n",
      "819:\tlearn: 0.3573629\ttotal: 1m 10s\tremaining: 15.4s\n",
      "820:\tlearn: 0.3572982\ttotal: 1m 10s\tremaining: 15.3s\n",
      "821:\tlearn: 0.3572238\ttotal: 1m 10s\tremaining: 15.2s\n",
      "822:\tlearn: 0.3571483\ttotal: 1m 10s\tremaining: 15.1s\n",
      "823:\tlearn: 0.3570601\ttotal: 1m 10s\tremaining: 15.1s\n",
      "824:\tlearn: 0.3569796\ttotal: 1m 10s\tremaining: 15s\n",
      "825:\tlearn: 0.3569109\ttotal: 1m 10s\tremaining: 14.9s\n",
      "826:\tlearn: 0.3568207\ttotal: 1m 10s\tremaining: 14.8s\n",
      "827:\tlearn: 0.3567456\ttotal: 1m 10s\tremaining: 14.7s\n",
      "828:\tlearn: 0.3566707\ttotal: 1m 10s\tremaining: 14.6s\n",
      "829:\tlearn: 0.3566054\ttotal: 1m 11s\tremaining: 14.6s\n",
      "830:\tlearn: 0.3565317\ttotal: 1m 11s\tremaining: 14.5s\n",
      "831:\tlearn: 0.3564597\ttotal: 1m 11s\tremaining: 14.4s\n",
      "832:\tlearn: 0.3563827\ttotal: 1m 11s\tremaining: 14.3s\n",
      "833:\tlearn: 0.3563488\ttotal: 1m 11s\tremaining: 14.2s\n",
      "834:\tlearn: 0.3562796\ttotal: 1m 11s\tremaining: 14.1s\n",
      "835:\tlearn: 0.3562154\ttotal: 1m 11s\tremaining: 14s\n",
      "836:\tlearn: 0.3561653\ttotal: 1m 11s\tremaining: 14s\n",
      "837:\tlearn: 0.3560777\ttotal: 1m 11s\tremaining: 13.9s\n",
      "838:\tlearn: 0.3560124\ttotal: 1m 11s\tremaining: 13.8s\n",
      "839:\tlearn: 0.3559434\ttotal: 1m 11s\tremaining: 13.7s\n",
      "840:\tlearn: 0.3558689\ttotal: 1m 12s\tremaining: 13.6s\n",
      "841:\tlearn: 0.3557980\ttotal: 1m 12s\tremaining: 13.5s\n",
      "842:\tlearn: 0.3557760\ttotal: 1m 12s\tremaining: 13.4s\n",
      "843:\tlearn: 0.3556870\ttotal: 1m 12s\tremaining: 13.4s\n",
      "844:\tlearn: 0.3556798\ttotal: 1m 12s\tremaining: 13.3s\n",
      "845:\tlearn: 0.3556057\ttotal: 1m 12s\tremaining: 13.2s\n",
      "846:\tlearn: 0.3554361\ttotal: 1m 12s\tremaining: 13.1s\n",
      "847:\tlearn: 0.3553355\ttotal: 1m 12s\tremaining: 13s\n",
      "848:\tlearn: 0.3553181\ttotal: 1m 12s\tremaining: 12.9s\n",
      "849:\tlearn: 0.3552429\ttotal: 1m 12s\tremaining: 12.8s\n",
      "850:\tlearn: 0.3551794\ttotal: 1m 12s\tremaining: 12.8s\n",
      "851:\tlearn: 0.3551049\ttotal: 1m 12s\tremaining: 12.7s\n",
      "852:\tlearn: 0.3550014\ttotal: 1m 13s\tremaining: 12.6s\n",
      "853:\tlearn: 0.3549275\ttotal: 1m 13s\tremaining: 12.5s\n",
      "854:\tlearn: 0.3548406\ttotal: 1m 13s\tremaining: 12.4s\n",
      "855:\tlearn: 0.3547830\ttotal: 1m 13s\tremaining: 12.3s\n",
      "856:\tlearn: 0.3547189\ttotal: 1m 13s\tremaining: 12.2s\n",
      "857:\tlearn: 0.3546577\ttotal: 1m 13s\tremaining: 12.2s\n",
      "858:\tlearn: 0.3545949\ttotal: 1m 13s\tremaining: 12.1s\n",
      "859:\tlearn: 0.3545122\ttotal: 1m 13s\tremaining: 12s\n",
      "860:\tlearn: 0.3544133\ttotal: 1m 13s\tremaining: 11.9s\n",
      "861:\tlearn: 0.3543526\ttotal: 1m 13s\tremaining: 11.8s\n",
      "862:\tlearn: 0.3542534\ttotal: 1m 13s\tremaining: 11.7s\n",
      "863:\tlearn: 0.3541858\ttotal: 1m 13s\tremaining: 11.6s\n",
      "864:\tlearn: 0.3541241\ttotal: 1m 13s\tremaining: 11.5s\n",
      "865:\tlearn: 0.3540530\ttotal: 1m 14s\tremaining: 11.5s\n",
      "866:\tlearn: 0.3539900\ttotal: 1m 14s\tremaining: 11.4s\n",
      "867:\tlearn: 0.3539263\ttotal: 1m 14s\tremaining: 11.3s\n",
      "868:\tlearn: 0.3538599\ttotal: 1m 14s\tremaining: 11.2s\n",
      "869:\tlearn: 0.3538281\ttotal: 1m 14s\tremaining: 11.1s\n",
      "870:\tlearn: 0.3537596\ttotal: 1m 14s\tremaining: 11s\n",
      "871:\tlearn: 0.3536753\ttotal: 1m 14s\tremaining: 11s\n",
      "872:\tlearn: 0.3536022\ttotal: 1m 14s\tremaining: 10.9s\n",
      "873:\tlearn: 0.3535411\ttotal: 1m 14s\tremaining: 10.8s\n",
      "874:\tlearn: 0.3534694\ttotal: 1m 14s\tremaining: 10.7s\n",
      "875:\tlearn: 0.3533932\ttotal: 1m 14s\tremaining: 10.6s\n",
      "876:\tlearn: 0.3533270\ttotal: 1m 15s\tremaining: 10.5s\n",
      "877:\tlearn: 0.3532460\ttotal: 1m 15s\tremaining: 10.4s\n",
      "878:\tlearn: 0.3531906\ttotal: 1m 15s\tremaining: 10.4s\n",
      "879:\tlearn: 0.3530903\ttotal: 1m 15s\tremaining: 10.3s\n",
      "880:\tlearn: 0.3530015\ttotal: 1m 15s\tremaining: 10.2s\n",
      "881:\tlearn: 0.3529381\ttotal: 1m 15s\tremaining: 10.1s\n",
      "882:\tlearn: 0.3528657\ttotal: 1m 15s\tremaining: 10s\n",
      "883:\tlearn: 0.3528097\ttotal: 1m 15s\tremaining: 9.93s\n",
      "884:\tlearn: 0.3527425\ttotal: 1m 15s\tremaining: 9.84s\n",
      "885:\tlearn: 0.3526732\ttotal: 1m 15s\tremaining: 9.76s\n",
      "886:\tlearn: 0.3526253\ttotal: 1m 15s\tremaining: 9.67s\n",
      "887:\tlearn: 0.3525550\ttotal: 1m 16s\tremaining: 9.59s\n",
      "888:\tlearn: 0.3524886\ttotal: 1m 16s\tremaining: 9.5s\n",
      "889:\tlearn: 0.3524204\ttotal: 1m 16s\tremaining: 9.41s\n",
      "890:\tlearn: 0.3523592\ttotal: 1m 16s\tremaining: 9.33s\n",
      "891:\tlearn: 0.3522929\ttotal: 1m 16s\tremaining: 9.24s\n",
      "892:\tlearn: 0.3522465\ttotal: 1m 16s\tremaining: 9.16s\n",
      "893:\tlearn: 0.3521841\ttotal: 1m 16s\tremaining: 9.07s\n",
      "894:\tlearn: 0.3521183\ttotal: 1m 16s\tremaining: 8.99s\n",
      "895:\tlearn: 0.3520601\ttotal: 1m 16s\tremaining: 8.9s\n",
      "896:\tlearn: 0.3519991\ttotal: 1m 16s\tremaining: 8.82s\n",
      "897:\tlearn: 0.3519301\ttotal: 1m 16s\tremaining: 8.73s\n",
      "898:\tlearn: 0.3518971\ttotal: 1m 16s\tremaining: 8.65s\n",
      "899:\tlearn: 0.3518400\ttotal: 1m 17s\tremaining: 8.56s\n",
      "900:\tlearn: 0.3517646\ttotal: 1m 17s\tremaining: 8.47s\n",
      "901:\tlearn: 0.3517097\ttotal: 1m 17s\tremaining: 8.39s\n",
      "902:\tlearn: 0.3516492\ttotal: 1m 17s\tremaining: 8.3s\n",
      "903:\tlearn: 0.3515841\ttotal: 1m 17s\tremaining: 8.22s\n",
      "904:\tlearn: 0.3515196\ttotal: 1m 17s\tremaining: 8.13s\n",
      "905:\tlearn: 0.3514704\ttotal: 1m 17s\tremaining: 8.05s\n",
      "906:\tlearn: 0.3514613\ttotal: 1m 17s\tremaining: 7.96s\n",
      "907:\tlearn: 0.3513870\ttotal: 1m 17s\tremaining: 7.87s\n",
      "908:\tlearn: 0.3513079\ttotal: 1m 17s\tremaining: 7.79s\n",
      "909:\tlearn: 0.3512361\ttotal: 1m 17s\tremaining: 7.7s\n",
      "910:\tlearn: 0.3511766\ttotal: 1m 17s\tremaining: 7.62s\n",
      "911:\tlearn: 0.3511118\ttotal: 1m 18s\tremaining: 7.53s\n",
      "912:\tlearn: 0.3510168\ttotal: 1m 18s\tremaining: 7.45s\n",
      "913:\tlearn: 0.3509516\ttotal: 1m 18s\tremaining: 7.36s\n",
      "914:\tlearn: 0.3508936\ttotal: 1m 18s\tremaining: 7.28s\n",
      "915:\tlearn: 0.3508315\ttotal: 1m 18s\tremaining: 7.19s\n",
      "916:\tlearn: 0.3507658\ttotal: 1m 18s\tremaining: 7.11s\n",
      "917:\tlearn: 0.3506892\ttotal: 1m 18s\tremaining: 7.02s\n",
      "918:\tlearn: 0.3506413\ttotal: 1m 18s\tremaining: 6.94s\n",
      "919:\tlearn: 0.3505671\ttotal: 1m 18s\tremaining: 6.85s\n",
      "920:\tlearn: 0.3505605\ttotal: 1m 18s\tremaining: 6.76s\n",
      "921:\tlearn: 0.3504781\ttotal: 1m 18s\tremaining: 6.68s\n",
      "922:\tlearn: 0.3504056\ttotal: 1m 19s\tremaining: 6.59s\n",
      "923:\tlearn: 0.3503994\ttotal: 1m 19s\tremaining: 6.51s\n",
      "924:\tlearn: 0.3503263\ttotal: 1m 19s\tremaining: 6.42s\n",
      "925:\tlearn: 0.3502605\ttotal: 1m 19s\tremaining: 6.34s\n",
      "926:\tlearn: 0.3501671\ttotal: 1m 19s\tremaining: 6.25s\n",
      "927:\tlearn: 0.3500913\ttotal: 1m 19s\tremaining: 6.17s\n",
      "928:\tlearn: 0.3500231\ttotal: 1m 19s\tremaining: 6.08s\n",
      "929:\tlearn: 0.3499621\ttotal: 1m 19s\tremaining: 6s\n",
      "930:\tlearn: 0.3499000\ttotal: 1m 19s\tremaining: 5.91s\n",
      "931:\tlearn: 0.3498252\ttotal: 1m 19s\tremaining: 5.83s\n",
      "932:\tlearn: 0.3497534\ttotal: 1m 19s\tremaining: 5.74s\n",
      "933:\tlearn: 0.3496814\ttotal: 1m 20s\tremaining: 5.65s\n",
      "934:\tlearn: 0.3496387\ttotal: 1m 20s\tremaining: 5.57s\n",
      "935:\tlearn: 0.3495819\ttotal: 1m 20s\tremaining: 5.48s\n",
      "936:\tlearn: 0.3495163\ttotal: 1m 20s\tremaining: 5.39s\n",
      "937:\tlearn: 0.3494518\ttotal: 1m 20s\tremaining: 5.31s\n",
      "938:\tlearn: 0.3494092\ttotal: 1m 20s\tremaining: 5.22s\n",
      "939:\tlearn: 0.3493633\ttotal: 1m 20s\tremaining: 5.14s\n",
      "940:\tlearn: 0.3492965\ttotal: 1m 20s\tremaining: 5.05s\n",
      "941:\tlearn: 0.3492304\ttotal: 1m 20s\tremaining: 4.97s\n",
      "942:\tlearn: 0.3491833\ttotal: 1m 20s\tremaining: 4.88s\n",
      "943:\tlearn: 0.3490930\ttotal: 1m 20s\tremaining: 4.79s\n",
      "944:\tlearn: 0.3490300\ttotal: 1m 20s\tremaining: 4.71s\n",
      "945:\tlearn: 0.3490118\ttotal: 1m 20s\tremaining: 4.62s\n",
      "946:\tlearn: 0.3489532\ttotal: 1m 21s\tremaining: 4.54s\n",
      "947:\tlearn: 0.3488750\ttotal: 1m 21s\tremaining: 4.45s\n",
      "948:\tlearn: 0.3488084\ttotal: 1m 21s\tremaining: 4.37s\n",
      "949:\tlearn: 0.3487492\ttotal: 1m 21s\tremaining: 4.28s\n",
      "950:\tlearn: 0.3487433\ttotal: 1m 21s\tremaining: 4.19s\n",
      "951:\tlearn: 0.3486837\ttotal: 1m 21s\tremaining: 4.11s\n",
      "952:\tlearn: 0.3486496\ttotal: 1m 21s\tremaining: 4.02s\n",
      "953:\tlearn: 0.3485700\ttotal: 1m 21s\tremaining: 3.94s\n",
      "954:\tlearn: 0.3484871\ttotal: 1m 21s\tremaining: 3.85s\n",
      "955:\tlearn: 0.3484054\ttotal: 1m 21s\tremaining: 3.77s\n",
      "956:\tlearn: 0.3483677\ttotal: 1m 21s\tremaining: 3.68s\n",
      "957:\tlearn: 0.3482988\ttotal: 1m 22s\tremaining: 3.6s\n",
      "958:\tlearn: 0.3482097\ttotal: 1m 22s\tremaining: 3.51s\n",
      "959:\tlearn: 0.3481394\ttotal: 1m 22s\tremaining: 3.42s\n",
      "960:\tlearn: 0.3480698\ttotal: 1m 22s\tremaining: 3.34s\n",
      "961:\tlearn: 0.3480131\ttotal: 1m 22s\tremaining: 3.25s\n",
      "962:\tlearn: 0.3479538\ttotal: 1m 22s\tremaining: 3.17s\n",
      "963:\tlearn: 0.3479359\ttotal: 1m 22s\tremaining: 3.08s\n",
      "964:\tlearn: 0.3479295\ttotal: 1m 22s\tremaining: 3s\n",
      "965:\tlearn: 0.3478990\ttotal: 1m 22s\tremaining: 2.91s\n",
      "966:\tlearn: 0.3478275\ttotal: 1m 22s\tremaining: 2.82s\n",
      "967:\tlearn: 0.3478071\ttotal: 1m 22s\tremaining: 2.74s\n",
      "968:\tlearn: 0.3477269\ttotal: 1m 22s\tremaining: 2.65s\n",
      "969:\tlearn: 0.3476497\ttotal: 1m 23s\tremaining: 2.57s\n",
      "970:\tlearn: 0.3475881\ttotal: 1m 23s\tremaining: 2.48s\n",
      "971:\tlearn: 0.3475467\ttotal: 1m 23s\tremaining: 2.4s\n",
      "972:\tlearn: 0.3474900\ttotal: 1m 23s\tremaining: 2.31s\n",
      "973:\tlearn: 0.3474205\ttotal: 1m 23s\tremaining: 2.23s\n",
      "974:\tlearn: 0.3473627\ttotal: 1m 23s\tremaining: 2.14s\n",
      "975:\tlearn: 0.3473092\ttotal: 1m 23s\tremaining: 2.06s\n",
      "976:\tlearn: 0.3472213\ttotal: 1m 23s\tremaining: 1.97s\n",
      "977:\tlearn: 0.3471666\ttotal: 1m 23s\tremaining: 1.88s\n",
      "978:\tlearn: 0.3470995\ttotal: 1m 23s\tremaining: 1.8s\n",
      "979:\tlearn: 0.3470344\ttotal: 1m 23s\tremaining: 1.71s\n",
      "980:\tlearn: 0.3470005\ttotal: 1m 23s\tremaining: 1.63s\n",
      "981:\tlearn: 0.3469335\ttotal: 1m 24s\tremaining: 1.54s\n",
      "982:\tlearn: 0.3468700\ttotal: 1m 24s\tremaining: 1.46s\n",
      "983:\tlearn: 0.3467971\ttotal: 1m 24s\tremaining: 1.37s\n",
      "984:\tlearn: 0.3467237\ttotal: 1m 24s\tremaining: 1.28s\n",
      "985:\tlearn: 0.3466422\ttotal: 1m 24s\tremaining: 1.2s\n",
      "986:\tlearn: 0.3465788\ttotal: 1m 24s\tremaining: 1.11s\n",
      "987:\tlearn: 0.3465171\ttotal: 1m 24s\tremaining: 1.03s\n",
      "988:\tlearn: 0.3464383\ttotal: 1m 24s\tremaining: 942ms\n",
      "989:\tlearn: 0.3463569\ttotal: 1m 24s\tremaining: 857ms\n",
      "990:\tlearn: 0.3463001\ttotal: 1m 24s\tremaining: 771ms\n",
      "991:\tlearn: 0.3462374\ttotal: 1m 24s\tremaining: 685ms\n",
      "992:\tlearn: 0.3461626\ttotal: 1m 25s\tremaining: 600ms\n",
      "993:\tlearn: 0.3460431\ttotal: 1m 25s\tremaining: 514ms\n",
      "994:\tlearn: 0.3459790\ttotal: 1m 25s\tremaining: 428ms\n",
      "995:\tlearn: 0.3459154\ttotal: 1m 25s\tremaining: 343ms\n",
      "996:\tlearn: 0.3458534\ttotal: 1m 25s\tremaining: 257ms\n",
      "997:\tlearn: 0.3457635\ttotal: 1m 25s\tremaining: 171ms\n",
      "998:\tlearn: 0.3456946\ttotal: 1m 25s\tremaining: 85.6ms\n",
      "999:\tlearn: 0.3456500\ttotal: 1m 25s\tremaining: 0us\n",
      "f1 score: 0.8140355324587533\n"
     ]
    }
   ],
   "source": [
    "# train with catboost\n",
    "model_catboost_w2v_onehot = train_catboost(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment. Mixture of categorical columns and TFIDF w2v matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = DATA.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na on Consumer complaint narrative column, Consumer disputed? column\n",
    "\n",
    "dfm.dropna(subset=['Consumer complaint narrative', 'Consumer disputed?'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Tags, ZIP code, Complaint ID, Timely response? columns\n",
    "\n",
    "dfm.drop(['ZIP code', 'Complaint ID', 'Timely response?'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date received                      766\n",
       "Product                             12\n",
       "Sub-product                         46\n",
       "Issue                               91\n",
       "Sub-issue                           57\n",
       "Consumer complaint narrative    160940\n",
       "Company public response             10\n",
       "Company                           3148\n",
       "State                               62\n",
       "Tags                                 3\n",
       "Consumer consent provided?           1\n",
       "Submitted via                        1\n",
       "Date sent to company               862\n",
       "Company response to consumer         5\n",
       "Consumer disputed?                   2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfm unique values\n",
    "\n",
    "dfm.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date received                      766\n",
       "Product                             12\n",
       "Sub-product                         46\n",
       "Issue                               91\n",
       "Sub-issue                           57\n",
       "Consumer complaint narrative    160940\n",
       "Company public response             10\n",
       "Company                           3148\n",
       "State                               62\n",
       "Tags                                 3\n",
       "Date sent to company               862\n",
       "Company response to consumer         5\n",
       "Consumer disputed?                   2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop Consumer consent provided? column, submitted via column. no variance. \n",
    "\n",
    "dfm.drop(['Consumer consent provided?', 'Submitted via'], axis=1, inplace=True)\n",
    "\n",
    "dfm.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product                             12\n",
       "Sub-product                         46\n",
       "Issue                               91\n",
       "Sub-issue                           57\n",
       "Consumer complaint narrative    160940\n",
       "Company public response             10\n",
       "Company                           3148\n",
       "State                               62\n",
       "Tags                                 3\n",
       "Company response to consumer         5\n",
       "Consumer disputed?                   2\n",
       "days                               269\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert date received and date sent to company to datetime format, and compute date sent - date received, store in column 'days' as int\n",
    "\n",
    "dfm['Date received'] = pd.to_datetime(dfm['Date received'])\n",
    "dfm['Date sent to company'] = pd.to_datetime(dfm['Date sent to company'])\n",
    "\n",
    "dfm['days'] = (dfm['Date sent to company'] - dfm['Date received']).dt.days\n",
    "\n",
    "# drop Date received and Date sent to company columns\n",
    "dfm.drop(['Date received', 'Date sent to company'], axis=1, inplace=True)\n",
    "dfm.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product                         0\n",
       "Sub-product                     0\n",
       "Issue                           0\n",
       "Sub-issue                       0\n",
       "Consumer complaint narrative    0\n",
       "Company public response         0\n",
       "Company                         0\n",
       "State                           0\n",
       "Tags                            0\n",
       "Company response to consumer    0\n",
       "Consumer disputed?              0\n",
       "days                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill na with 'Unknown' in all columns\n",
    "\n",
    "dfm.fillna('Unknown', inplace=True)\n",
    "\n",
    "dfm.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    128227\n",
       "1     35807\n",
       "Name: Consumer disputed?, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode disputed column to 0 and 1\n",
    "\n",
    "dfm['Consumer disputed?'] = dfm['Consumer disputed?'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "dfm['Consumer disputed?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/164034 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164034/164034 [00:17<00:00, 9620.37it/s] \n"
     ]
    }
   ],
   "source": [
    "# preprocess narrative column and store in narrative_processed column\n",
    "\n",
    "dfm['narrative_processed'] = dfm['Consumer complaint narrative'].progress_apply(preprocess_narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# fit and transform tfidf vectorizer on narrative_processed column\n",
    "tfidf.fit(dfm.narrative_processed)\n",
    "\n",
    "# get tfidf feature names\n",
    "tfidf_features = tfidf.get_feature_names()\n",
    "\n",
    "# get tfidf weights\n",
    "tfidf_weights = tfidf.idf_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164034/164034 [1:10:02<00:00, 39.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(164034, 300)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# compute mean tfidf weighted word2vec vector for each narrative, reuse function from above\n",
    "dfm['narrative_tw'] = dfm['narrative_processed'].progress_apply(get_mean_tfidf_weighted_word2vec)\n",
    "\n",
    "# convert dfm['narrative_tw']  to numpy array tw_array\n",
    "tw_array = np.array(dfm.narrative_tw.tolist())\n",
    "\n",
    "# print shape of tw_array\n",
    "tw_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Product', 'Sub-product', 'Issue', 'Sub-issue',\n",
       "       'Consumer complaint narrative', 'Company public response', 'Company',\n",
       "       'State', 'Tags', 'Company response to consumer', 'Consumer disputed?',\n",
       "       'days', 'narrative_processed', 'narrative_tw'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy target column to y\n",
    "y = dfm['Consumer disputed?'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Consumer complaint narrative, narrative_processed, Consumer disputed? columns\n",
    "\n",
    "dfm.drop(['Consumer complaint narrative', 'narrative_processed', 'narrative_tw', 'Consumer disputed?'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product                           12\n",
       "Sub-product                       47\n",
       "Issue                             91\n",
       "Sub-issue                         58\n",
       "Company public response           11\n",
       "Company                         3148\n",
       "State                             63\n",
       "Tags                               4\n",
       "Company response to consumer       5\n",
       "days                             269\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['days', 'Product_Bank account or service', 'Product_Consumer Loan',\n",
      "       'Product_Credit card', 'Product_Credit reporting',\n",
      "       'Product_Debt collection', 'Product_Money transfers',\n",
      "       'Product_Mortgage', 'Product_Other financial service',\n",
      "       'Product_Payday loan',\n",
      "       ...\n",
      "       'State_WY', 'Tags_Older American', 'Tags_Older American, Servicemember',\n",
      "       'Tags_Servicemember', 'Tags_Unknown',\n",
      "       'Company response to consumer_Closed',\n",
      "       'Company response to consumer_Closed with explanation',\n",
      "       'Company response to consumer_Closed with monetary relief',\n",
      "       'Company response to consumer_Closed with non-monetary relief',\n",
      "       'Company response to consumer_Untimely response'],\n",
      "      dtype='object', length=3440)\n",
      "(164034, 3440)\n"
     ]
    }
   ],
   "source": [
    "# one hot encode categorical columns, reuse cat_cols from above\n",
    "\n",
    "cats_cols = [\n",
    "    'Product', \n",
    "    'Sub-product', \n",
    "    'Issue', \n",
    "    'Sub-issue',\n",
    "    'Company public response', \n",
    "    'Company',\n",
    "    'State', \n",
    "    'Tags', \n",
    "    'Company response to consumer',\n",
    "]\n",
    "\n",
    "\n",
    "dfm = pd.get_dummies(dfm, columns=cats_cols)\n",
    "\n",
    "print(dfm.columns)\n",
    "\n",
    "print(dfm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164034, 3740)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate tw_array with dfm\n",
    "X = np.concatenate((tw_array, dfm), axis=1)\n",
    "\n",
    "# check shape of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256454, 3740) (256454,)\n"
     ]
    }
   ],
   "source": [
    "# smoote to oversample minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# print shape of X_smote, y_smote\n",
    "print(X_smote.shape, y_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205163, 3740) (51291, 3740) (205163,) (51291,)\n"
     ]
    }
   ],
   "source": [
    "# train test split, 20% test, random_state=42, stratify=y_smote\n",
    "\n",
    "X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, stratify=y_smote, random_state=42, test_size=0.2)\n",
    "\n",
    "# print shape\n",
    "\n",
    "print(X_smote_train.shape, X_smote_test.shape, y_smote_train.shape, y_smote_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 102582, number of negative: 102581\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.913143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 138688\n",
      "[LightGBM] [Info] Number of data points in the train set: 205163, number of used features: 1395\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000010\n",
      "[LightGBM] [Info] Start training from score 0.000010\n",
      "f1 score: 0.8006513541309566\n"
     ]
    }
   ],
   "source": [
    "# train lightgbm with tfidf weighted word2vec data\n",
    "\n",
    "model_lgbm_tw_onehot = train_lgbm(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "catboost mixture categorical with tdidf w2v: f1 0.823 (BEST SO FAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train catboost with tfidf weighted word2vec data\n",
    "\n",
    "model_catboost_tw_onehot = train_catboost(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.6275768147305189\n"
     ]
    }
   ],
   "source": [
    "# train with naive bayes, scale values\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "model_nb_tw_onehot = train_naive_bayes(min_max_scaler.fit_transform(X_smote_train), y_smote_train, min_max_scaler.transform(X_smote_test), y_smote_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.6619451260316753\n"
     ]
    }
   ],
   "source": [
    "# train with logistic regression\n",
    "\n",
    "model_lr_tw_onehot = train_lr(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to train ann with tfidf weighted word2vec data\n",
    "\n",
    "def train_ann_tw_onehot(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # instantiate ann model\n",
    "    ann = Sequential()\n",
    "\n",
    "    # add input layer\n",
    "    ann.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    # add hidden layers\n",
    "    ann.add(Dense(64, activation='relu'))\n",
    "    ann.add(Dense(32, activation='relu'))\n",
    "    ann.add(Dense(16, activation='relu'))\n",
    "\n",
    "    # add output layer\n",
    "    ann.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile model\n",
    "    ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    ann.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # predict on test set\n",
    "    y_pred = ann.predict(X_test)\n",
    "\n",
    "    # print f1 score\n",
    "    print(f\"f1 score: {f1_score(y_test, y_pred.round())}\")\n",
    "\n",
    "    return ann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6412/6412 [==============================] - 36s 6ms/step - loss: 0.6138 - accuracy: 0.6618 - val_loss: 0.5767 - val_accuracy: 0.7014\n",
      "Epoch 2/10\n",
      "6412/6412 [==============================] - 37s 6ms/step - loss: 0.5469 - accuracy: 0.7236 - val_loss: 0.5510 - val_accuracy: 0.7182\n",
      "Epoch 3/10\n",
      "6412/6412 [==============================] - 37s 6ms/step - loss: 0.4997 - accuracy: 0.7579 - val_loss: 0.5347 - val_accuracy: 0.7322\n",
      "Epoch 4/10\n",
      "6412/6412 [==============================] - 37s 6ms/step - loss: 0.4608 - accuracy: 0.7832 - val_loss: 0.5046 - val_accuracy: 0.7590\n",
      "Epoch 5/10\n",
      "6412/6412 [==============================] - 38s 6ms/step - loss: 0.4279 - accuracy: 0.8020 - val_loss: 0.4932 - val_accuracy: 0.7681\n",
      "Epoch 6/10\n",
      "6412/6412 [==============================] - 40s 6ms/step - loss: 0.4024 - accuracy: 0.8166 - val_loss: 0.4933 - val_accuracy: 0.7730\n",
      "Epoch 7/10\n",
      "6412/6412 [==============================] - 40s 6ms/step - loss: 0.3807 - accuracy: 0.8278 - val_loss: 0.4932 - val_accuracy: 0.7789\n",
      "Epoch 8/10\n",
      "6412/6412 [==============================] - 41s 6ms/step - loss: 0.3621 - accuracy: 0.8384 - val_loss: 0.4862 - val_accuracy: 0.7801\n",
      "Epoch 9/10\n",
      "6412/6412 [==============================] - 41s 6ms/step - loss: 0.3454 - accuracy: 0.8474 - val_loss: 0.4908 - val_accuracy: 0.7827\n",
      "Epoch 10/10\n",
      "6412/6412 [==============================] - 43s 7ms/step - loss: 0.3303 - accuracy: 0.8548 - val_loss: 0.5028 - val_accuracy: 0.7864\n",
      "1603/1603 [==============================] - 3s 2ms/step\n",
      "f1 score: 0.7908743795341734\n"
     ]
    }
   ],
   "source": [
    "# train with ann\n",
    "\n",
    "model_ann_tw_onehot = train_ann_tw_onehot(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb 0.812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:53:42] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1 score: 0.811506402891224\n"
     ]
    }
   ],
   "source": [
    "# train with xgboost\n",
    "\n",
    "model_xgb_tw_onehot = train_xgb(X_smote_train, y_smote_train, X_smote_test, y_smote_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hypothsis: remove company. 3400+ instance to expand columns. added noise? (no, it added information. tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
