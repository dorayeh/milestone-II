{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2023-09-30 19:35:45.643996: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Built-in libraries\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "# Third-party libraries for data handling and processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Pre-processing\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Model selection and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Deep Learning Libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "# Miscellaneous\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# import google drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google: '/content/drive/My Drive/Colab Notebooks/assets/'\n",
    "\n",
    "file01 = 'complaints.csv'\n",
    "file02 = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "# file01 = '/content/drive/My Drive/Colab Notebooks/assets/complaints.csv'\n",
    "# file02 = '/content/drive/My Drive/Colab Notebooks/assets/GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "DATA = pd.read_csv(file01)\n",
    "# EMBEDDING = KeyedVectors.load_word2vec_format(file02, binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the narrative column\n",
    "\n",
    "def preprocess_narrative(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove XXXX like pattern\n",
    "    text = re.sub(r'x{2,}', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# define a function get mean word2vec vector for a narrative\n",
    "\n",
    "def get_mean_word2vec(word2vec, narrative):\n",
    "\n",
    "    # initialize vector\n",
    "    vector = np.zeros(300)\n",
    "\n",
    "    # get all words in narrative\n",
    "    words = narrative.split()\n",
    "    num_words = len(words)\n",
    "\n",
    "    if num_words == 0:  # edge case: empty narrative\n",
    "        return vector\n",
    "\n",
    "    # calculate word vectors using list comprehension\n",
    "    word_vectors = [word2vec[word] for word in words if word in word2vec]\n",
    "\n",
    "    if word_vectors:\n",
    "        vector = np.mean(word_vectors, axis=0)\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "# define a function get mean tfidf weighted word2vec vector for a narrative\n",
    "def get_mean_tfidf_weighted_word2vec(word2vec, tfidf_features, tfidf_weights, narrative):\n",
    "\n",
    "    # initialize vector\n",
    "    vector = np.zeros(300)\n",
    "\n",
    "    # get all words in narrative\n",
    "    words = narrative.split()\n",
    "    num_words = len(words)\n",
    "\n",
    "    if num_words == 0:  # edge case: empty narrative\n",
    "        return vector\n",
    "\n",
    "    # pre-calculate word-to-index mapping for tfidf_features for O(1) lookup\n",
    "    word_to_index = {word: idx for idx, word in enumerate(tfidf_features)}\n",
    "\n",
    "    # calculate word vectors using list comprehension\n",
    "    word_vectors = [\n",
    "        word2vec[word] * tfidf_weights[word_to_index[word]]\n",
    "        for word in words if word in word2vec and word in word_to_index\n",
    "    ]\n",
    "\n",
    "    if word_vectors:\n",
    "        vector = np.sum(word_vectors, axis=0) / num_words\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "# define a function to train and evaluate a model\n",
    "\n",
    "def train_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    # print model name\n",
    "    print(model.__class__.__name__)\n",
    "\n",
    "    # train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predict on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # print f1 score\n",
    "    print(\"F1 Score: {:.2f}\".format(f1_score(y_test, y_pred)))\n",
    "    print()\n",
    "\n",
    "    # print confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    \n",
    "    # print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get fresh copy to df from DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164034, 18)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DATA.copy()\n",
    "\n",
    "# drop narrative column and disputed column are null\n",
    "df.dropna(subset=['Consumer complaint narrative', 'Consumer disputed?'], inplace=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/164034 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164034/164034 [00:18<00:00, 8811.81it/s] \n"
     ]
    }
   ],
   "source": [
    "# preprocess the narrative column\n",
    "\n",
    "narrative_processed = df['Consumer complaint narrative'].progress_apply(preprocess_narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Consumer disputed?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test: text only, bow vs tf vs w2v vs tfidfw2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164034, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "bow = bow_vectorizer.fit_transform(narrative_processed)\n",
    "\n",
    "bow.shape\n",
    "\n",
    "# tfidf\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(narrative_processed)\n",
    "\n",
    "tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# word2vec\n",
    "\n",
    "word2vec = EMBEDDING\n",
    "\n",
    "word2vec_vectors = np.array([\n",
    "    get_mean_word2vec(word2vec, narrative)\n",
    "    for narrative in tqdm(narrative_processed)\n",
    "])\n",
    "\n",
    "word2vec_vectors.shape\n",
    "\n",
    "# tfidf weighted word2vec\n",
    "\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_weights = tfidf_vectorizer.idf_\n",
    "\n",
    "tfidf_weighted_word2vec_vectors = np.array([\n",
    "\n",
    "    get_mean_tfidf_weighted_word2vec(word2vec, tfidf_features, tfidf_weights, narrative)\n",
    "    for narrative in tqdm(narrative_processed)\n",
    "\n",
    "])\n",
    "\n",
    "tfidf_weighted_word2vec_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# label encode target variable\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['Consumer disputed?'] = le.fit_transform(df['Consumer disputed?'])\n",
    "\n",
    "y = df['Consumer disputed?']\n",
    "\n",
    "# split data into train and test sets for each feature set\n",
    "\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(word2vec_vectors, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tfidf_weighted_word2vec, X_test_tfidf_weighted_word2vec, y_train_tfidf_weighted_word2vec, y_test_tfidf_weighted_word2vec = train_test_split(tfidf_weighted_word2vec_vectors, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train and evaluate models for each feature set\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Bag of Words\")\n",
    "\n",
    "nb_bow = train_evaluate_model(MultinomialNB(), X_train_bow, y_train_bow, X_test_bow, y_test_bow)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"TF-IDF\")\n",
    "\n",
    "nb_tfidf = train_evaluate_model(MultinomialNB(), X_train_tfidf, y_train_tfidf, X_test_tfidf, y_test_tfidf)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Word2Vec\")\n",
    "\n",
    "# Naive Bayes does not support negative values, scale word2vec vectors\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_word2vec_scaled = scaler.fit_transform(X_train_word2vec)\n",
    "\n",
    "X_test_word2vec_scaled = scaler.transform(X_test_word2vec)\n",
    "\n",
    "nb_word2vec = train_evaluate_model(MultinomialNB(), X_train_word2vec_scaled, y_train_word2vec, X_test_word2vec_scaled, y_test_word2vec)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"TF-IDF Weighted Word2Vec\")\n",
    "\n",
    "# Naive Bayes does not support negative values, scale tfidf weighted word2vec vectors\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_tfidf_weighted_word2vec_scaled = scaler.fit_transform(X_train_tfidf_weighted_word2vec)\n",
    "\n",
    "X_test_tfidf_weighted_word2vec_scaled = scaler.transform(X_test_tfidf_weighted_word2vec)\n",
    "\n",
    "nb_tfidf_weighted_word2vec = train_evaluate_model(MultinomialNB(), X_train_tfidf_weighted_word2vec_scaled, y_train_tfidf_weighted_word2vec, X_test_tfidf_weighted_word2vec_scaled, y_test_tfidf_weighted_word2vec)\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with SMOTE\n",
      "\n",
      "Bag of Words\n",
      "F1 Score: 0.33\n",
      "\n",
      "Confusion Matrix:\n",
      "[[17051  8618]\n",
      " [ 4024  3114]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.66      0.73     25669\n",
      "           1       0.27      0.44      0.33      7138\n",
      "\n",
      "    accuracy                           0.61     32807\n",
      "   macro avg       0.54      0.55      0.53     32807\n",
      "weighted avg       0.69      0.61      0.64     32807\n",
      "\n",
      "\n",
      "TF-IDF\n",
      "F1 Score: 0.37\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15516 10153]\n",
      " [ 3160  3978]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.60      0.70     25669\n",
      "           1       0.28      0.56      0.37      7138\n",
      "\n",
      "    accuracy                           0.59     32807\n",
      "   macro avg       0.56      0.58      0.54     32807\n",
      "weighted avg       0.71      0.59      0.63     32807\n",
      "\n",
      "\n",
      "Word2Vec\n",
      "F1 Score: 0.35\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15164 10505]\n",
      " [ 3401  3737]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.59      0.69     25669\n",
      "           1       0.26      0.52      0.35      7138\n",
      "\n",
      "    accuracy                           0.58     32807\n",
      "   macro avg       0.54      0.56      0.52     32807\n",
      "weighted avg       0.70      0.58      0.61     32807\n",
      "\n",
      "\n",
      "TF-IDF Weighted Word2Vec\n",
      "F1 Score: 0.35\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15154 10515]\n",
      " [ 3403  3735]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.59      0.69     25669\n",
      "           1       0.26      0.52      0.35      7138\n",
      "\n",
      "    accuracy                           0.58     32807\n",
      "   macro avg       0.54      0.56      0.52     32807\n",
      "weighted avg       0.70      0.58      0.61     32807\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# smote to handle class imbalance for each feature set\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_bow_smote, y_train_bow_smote = smote.fit_resample(X_train_bow, y_train_bow)\n",
    "X_train_tfidf_smote, y_train_tfidf_smote = smote.fit_resample(X_train_tfidf, y_train_tfidf)\n",
    "X_train_word2vec_smote, y_train_word2vec_smote = smote.fit_resample(X_train_word2vec, y_train_word2vec)\n",
    "X_train_tfidf_weighted_word2vec_smote, y_train_tfidf_weighted_word2vec_smote = smote.fit_resample(X_train_tfidf_weighted_word2vec, y_train_tfidf_weighted_word2vec)\n",
    "\n",
    "# train and evaluate models for each feature set with smote\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "print(\"Naive Bayes with SMOTE\")\n",
    "print()\n",
    "\n",
    "print(\"Bag of Words\")\n",
    "nb_bow_smote = train_evaluate_model(MultinomialNB(), X_train_bow_smote, y_train_bow_smote, X_test_bow, y_test_bow)\n",
    "\n",
    "print()\n",
    "print(\"TF-IDF\")\n",
    "nb_tfidf_smote = train_evaluate_model(MultinomialNB(), X_train_tfidf_smote, y_train_tfidf_smote, X_test_tfidf, y_test_tfidf)\n",
    "\n",
    "print()\n",
    "print(\"Word2Vec\")\n",
    "# Naive Bayes does not support negative values, scale word2vec vectors\n",
    "scaler = MinMaxScaler()\n",
    "X_train_word2vec_smote_scaled = scaler.fit_transform(X_train_word2vec_smote)\n",
    "X_test_word2vec_scaled = scaler.transform(X_test_word2vec)\n",
    "nb_word2vec_smote = train_evaluate_model(MultinomialNB(), X_train_word2vec_smote_scaled, y_train_word2vec_smote, X_test_word2vec_scaled, y_test_word2vec)\n",
    "\n",
    "print()\n",
    "print(\"TF-IDF Weighted Word2Vec\")\n",
    "\n",
    "# Naive Bayes does not support negative values, scale tfidf weighted word2vec vectors\n",
    "scaler = MinMaxScaler()\n",
    "X_train_tfidf_weighted_word2vec_smote_scaled = scaler.fit_transform(X_train_tfidf_weighted_word2vec_smote)\n",
    "X_test_tfidf_weighted_word2vec_scaled = scaler.transform(X_test_tfidf_weighted_word2vec)\n",
    "nb_tfidf_weighted_word2vec_smote = train_evaluate_model(MultinomialNB(), X_train_tfidf_weighted_word2vec_smote_scaled, y_train_tfidf_weighted_word2vec_smote, X_test_tfidf_weighted_word2vec_scaled, y_test_tfidf_weighted_word2vec)\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mark: straight training, bow vs tf vs w2v vs tfidfw2v\n",
    "\n",
    "bow 0.32\n",
    "tfidf 0.02\n",
    "w2v 0.00\n",
    "tfidfw2v 0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mark: with smote\n",
    "\n",
    "bow: 0.33\n",
    "tfidf: 0.37\n",
    "w2c: 0.35\n",
    "tfidfw2v: 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression without SMOTE\n",
      "\n",
      "Bag of Words\n",
      "F1 Score: 0.19\n",
      "\n",
      "Confusion Matrix:\n",
      "[[24496  1173]\n",
      " [ 6277   861]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87     25669\n",
      "           1       0.42      0.12      0.19      7138\n",
      "\n",
      "    accuracy                           0.77     32807\n",
      "   macro avg       0.61      0.54      0.53     32807\n",
      "weighted avg       0.71      0.77      0.72     32807\n",
      "\n",
      "\n",
      "TF-IDF\n",
      "F1 Score: 0.09\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25372   297]\n",
      " [ 6791   347]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88     25669\n",
      "           1       0.54      0.05      0.09      7138\n",
      "\n",
      "    accuracy                           0.78     32807\n",
      "   macro avg       0.66      0.52      0.48     32807\n",
      "weighted avg       0.73      0.78      0.71     32807\n",
      "\n",
      "\n",
      "Word2Vec\n",
      "F1 Score: 0.01\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25629    40]\n",
      " [ 7106    32]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88     25669\n",
      "           1       0.44      0.00      0.01      7138\n",
      "\n",
      "    accuracy                           0.78     32807\n",
      "   macro avg       0.61      0.50      0.44     32807\n",
      "weighted avg       0.71      0.78      0.69     32807\n",
      "\n",
      "\n",
      "TF-IDF Weighted Word2Vec\n",
      "F1 Score: 0.01\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25601    68]\n",
      " [ 7093    45]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88     25669\n",
      "           1       0.40      0.01      0.01      7138\n",
      "\n",
      "    accuracy                           0.78     32807\n",
      "   macro avg       0.59      0.50      0.44     32807\n",
      "weighted avg       0.70      0.78      0.69     32807\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression without SMOTE for each feature set\n",
    "\n",
    "print(\"Logistic Regression without SMOTE\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Bag of Words\")\n",
    "\n",
    "lr_bow = train_evaluate_model(LogisticRegression(), X_train_bow, y_train_bow, X_test_bow, y_test_bow)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"TF-IDF\")\n",
    "\n",
    "lr_tfidf = train_evaluate_model(LogisticRegression(), X_train_tfidf, y_train_tfidf, X_test_tfidf, y_test_tfidf)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Word2Vec\")\n",
    "\n",
    "lr_word2vec = train_evaluate_model(LogisticRegression(), X_train_word2vec, y_train_word2vec, X_test_word2vec, y_test_word2vec)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"TF-IDF Weighted Word2Vec\")\n",
    "\n",
    "lr_tfidf_weighted_word2vec = train_evaluate_model(LogisticRegression(), X_train_tfidf_weighted_word2vec, y_train_tfidf_weighted_word2vec, X_test_tfidf_weighted_word2vec, y_test_tfidf_weighted_word2vec)\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with SMOTE\n",
      "\n",
      "Bag of Words\n",
      "F1 Score: 0.30\n",
      "\n",
      "Confusion Matrix:\n",
      "[[17877  7792]\n",
      " [ 4502  2636]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.70      0.74     25669\n",
      "           1       0.25      0.37      0.30      7138\n",
      "\n",
      "    accuracy                           0.63     32807\n",
      "   macro avg       0.53      0.53      0.52     32807\n",
      "weighted avg       0.68      0.63      0.65     32807\n",
      "\n",
      "\n",
      "TF-IDF\n",
      "F1 Score: 0.37\n",
      "\n",
      "Confusion Matrix:\n",
      "[[17324  8345]\n",
      " [ 3575  3563]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.67      0.74     25669\n",
      "           1       0.30      0.50      0.37      7138\n",
      "\n",
      "    accuracy                           0.64     32807\n",
      "   macro avg       0.56      0.59      0.56     32807\n",
      "weighted avg       0.71      0.64      0.66     32807\n",
      "\n",
      "\n",
      "Word2Vec\n",
      "F1 Score: 0.36\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14920 10749]\n",
      " [ 3207  3931]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.58      0.68     25669\n",
      "           1       0.27      0.55      0.36      7138\n",
      "\n",
      "    accuracy                           0.57     32807\n",
      "   macro avg       0.55      0.57      0.52     32807\n",
      "weighted avg       0.70      0.57      0.61     32807\n",
      "\n",
      "\n",
      "TF-IDF Weighted Word2Vec\n",
      "F1 Score: 0.36\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14675 10994]\n",
      " [ 3181  3957]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.57      0.67     25669\n",
      "           1       0.26      0.55      0.36      7138\n",
      "\n",
      "    accuracy                           0.57     32807\n",
      "   macro avg       0.54      0.56      0.52     32807\n",
      "weighted avg       0.70      0.57      0.61     32807\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with SMOTE for each feature set\n",
    "\n",
    "print(\"Logistic Regression with SMOTE\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Bag of Words\")\n",
    "\n",
    "lr_bow_smote = train_evaluate_model(LogisticRegression(), X_train_bow_smote, y_train_bow_smote, X_test_bow, y_test_bow)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"TF-IDF\")\n",
    "\n",
    "lr_tfidf_smote = train_evaluate_model(LogisticRegression(), X_train_tfidf_smote, y_train_tfidf_smote, X_test_tfidf, y_test_tfidf)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Word2Vec\")\n",
    "\n",
    "lr_word2vec_smote = train_evaluate_model(LogisticRegression(), X_train_word2vec_smote, y_train_word2vec_smote, X_test_word2vec, y_test_word2vec)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"TF-IDF Weighted Word2Vec\")\n",
    "\n",
    "lr_tfidf_weighted_word2vec_smote = train_evaluate_model(LogisticRegression(), X_train_tfidf_weighted_word2vec_smote, y_train_tfidf_weighted_word2vec_smote, X_test_tfidf_weighted_word2vec, y_test_tfidf_weighted_word2vec)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l r w/ smote\n",
    "\n",
    "bow 0.30\n",
    "tfidf 0.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue',\n",
       "       'Consumer complaint narrative', 'Company public response', 'Company',\n",
       "       'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n",
       "       'Submitted via', 'Date sent to company', 'Company response to consumer',\n",
       "       'Timely response?', 'Consumer disputed?', 'Complaint ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering: days between received and sent date, convert to int type\n",
    "\n",
    "df['Date received'] = pd.to_datetime(df['Date received'])\n",
    "df['Date sent to company'] = pd.to_datetime(df['Date sent to company'])\n",
    "\n",
    "df['days_between_received_sent'] = (df['Date sent to company'] - df['Date received']).dt.days\n",
    "df['days_between_received_sent'] = df['days_between_received_sent'].astype(int)\n",
    "\n",
    "# drop Date received and Date sent to company columns\n",
    "\n",
    "df.drop(columns=['Date received', 'Date sent to company'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product, Subproduct, Issue, Subisses, fill na with 'Not Provided'\n",
    "\n",
    "df['Product'].fillna('Not Provided', inplace=True)\n",
    "df['Sub-product'].fillna('Not Provided', inplace=True)\n",
    "\n",
    "df['Issue'].fillna('Not Provided', inplace=True)\n",
    "df['Sub-issue'].fillna('Not Provided', inplace=True)\n",
    "\n",
    "# feature engineering: combine Product, Subproduct as product_pubproduct\n",
    "df['product_subproduct'] = df['Product'] + ' ' + df['Sub-product']\n",
    "\n",
    "# feature engineering: combine Issue, Subissue as issue_subissue\n",
    "df['issue_subissue'] = df['Issue'] + ' ' + df['Sub-issue']\n",
    "\n",
    "# drop Product, Subproduct, Issue, Subissue columns\n",
    "df.drop(columns=['Product', 'Sub-product', 'Issue', 'Sub-issue'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Consumer complaint narrative', 'Company public response', 'Company',\n",
       "       'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n",
       "       'Submitted via', 'Company response to consumer', 'Timely response?',\n",
       "       'Consumer disputed?', 'Complaint ID', 'days_between_received_sent',\n",
       "       'product_subproduct', 'issue_subissue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company public response', 'Company', 'State', 'Tags',\n",
       "       'Company response to consumer', 'Consumer disputed?',\n",
       "       'days_between_received_sent', 'product_subproduct', 'issue_subissue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop Consumer complaint narrative column, Zip code column, Consumer consent provided? column, Submitted via, Timely responses? column, Complaint ID column\n",
    "\n",
    "df.drop(columns=['Consumer complaint narrative', 'ZIP code', 'Consumer consent provided?', 'Submitted via', 'Timely response?', 'Complaint ID'], inplace=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na with 'Not Provided'\n",
    "\n",
    "df.fillna('Not Provided', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company public response', 'Company', 'State', 'Tags',\n",
       "       'Company response to consumer', 'Consumer disputed?',\n",
       "       'days_between_received_sent', 'product_subproduct', 'issue_subissue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering: public response and response to consumer, combine them as public_response_response_to_consumer\n",
    "\n",
    "df['public_response_response_to_consumer'] = df['Company public response'] + ' ' + df['Company response to consumer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company public response                   11\n",
       "Company                                 3148\n",
       "State                                     63\n",
       "Tags                                       4\n",
       "Company response to consumer               5\n",
       "Consumer disputed?                         2\n",
       "days_between_received_sent               269\n",
       "product_subproduct                        52\n",
       "issue_subissue                           134\n",
       "public_response_response_to_consumer      44\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company', 'State', 'Tags', 'Consumer disputed?',\n",
       "       'days_between_received_sent', 'product_subproduct', 'issue_subissue',\n",
       "       'public_response_response_to_consumer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop Company public response and Company response to consumer columns\n",
    "\n",
    "df.drop(columns=['Company public response', 'Company response to consumer'], inplace=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode disputed column\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['Consumer disputed?'] = le.fit_transform(df['Consumer disputed?'])\n",
    "\n",
    "y = df['Consumer disputed?']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering: compute the ratio of complaints disputed to total complaints for each company\n",
    "\n",
    "df['company_complaints_disputed_ratio'] = df.groupby('Company')['Consumer disputed?'].transform(lambda x: x.sum() / x.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Consumer disputed?', 'days_between_received_sent',\n",
       "       'company_complaints_disputed_ratio',\n",
       "       'Company_1ST ALLIANCE LENDING, LLC',\n",
       "       'Company_1ST PREFERENCE MORTGAGE CORP',\n",
       "       'Company_1st Capital Mortgage, LLC',\n",
       "       'Company_1st Franklin Financial Corporation',\n",
       "       'Company_1st Money Center, Inc., Hurst, TX Branch',\n",
       "       'Company_21ST MORTGAGE CORP.', 'Company_2233 Paradise Road LLC',\n",
       "       ...\n",
       "       'public_response_response_to_consumer_Company disputes the facts presented in the complaint Closed with non-monetary relief',\n",
       "       'public_response_response_to_consumer_Company has responded to the consumer and the CFPB and chooses not to provide a public response Closed',\n",
       "       'public_response_response_to_consumer_Company has responded to the consumer and the CFPB and chooses not to provide a public response Closed with explanation',\n",
       "       'public_response_response_to_consumer_Company has responded to the consumer and the CFPB and chooses not to provide a public response Closed with monetary relief',\n",
       "       'public_response_response_to_consumer_Company has responded to the consumer and the CFPB and chooses not to provide a public response Closed with non-monetary relief',\n",
       "       'public_response_response_to_consumer_Not Provided Closed',\n",
       "       'public_response_response_to_consumer_Not Provided Closed with explanation',\n",
       "       'public_response_response_to_consumer_Not Provided Closed with monetary relief',\n",
       "       'public_response_response_to_consumer_Not Provided Closed with non-monetary relief',\n",
       "       'public_response_response_to_consumer_Not Provided Untimely response'],\n",
       "      dtype='object', length=3442)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode all categorical features\n",
    "\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "X = df.drop(columns=['Consumer disputed?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164034, 8441)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate X and bow features\n",
    "\n",
    "X_bow = np.concatenate((X.values, bow.toarray()), axis=1)\n",
    "\n",
    "X_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164034, 8441)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate X and tfidf features\n",
    "\n",
    "X_tfidf = np.concatenate((X.values, tfidf.toarray()), axis=1)\n",
    "\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256454, 8441) (256454,)\n",
      "(256454, 8441) (256454,)\n"
     ]
    }
   ],
   "source": [
    "# smote to handle class imbalance of X_bow and X_tfidf\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_bow_smote, y_bow_smote = smote.fit_resample(X_bow, y)\n",
    "X_tfidf_smote, y_tfidf_smote = smote.fit_resample(X_tfidf, y)\n",
    "\n",
    "# print shape of X_bow_smote and X_tfidf_smote\n",
    "print(X_bow_smote.shape, y_bow_smote.shape)\n",
    "print(X_tfidf_smote.shape, y_tfidf_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205163, 8441) (51291, 8441) (205163,) (51291,)\n",
      "(205163, 8441) (51291, 8441) (205163,) (51291,)\n"
     ]
    }
   ],
   "source": [
    "# train test split for X_bow and X_tfidf smoted\n",
    "\n",
    "X_train_bow_smote, X_test_bow_smote, y_train_bow_smote, y_test_bow_smote = train_test_split(X_bow_smote, y_bow_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tfidf_smote, X_test_tfidf_smote, y_train_tfidf_smote, y_test_tfidf_smote = train_test_split(X_tfidf_smote, y_tfidf_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# print shape of X_train_bow_smote, X_test_bow_smote, y_train_bow_smote, y_test_bow_smote\n",
    "print(X_train_bow_smote.shape, X_test_bow_smote.shape, y_train_bow_smote.shape, y_test_bow_smote.shape)\n",
    "\n",
    "# print shape of X_train_tfidf_smote, X_test_tfidf_smote, y_train_tfidf_smote, y_test_tfidf_smote\n",
    "print(X_train_tfidf_smote.shape, X_test_tfidf_smote.shape, y_train_tfidf_smote.shape, y_test_tfidf_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LGBMClassifier\n",
      "[LightGBM] [Info] Number of positive: 102570, number of negative: 102593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.594000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 843543\n",
      "[LightGBM] [Info] Number of data points in the train set: 205163, number of used features: 6083\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499944 -> initscore=-0.000224\n",
      "[LightGBM] [Info] Start training from score -0.000224\n",
      "F1 Score: 0.85\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25346   288]\n",
      " [ 6594 19063]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88     25634\n",
      "           1       0.99      0.74      0.85     25657\n",
      "\n",
      "    accuracy                           0.87     51291\n",
      "   macro avg       0.89      0.87      0.86     51291\n",
      "weighted avg       0.89      0.87      0.86     51291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lightgbm for bow\n",
    "\n",
    "lgbm_bow = train_evaluate_model(LGBMClassifier(), X_train_bow_smote, y_train_bow_smote, X_test_bow_smote, y_test_bow_smote)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LGBMClassifier\n",
      "[LightGBM] [Info] Number of positive: 102570, number of negative: 102593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.157780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1037029\n",
      "[LightGBM] [Info] Number of data points in the train set: 205163, number of used features: 6064\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499944 -> initscore=-0.000224\n",
      "[LightGBM] [Info] Start training from score -0.000224\n",
      "F1 Score: 0.82\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23298  2336]\n",
      " [ 6225 19432]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.84     25634\n",
      "           1       0.89      0.76      0.82     25657\n",
      "\n",
      "    accuracy                           0.83     51291\n",
      "   macro avg       0.84      0.83      0.83     51291\n",
      "weighted avg       0.84      0.83      0.83     51291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lightgbm for tfidf\n",
    "\n",
    "lgbm_tfidf = train_evaluate_model(LGBMClassifier(), X_train_tfidf_smote, y_train_tfidf_smote, X_test_tfidf_smote, y_test_tfidf_smote)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "CatBoostClassifier\n",
      "F1 Score: 0.85\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25101   533]\n",
      " [ 6325 19332]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88     25634\n",
      "           1       0.97      0.75      0.85     25657\n",
      "\n",
      "    accuracy                           0.87     51291\n",
      "   macro avg       0.89      0.87      0.86     51291\n",
      "weighted avg       0.89      0.87      0.86     51291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# catboost for bow\n",
    "\n",
    "cat_bow = train_evaluate_model(CatBoostClassifier(verbose=False, random_state=42), X_train_bow_smote, y_train_bow_smote, X_test_bow_smote, y_test_bow_smote)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "CatBoostClassifier\n",
      "F1 Score: 0.84\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23893  1741]\n",
      " [ 5997 19660]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.93      0.86     25634\n",
      "           1       0.92      0.77      0.84     25657\n",
      "\n",
      "    accuracy                           0.85     51291\n",
      "   macro avg       0.86      0.85      0.85     51291\n",
      "weighted avg       0.86      0.85      0.85     51291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# catboost for tfidf\n",
    "\n",
    "cat_tfidf = train_evaluate_model(CatBoostClassifier(verbose=False, random_state=42), X_train_tfidf_smote, y_train_tfidf_smote, X_test_tfidf_smote, y_test_tfidf_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "XGBClassifier\n",
      "F1 Score: 0.85\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25109   525]\n",
      " [ 6396 19261]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88     25634\n",
      "           1       0.97      0.75      0.85     25657\n",
      "\n",
      "    accuracy                           0.87     51291\n",
      "   macro avg       0.89      0.87      0.86     51291\n",
      "weighted avg       0.89      0.87      0.86     51291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgboost for bow\n",
    "\n",
    "xgb_bow = train_evaluate_model(XGBClassifier(random_state=42), X_train_bow_smote, y_train_bow_smote, X_test_bow_smote, y_test_bow_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "XGBClassifier\n",
      "F1 Score: 0.82\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23321  2313]\n",
      " [ 6075 19582]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85     25634\n",
      "           1       0.89      0.76      0.82     25657\n",
      "\n",
      "    accuracy                           0.84     51291\n",
      "   macro avg       0.84      0.84      0.84     51291\n",
      "weighted avg       0.84      0.84      0.84     51291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgboost for tfidf\n",
    "\n",
    "xgb_tfidf = train_evaluate_model(XGBClassifier(random_state=42), X_train_tfidf_smote, y_train_tfidf_smote, X_test_tfidf_smote, y_test_tfidf_smote)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bow nb: 0.62<br>\n",
    "tfidf nb: 0.66<br>\n",
    "\n",
    "bow lr: 0.68<br>\n",
    "tfidf lr: 0.67<br>\n",
    "\n",
    "bow light 0.85<br>\n",
    "tfidf light 0.82<br>\n",
    "\n",
    "bow cat: 0.85<br>\n",
    "tfidf cat: 0.84<br>\n",
    "\n",
    "bow xgb: 0.85<br>\n",
    "tfidf xgb: 0.82<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a fully connected neural network ANN for bow\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=X_train_bow_smote.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(X_train_bow_smote, y_train_bow_smote, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "# plot loss and accuracy\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# predict on test data\n",
    "y_pred = model.predict_classes(X_test_bow_smote)\n",
    "\n",
    "# print f1 score\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_test_bow_smote, y_pred)))\n",
    "\n",
    "# print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_bow_smote, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the model in a function\n",
    "\n",
    "def train_evaluate_ann(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    # plot loss and accuracy\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # predict on test data\n",
    "    y_pred = model.predict_classes(X_test)\n",
    "    \n",
    "    # print f1 score\n",
    "    print(\"F1 Score: {:.2f}\".format(f1_score(y_test, y_pred)))\n",
    "    \n",
    "    # print confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# train and evaluate ANN for tfidf\n",
    "\n",
    "ann_tfidf = train_evaluate_ann(X_train_tfidf_smote, y_train_tfidf_smote, X_test_tfidf_smote, y_test_tfidf_smote)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
