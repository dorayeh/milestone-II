{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised learning - reusable data creation"
      ],
      "metadata": {
        "id": "4QrIPiEKJDZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The notebook is intended to serve as a guide on how to prepare and manipulate data for reusability, and avoid the need to re-run the whole notebook.\n",
        "\n",
        "We will demonstrate how we create the HDF files for use in File 'Feature_Importance_and_Feature_Ablation.ipynb' in the first section, and the creation of HDF files for use in 'Analysis and Failure Analysis.ipynb' and 'Learning curve for all supervised models.ipynb' in the second section below.\n",
        "\n"
      ],
      "metadata": {
        "id": "ywvORPLYFoSF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pQpU5I1Y7lqN",
        "outputId": "60031570-3e5b-4dbe-ad62-9d623b3db443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Built-in libraries\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "\n",
        "# Third-party libraries for data handling and processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Pre-processing\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, MaxAbsScaler\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "\n",
        "\n",
        "# Miscellaneous\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tqdm.pandas()\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "k87noVVm8o7x",
        "outputId": "c4a38f87-bd97-4f9a-88be-71fcce542dd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.  \n",
        "#### data_X.csv and y.csv below are clean and manipulated data created in file 'supervised_1007.ipynb'.  \n",
        "#### data_X.csv and y.csv are used directly in file 'Feature Importance and Feature Ablation before SMOTE.ipynb'"
      ],
      "metadata": {
        "id": "easjjJUG5W_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pmAeGG8k8gi-"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data_X.csv')\n",
        "y = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/y.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hZuTthRG9Zf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4a29de5c-f336-4524-ab5f-9ddf12349419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(164034, 8435)\n",
            "(164034, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.    \n",
        "#### Creating Preprocessed data X_smote and y_smote used in file 'Feature_Importance_and_Feature_Ablation.ipynb' by conducting SMOTE on both data_X.csv and y.csv"
      ],
      "metadata": {
        "id": "42Cq6gtnT738"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_smote, y_smote = smote.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "9Mky8Q6aUuRn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape\n",
        "print(X_smote.shape, y_smote.shape)"
      ],
      "metadata": {
        "id": "2LtcO2X3W2Ks",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "30c51929-5e7f-4dc8-803e-60c0bbadaea9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256454, 8435) (256454, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_smote.to_hdf('/content/drive/MyDrive/Colab Notebooks/X_smote.h5', 'X')\n",
        "y_smote.to_hdf('/content/drive/MyDrive/Colab Notebooks/y_smote.h5', 'y')"
      ],
      "metadata": {
        "id": "NaQizVV34H54"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.  \n",
        "#### Creating Preprocessed data X.h5 and y.h5 used in file 'Analysis and Failure Analysis.ipynb' and 'Learning curve for all supervised models.ipynb'"
      ],
      "metadata": {
        "id": "yM7UwvXv6lRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/complaints.csv')\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h5i_ehbz7RfN",
        "outputId": "29091d96-8ab8-451c-cf7a-18433ce4d5c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4028530, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the narrative column\n",
        "def preprocess_narrative(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove XXXX like pattern\n",
        "    text = re.sub(r'x{2,}', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove stopwords\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove leading and trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# define a function get mean word2vec vector for a narrative\n",
        "def get_mean_word2vec(narrative):\n",
        "\n",
        "    # initialize vector\n",
        "    vector = np.zeros(300)\n",
        "\n",
        "    # get all words in narrative\n",
        "    words = narrative.split()\n",
        "    num_words = len(words)\n",
        "\n",
        "    if num_words == 0:  # edge case: empty narrative\n",
        "        return vector\n",
        "\n",
        "    # calculate word vectors using list comprehension\n",
        "    word_vectors = [word2vec[word] for word in words if word in word2vec]\n",
        "\n",
        "    if word_vectors:\n",
        "        vector = np.mean(word_vectors, axis=0)\n",
        "\n",
        "    return vector"
      ],
      "metadata": {
        "id": "QBtngkse7mqh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows with missing values on narrative column and dispute column\n",
        "df.dropna(subset=['Consumer complaint narrative', 'Consumer disputed?'], inplace=True)"
      ],
      "metadata": {
        "id": "jXb_vCt67sSm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the narrative column\n",
        "df['narrative_clean'] = df['Consumer complaint narrative'].progress_apply(preprocess_narrative)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hYzyXtp87uxZ",
        "outputId": "9bdb7ab2-4365-4363-ae69-c56fc76b5b67"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 164034/164034 [00:16<00:00, 9687.04it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode target variable\n",
        "df['Consumer disputed?'] = df['Consumer disputed?'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "y_tw = df['Consumer disputed?'].values"
      ],
      "metadata": {
        "id": "Pr8xG64a7wvt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering: days between date received and date sent to company"
      ],
      "metadata": {
        "id": "hGhRsIMm7z3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering: days between date received and date sent to company\n",
        "\n",
        "# convert date received and date sent to company to datetime\n",
        "df['Date received'] = pd.to_datetime(df['Date received'])\n",
        "df['Date sent to company'] = pd.to_datetime(df['Date sent to company'])\n",
        "\n",
        "# calculate days between date received and date sent to company\n",
        "df['days_between'] = (df['Date sent to company'] - df['Date received']).dt.days\n",
        "\n",
        "# int\n",
        "df['days_between'] = df['days_between'].astype(int)\n",
        "\n",
        "# fill with 0 for any negative values\n",
        "df['days_between'] = df['days_between'].apply(lambda x: 0 if x < 0 else x)"
      ],
      "metadata": {
        "id": "-TODEyWU72hk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering: number of words in complaint narrative"
      ],
      "metadata": {
        "id": "57QiO1nF75oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering: number of words in narrative (on original narrative column)\n",
        "\n",
        "# get number of words in narrative\n",
        "\n",
        "df['narrative_word_count'] = df['Consumer complaint narrative'].apply(lambda x: len(x.split()))"
      ],
      "metadata": {
        "id": "SbKFFGhP783Q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering: number of disputed for the company in the last 90 days"
      ],
      "metadata": {
        "id": "yeBh6SRw8ARO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering:\n",
        "# For each row, calculate the number of disputed for that company in the last 90 days\n",
        "# Define the lambda expression to compute disputed count for each row\n",
        "\n",
        "df[\"Disputed_count_last_90_days\"] = df.progress_apply(\n",
        "    lambda row: df[\n",
        "        (df[\"Date received\"] <= row[\"Date received\"]) &\n",
        "        (df[\"Date received\"] > (row[\"Date received\"] - pd.Timedelta(days=90))) &\n",
        "        (df[\"Company\"] == row[\"Company\"]) &\n",
        "        (df[\"Consumer disputed?\"] == 1)\n",
        "    ].shape[0],\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WAM_ggWw8B__",
        "outputId": "c67b9f55-35c5-4552-addf-9d56d53fdd7d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 164034/164034 [36:03<00:00, 75.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering: number of complaints for the company in the last 90 days"
      ],
      "metadata": {
        "id": "Srg9D10z8FQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering:\n",
        "# For each row, calculate the number of complaints for that company in the last 90 days\n",
        "df[\"Complaints_last_90_days\"] = df.progress_apply(\n",
        "    lambda row: df[\n",
        "        (df[\"Date received\"] <= row[\"Date received\"]) &\n",
        "        (df[\"Date received\"] > (row[\"Date received\"] - pd.Timedelta(days=90))) &\n",
        "        (df[\"Company\"] == row[\"Company\"])\n",
        "    ].shape[0],\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "j_nhpKgJ8HnQ",
        "outputId": "228a4ec6-03f7-45db-f226-61b2545b4a57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 164034/164034 [37:24<00:00, 73.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering: ratio. Rate of disputed"
      ],
      "metadata": {
        "id": "atkNy0FQ8J3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering:\n",
        "# ratio of number of disputed in the last 90 days / number of complaints in the last 90 days\n",
        "\n",
        "df['Disputed_ratio_last_90_days'] = df['Disputed_count_last_90_days'] / df['Complaints_last_90_days']"
      ],
      "metadata": {
        "id": "s_dCAL6o8MJc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import word2vec model (this take a while to load)\n",
        "word2vec = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "t5ElFn8R8xjy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute mean tfidf weighted word2vec vector for each narrative (take a while)\n",
        "df['narrative_clean_'] = df['narrative_clean'].progress_apply(get_mean_word2vec)\n",
        "\n",
        "# convert df['narrative_clean']  to numpy array tw_array\n",
        "tw_array = np.array(df.narrative_clean_.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n1W9ORns8Ou6",
        "outputId": "a88a7d4f-c629-4ead-ef2e-8bb52b522822"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 164034/164034 [00:46<00:00, 3520.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop columns logic"
      ],
      "metadata": {
        "id": "fn2T5TJy9MLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop columns\n",
        "# 1. date received (computed to days between, redundant)\n",
        "# 2. date sent to company (computed to days between, redundant)\n",
        "# 3. consumer complaint narrative (already vectorized to tfidf features)\n",
        "# 4. complaint ID (unique identifier, not useful for modeling)\n",
        "# 5. timely response? (highly skewed, 99-1, not useful for modeling\n",
        "# 6. consumer consent provided? (only one value, no variance not useful for modeling)\n",
        "# 7. submitted via (only one value, no variance not useful for modeling)\n",
        "# 8. narrative_clean (already vectorized to tfidf features)\n",
        "# 9. zip code (high cardinality, would expand almost 7000 dimension if included. likely correlate with State)\n",
        "# 10. consumer disputed? (remove target variable)\n",
        "\n",
        "drop_cols = [\n",
        "    'Date received',\n",
        "    'Date sent to company',\n",
        "    'Consumer complaint narrative',\n",
        "    'Complaint ID',\n",
        "    'Timely response?',\n",
        "    'Consumer consent provided?',\n",
        "    'Submitted via',\n",
        "    'narrative_clean',\n",
        "    'ZIP code',\n",
        "    'Consumer disputed?',\n",
        "    'narrative_clean_'\n",
        "]\n",
        "\n",
        "df.drop(drop_cols, axis=1, inplace=True)\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Rh-n0XzY9JKh",
        "outputId": "366e287f-687c-491f-c63f-f1e7d695d888"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 164034 entries, 31338 to 4028159\n",
            "Data columns (total 15 columns):\n",
            " #   Column                        Non-Null Count   Dtype  \n",
            "---  ------                        --------------   -----  \n",
            " 0   Product                       164034 non-null  object \n",
            " 1   Sub-product                   111863 non-null  object \n",
            " 2   Issue                         164034 non-null  object \n",
            " 3   Sub-issue                     83055 non-null   object \n",
            " 4   Company public response       78122 non-null   object \n",
            " 5   Company                       164034 non-null  object \n",
            " 6   State                         163575 non-null  object \n",
            " 7   Tags                          26994 non-null   object \n",
            " 8   Company response to consumer  164034 non-null  object \n",
            " 9   days_between                  164034 non-null  int64  \n",
            " 10  narrative_word_count          164034 non-null  int64  \n",
            " 11  Disputed_count_last_90_days   164034 non-null  int64  \n",
            " 12  Complaints_last_90_days       164034 non-null  int64  \n",
            " 13  Disputed_ratio_last_90_days   164034 non-null  float64\n",
            " 14  narrative_clean_              164034 non-null  object \n",
            "dtypes: float64(1), int64(4), object(10)\n",
            "memory usage: 20.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop_cols = [\n",
        "    'narrative_clean_'\n",
        "]\n",
        "\n",
        "df.drop(drop_cols, axis=1, inplace=True)\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ErOaq5IX3l-i",
        "outputId": "365cfe16-f012-4715-dd4e-4faacea4d3a6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 164034 entries, 31338 to 4028159\n",
            "Data columns (total 14 columns):\n",
            " #   Column                        Non-Null Count   Dtype  \n",
            "---  ------                        --------------   -----  \n",
            " 0   Product                       164034 non-null  object \n",
            " 1   Sub-product                   111863 non-null  object \n",
            " 2   Issue                         164034 non-null  object \n",
            " 3   Sub-issue                     83055 non-null   object \n",
            " 4   Company public response       78122 non-null   object \n",
            " 5   Company                       164034 non-null  object \n",
            " 6   State                         163575 non-null  object \n",
            " 7   Tags                          26994 non-null   object \n",
            " 8   Company response to consumer  164034 non-null  object \n",
            " 9   days_between                  164034 non-null  int64  \n",
            " 10  narrative_word_count          164034 non-null  int64  \n",
            " 11  Disputed_count_last_90_days   164034 non-null  int64  \n",
            " 12  Complaints_last_90_days       164034 non-null  int64  \n",
            " 13  Disputed_ratio_last_90_days   164034 non-null  float64\n",
            "dtypes: float64(1), int64(4), object(9)\n",
            "memory usage: 18.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fill na with 'Unknown'\n",
        "df.fillna('Unknown', inplace=True)"
      ],
      "metadata": {
        "id": "UpRi94Kj9pCM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot encode categorical features"
      ],
      "metadata": {
        "id": "fRyofxkY9sK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode categorical features\n",
        "df = pd.get_dummies(df, drop_first=True)"
      ],
      "metadata": {
        "id": "8Pw1v_mY9uEu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenate word2vec features and encoded dataframe"
      ],
      "metadata": {
        "id": "68jCeDwp-deC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# concat df with X_bow and df.values\n",
        "X_tw = np.concatenate((tw_array, df.values), axis=1)"
      ],
      "metadata": {
        "id": "MKMZJZyz-irQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imbalance Handling"
      ],
      "metadata": {
        "id": "Re2ZLpaS-XM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# smote to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_tw_smote, y_tw_smote = smote.fit_resample(X_tw, y_tw)\n",
        "\n",
        "# shape\n",
        "print(X_tw_smote.shape, y_tw_smote.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LMa0QcJ3-WtI",
        "outputId": "84a29b7d-800b-4b27-ec5d-e0445e74f55a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256454, 3735) (256454,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting cleaned and manipulated data to csv for reusability and avoid re-run the whole notebook."
      ],
      "metadata": {
        "id": "2aMJG8vZ9yQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting cleaned and manipulated data to hdf\n",
        "# save X and y for future use.\n",
        "# (ready-to-train data and target)\n",
        "\n",
        "# Define the path where we want to save the hdf files\n",
        "X_tw_smote_path = '/content/drive/MyDrive/Colab Notebooks/X.h5'\n",
        "y_tw_smote_path = '/content/drive/MyDrive/Colab Notebooks/y.h5'\n",
        "\n",
        "# Create a DataFrame\n",
        "data_X = pd.DataFrame(X_tw_smote)\n",
        "\n",
        "# Save data_X to HDF\n",
        "data_X.to_hdf(X_tw_smote_path, key='123')\n",
        "\n",
        "# Save y to HDF\n",
        "pd.DataFrame(y_tw_smote).to_hdf(y_tw_smote_path, key='124')"
      ],
      "metadata": {
        "id": "ku_RE_ZH9InE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to restore, do the following:\n",
        "\n",
        "# data_X = pd.read_hdf('X.h5', key='123')\n",
        "# y = pd.read_csv('y.h5.csv', key='124')"
      ],
      "metadata": {
        "id": "T_3LzqLj9075"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}